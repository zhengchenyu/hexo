<!DOCTYPE html>
<html >
<head>

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no" />
<meta name="author" content="zhengchenyu" />


    
    


<meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">

<link rel="apple-touch-icon" href= "/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">



    <link rel="shortcut icon" href="/favicon.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">


<link rel="stylesheet" href="/css/style.css">


    <style> .article { opacity: 0;} </style>


<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>Hexo</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: undefined
    }
</script>


    <script> yiliaConfig.jquery_ui = [false]; </script>



    <script> yiliaConfig.rootUrl = "\/";</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->






</head>
<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/avatar.png" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">zhengchenyu</a></h1>
        </hgroup>

        

        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                        <li>关于我</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/">标签云</a></li>
                        
                            <li><a href="/about/">关于我</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="mailto:123@123.com" title="Email"></a>
                            
                                <a class="fa GitHub" href="#" title="GitHub"></a>
                            
                                <a class="fa RSS" href="/atom.xml" title="RSS"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" href="https://hexo.io">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://pages.github.com/">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" href="http://moxfive.xyz/">MOxFIVE</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">专注于前端</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">zhengchenyu</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/avatar.png" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">zhengchenyu</a></h1>
            </hgroup>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/">标签云</a></li>
                
                    <li><a href="/about/">关于我</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="mailto:123@123.com" title="Email"></a>
                            
                                <a class="fa GitHub" target="_blank" href="#" title="GitHub"></a>
                            
                                <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="标签" friends="友情链接" about="关于我"/>
</nav>
      <div class="body-wrap">
  
    <article id="post-机器学习-3-2-非监督学习之主成分分析" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2017/10/22/机器学习-3-2-非监督学习之主成分分析/" class="article-date">
      <time datetime="2017-10-22T07:56:23.000Z" itemprop="datePublished">2017-10-22</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/10/22/机器学习-3-2-非监督学习之主成分分析/">机器学习-3.2-非监督学习之主成分分析.md</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <script type="text/javascript" src="/Users/zcy/Desktop/study/git/MathJax-master/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h1 id="主成分分析-PCA"><a href="#主成分分析-PCA" class="headerlink" title="主成分分析(PCA)"></a>主成分分析(PCA)</h1><blockquote>
<p>严格上说PCA应该算是一种降低维度的方法，这里暂时归类为非监督学习中。</p>
</blockquote>
<h2 id="1-PCA理论简述"><a href="#1-PCA理论简述" class="headerlink" title="1 PCA理论简述"></a>1 PCA理论简述</h2><p>PCA的主要思路是将\(n\)维数据降维到\(k\)维子空间中，以滤除不需要的噪声或没有意义的特征信息。譬如我们有汽车的一组运行数据，包括专项半径、速度等。其中就一个用英里/每小时和千米/每小时描述的速度，很明显两者是呈线性关系的，只是因为舍入带来一些误差，对于这样的问题我们完全可以将这个\(n\)维数据移到\(n-1\)维子空间中。</p>
<p>再举一个例子,对于RC直升机驾驶员的调查，\(x _1\)表示飞行员驾驶技能，\(x _2\)表示他对驾驶直升飞机的爱好程度。因为RC直升机很难驾驶，因此我们可以认为只有任务对驾驶RC直升机感兴趣的驾驶员才能学好它。因此,\(x _1\)和\(x _2\)有强相关性。如下图所示，我们可以假定一个方向\(u _1\)，用来表示\(x _1\)和\(x _2\)两个属性，仅仅在其垂直轴\(u _2\)上有少量噪声。</p>
<p><img src="/images/机器学习/PCA驾驶员技能兴趣关系曲线.png" width="50%" height="50%" text-align="center/"></p>
<p>对数据降维之前，我们需要对数据进行初始化，主要是一个归整的过程，将均值归整为0，将方差归整为1。依次按照如下公式进行归整。</p>
<ul>
<li>\(\mu  = \frac{1}{m}\sum\limits _{i = 1}^m {x^{(i)}} \)</li>
<li>\({x^{(i)}}: = {x^{(i)}} - \mu \)</li>
<li>\({\sigma ^2} = \frac{1}{m}\sum\limits _{i = 1}^m {(x^{(i)})^2} \)</li>
<li>\({x^{(i)}}: = {x^{(i)}}/\sigma \)</li>
</ul>
<p>归整后的数据如下，可以知道如果\(x\)在向量\(u\)的投影最大。说明\(u\)在对\(x _1\)和\(x _2\)的降维过程中带来的损失越小。</p>
<p><img src="/images/机器学习/PCA降维示例.png" width="50%" height="50%" text-align="center/"></p>
<p>因此，可以需要保证下式最大：</p>

$$\frac{1}{m}\sum\limits _{i = 1}^m {{{({x^{{{(i)}^T}}}u)}^2}}  = \frac{1}{m}\sum\limits _{i = 1}^m {{u^T}{x^{(i)}}{x^{{{(i)}^T}}}u}  = {u^T}(\frac{1}{m}\sum\limits _{i = 1}^m {{x^{(i)}}{x^{{{(i)}^T}}}} )u$$

<p>问题也就转变为找到一组\(u\)使得，上式最大的问题。然后我们假定\(u\)为一组标准正交基，因此\(||u||=1\)。因此问题可写为如下形式：</p>

$$\begin{gathered}   & \max {u^T}(\frac{1}{m}\sum\limits _{i = 1}^m {{x^{(i)}}{x^{{{(i)}^T}}}} )u \hfill \\\  s.t.: & ||u|| = 1 \hfill \\\ \end{gathered} $$

<p>组成其拉格朗日函数如下：</p>

$$L(u,\lambda ) = {u^T}(\frac{1}{m}\sum\limits _{i = 1}^m {x^{(i)}x^{(i)^T}} )u - \lambda (||u|| - 1)$$

<p>这里我们设\(\Sigma=\frac{1}{m}\sum\limits _{i = 1}^m {x^{(i)}{x^{(i)^T}}}\)，然后对\(u\)求导数，易得:</p>
$${\nabla _u}L(u,\lambda ) = \sum u - \lambda u = 0$$
<p>因此，我们知道\(\lambda\)为\(\Sigma\)的特征值，\(u\)为对应的特征向量。恰好为我们要求的向量\(u\)。得到一组新的正交基后，我们就可以映射到新的空间中了。具体如下:</p>

$${y^{(i)}} = {(u _1^T{x^{(i)}},u _2^T{x^{(i)}},...,u _k^T{x^{(i)}})^T}$$
<h2 id="2-PCA的奇异值分解"><a href="#2-PCA的奇异值分解" class="headerlink" title="2. PCA的奇异值分解"></a>2. PCA的奇异值分解</h2><p>由于一般为\(n*n\)的维度，因此往往是一个很大的矩阵。对于维度很大，样本数目要少于维度很多的数据，这样计算往往有不够划算。因此我们可以通过求解\(x\)的特征值的方法来求节\(\Sigma\)的特征向量。</p>
<blockquote>
<p>由于求矩阵的单位特征向量，可以不考虑1/m这个系数。</p>
</blockquote>
<p>假如对\(x\)进行奇异值分解，其中\(U\)和\(V\)均为酉阵，\(\Lambda\)为对角阵。另外值得注意的是酉阵的可逆矩阵与转置矩阵相同。具体分解形式如下：</p>
$$x = U\Lambda {V^T}$$

<p>因此可以得到：</p>
$$\Sigma  = x{x^T} = U\Lambda {V^T}V\Lambda {U^T} = U{\Lambda ^2}{U^T}$$$$\Sigma U = {\Lambda ^2}{U^T}$$
<p>因此可以知道\(\Sigma\)的特征向量对应着\(x\)的奇异值分解中的\(U\)。同时\(\Sigma\)的特征值为\(x\)的奇异值的平方。</p>
<h2 id="3-源码实现"><a href="#3-源码实现" class="headerlink" title="3 源码实现"></a>3 源码实现</h2><p>我们对k临近算法中的手写数字识别做分析。我们首先对数字映射到三维空间中，然后进行展示。然后在映射到样本数量维度的空间，实现对数字的识别。值得一提是，PCA可以将多种无法直接展示的多维数组转化为三维数据，然后更直观地进行分析。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> linalg</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> axes3d, Axes3D</div><div class="line"></div><div class="line"><span class="keyword">global</span> g_label      <span class="comment"># 训练集的label</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeTrain</span><span class="params">(dir)</span>:</span></div><div class="line">  dirs=os.listdir(dir)</div><div class="line">  files = filter(<span class="keyword">lambda</span> item:<span class="keyword">not</span> os.path.isdir(item), dirs)</div><div class="line">  mat = []</div><div class="line">  label = []</div><div class="line">  <span class="keyword">for</span> file <span class="keyword">in</span> files:</div><div class="line">    arr = []</div><div class="line">    f = open(dir+<span class="string">"/"</span>+file)</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">      line = f.readline()</div><div class="line">      <span class="keyword">if</span> <span class="keyword">not</span> line:</div><div class="line">        <span class="keyword">break</span></div><div class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> range(len(line)<span class="number">-1</span>):        <span class="comment"># line[len(line)]='\n'</span></div><div class="line">        arr.append(float(line[i]))</div><div class="line">    mat.append(arr)</div><div class="line">    label.append(int(file.split(<span class="string">"_"</span>)[<span class="number">0</span>]))</div><div class="line">  <span class="keyword">return</span> (mat,label)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocessing</span><span class="params">(trainList)</span>:</span></div><div class="line">  train = np.array(trainList)</div><div class="line">  rows = len(train)</div><div class="line">  cols = len(train[<span class="number">0</span>])</div><div class="line">  <span class="comment"># 使数学期望为0</span></div><div class="line">  <span class="keyword">for</span> col <span class="keyword">in</span> range(cols):</div><div class="line">    mean = np.mean(train[:,col])</div><div class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> range(rows):</div><div class="line">      train[row][col] = train[row][col]-mean</div><div class="line">    <span class="comment"># 使方差为1。 如果方差小于1,任务反差为0，则不更新该行。</span></div><div class="line">    <span class="comment"># 实际上，由于手写数字已经被归整为0,1这样的二值化图像，因此这里没有修改波定性。</span></div><div class="line">    var = np.var(train[:,col])</div><div class="line">    <span class="keyword">if</span> var &gt; <span class="number">1</span>:</div><div class="line">      print(<span class="string">"hello"</span>)</div><div class="line">      standard = np.sqrt(var)</div><div class="line">      <span class="keyword">for</span> row <span class="keyword">in</span> range(rows):</div><div class="line">        train[row][col] = train[row][col]/standard</div><div class="line">  <span class="keyword">return</span> train.tolist()</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lowerDimension</span><span class="params">(u3t,dir)</span>:</span></div><div class="line">  <span class="comment"># 该函数将1024维度的数据转化为3维</span></div><div class="line">  dirs=os.listdir(dir)</div><div class="line">  files = filter(<span class="keyword">lambda</span> item:<span class="keyword">not</span> os.path.isdir(item), dirs)</div><div class="line">  mat = []</div><div class="line">  label = []</div><div class="line">  <span class="keyword">for</span> file <span class="keyword">in</span> files:</div><div class="line">    arr = []</div><div class="line">    f = open(dir+<span class="string">"/"</span>+file)</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">      line = f.readline()</div><div class="line">      <span class="keyword">if</span> <span class="keyword">not</span> line:</div><div class="line">        <span class="keyword">break</span></div><div class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> range(len(line)<span class="number">-1</span>):        <span class="comment"># line[len(line)]='\n'</span></div><div class="line">        arr.append(float(line[i]))</div><div class="line">    new_mat = u3t*np.mat(arr).T</div><div class="line">    label.append(int(file.split(<span class="string">"_"</span>)[<span class="number">0</span>]))</div><div class="line">    mat.append((np.array(new_mat.T)[<span class="number">0</span>]).tolist())</div><div class="line">  <span class="keyword">return</span> (mat,label)         <span class="comment"># mat为num*3的list</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">show3D</span><span class="params">(g_train_3d)</span>:</span></div><div class="line">  fig = plt.figure()</div><div class="line">  ax = Axes3D(fig)</div><div class="line">  <span class="comment">#将数据点分成三部分画，在颜色上有区分度</span></div><div class="line">  m = len(g_train_3d)</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</div><div class="line">    <span class="keyword">if</span> g_label[i] == <span class="number">0</span>:</div><div class="line">      ax.scatter(g_train_3d[i][<span class="number">0</span>], g_train_3d[i][<span class="number">1</span>], g_train_3d[i][<span class="number">2</span>], c=<span class="string">'b'</span>)</div><div class="line">    <span class="keyword">elif</span> g_label[i] == <span class="number">1</span>:</div><div class="line">      ax.scatter(g_train_3d[i][<span class="number">0</span>], g_train_3d[i][<span class="number">1</span>], g_train_3d[i][<span class="number">2</span>], c=<span class="string">'c'</span>)</div><div class="line">    <span class="keyword">elif</span> g_label[i] == <span class="number">2</span>:</div><div class="line">      ax.scatter(g_train_3d[i][<span class="number">0</span>], g_train_3d[i][<span class="number">1</span>], g_train_3d[i][<span class="number">2</span>], c=<span class="string">'g'</span>)</div><div class="line">    <span class="keyword">elif</span> g_label[i] == <span class="number">3</span>:</div><div class="line">      ax.scatter(g_train_3d[i][<span class="number">0</span>], g_train_3d[i][<span class="number">1</span>], g_train_3d[i][<span class="number">2</span>], c=<span class="string">'k'</span>)</div><div class="line">    <span class="keyword">elif</span> g_label[i] == <span class="number">4</span>:</div><div class="line">      ax.scatter(g_train_3d[i][<span class="number">0</span>], g_train_3d[i][<span class="number">1</span>], g_train_3d[i][<span class="number">2</span>], c=<span class="string">'m'</span>)</div><div class="line">    <span class="keyword">elif</span> g_label[i] == <span class="number">5</span>:</div><div class="line">      ax.scatter(g_train_3d[i][<span class="number">0</span>], g_train_3d[i][<span class="number">1</span>], g_train_3d[i][<span class="number">2</span>], c=<span class="string">'r'</span>)</div><div class="line">    <span class="keyword">elif</span> g_label[i] == <span class="number">6</span>:</div><div class="line">      ax.scatter(g_train_3d[i][<span class="number">0</span>], g_train_3d[i][<span class="number">1</span>], g_train_3d[i][<span class="number">2</span>], c=<span class="string">'w'</span>)</div><div class="line">    <span class="keyword">elif</span> g_label[i] == <span class="number">7</span>:</div><div class="line">      ax.scatter(g_train_3d[i][<span class="number">0</span>], g_train_3d[i][<span class="number">1</span>], g_train_3d[i][<span class="number">2</span>], c=<span class="string">'y'</span>)</div><div class="line"><span class="comment">#    elif g_label[i] == 8:</span></div><div class="line"><span class="comment">#      ax.scatter(g_train_3d[i][0], g_train_3d[i][1], g_train_3d[i][2], c='b', depthshade=False)</span></div><div class="line"><span class="comment">#    elif g_label[i] == 9:</span></div><div class="line"><span class="comment">#      ax.scatter(g_train_3d[i][0], g_train_3d[i][1], g_train_3d[i][2], c='c', depthshade=False)</span></div><div class="line">  ax.set_zlabel(<span class="string">'Z'</span>) <span class="comment">#坐标轴</span></div><div class="line">  ax.set_ylabel(<span class="string">'Y'</span>)</div><div class="line">  ax.set_xlabel(<span class="string">'X'</span>)</div><div class="line">  plt.show()</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(ukt, train_3d, dir)</span>:</span></div><div class="line">  test_kd,test_label = lowerDimension(ukt, dir)</div><div class="line">  testN = len(test_kd)</div><div class="line">  right=<span class="number">0</span></div><div class="line">  wrong=<span class="number">0</span></div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(testN):</div><div class="line">    label = classify(np.array(test_kd[i]),np.array(train_3d),<span class="number">10</span>)</div><div class="line">    <span class="keyword">if</span> str(test_label[i]) == label:</div><div class="line">      right = right + <span class="number">1</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">      wrong = wrong + <span class="number">1</span></div><div class="line">  print(<span class="string">"right="</span>, right, <span class="string">", wrong="</span>, wrong)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(vec,train_kd,k)</span>:</span></div><div class="line">  <span class="comment"># 计算各个训练数据与测试数据的距离</span></div><div class="line">  m = len(g_label)</div><div class="line">  dis = []</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</div><div class="line">    dis.append([linalg.norm(vec-train_kd[i]),g_label[i]])</div><div class="line">  dis = sorted(dis, key=<span class="keyword">lambda</span> v:v[<span class="number">0</span>])</div><div class="line">  <span class="comment"># 计算相似度最高的k个值，这里写入map做累积</span></div><div class="line">  dic = &#123;&#125;</div><div class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> range(k):</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> str(dis[j][<span class="number">1</span>]) <span class="keyword">in</span> dic:</div><div class="line">      dic[str(dis[j][<span class="number">1</span>])]=<span class="number">1</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">      dic[str(dis[j][<span class="number">1</span>])]=dic[str(dis[j][<span class="number">1</span>])]+<span class="number">1</span></div><div class="line">  <span class="keyword">return</span> max(dic.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])[<span class="number">0</span>]</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</div><div class="line">  <span class="comment"># 1 预处理</span></div><div class="line">  <span class="comment"># 这里为了显示降低维度在训练样本中的作用，仅仅是用了300个样本</span></div><div class="line">  (train,g_label) = makeTrain(<span class="string">"/Users/zcy/Desktop/study/git/mlearning/res/trainingDigits1"</span>)</div><div class="line">  <span class="comment"># 进行预处理操作，将均值设置为0，将方差归整为1</span></div><div class="line">  train_processed = preprocessing(train)</div><div class="line"></div><div class="line">  <span class="comment"># 2 降低维度</span></div><div class="line">  <span class="comment"># 首先计算x的奇异值</span></div><div class="line">  train_mat = np.mat(train_processed)</div><div class="line">  train_mat = train_mat.T               <span class="comment"># 转化为n*m 1024*200</span></div><div class="line">  U,sigma,VT = linalg.svd(train_mat)      <span class="comment"># U的维度为n*n 即1024*1024. sigma为m*1. vt为300*300</span></div><div class="line">  u3=U[np.ix_(np.arange(<span class="number">1024</span>), np.arange(<span class="number">3</span>))]      <span class="comment"># 提取对应最高特征值最高的三个方向,u3的维度为1024*3</span></div><div class="line">  <span class="comment"># 将1024维的数字图像降低维度到三维向量</span></div><div class="line">  train_3d,_ = lowerDimension(u3.T,<span class="string">"/Users/zcy/Desktop/study/git/mlearning/res/trainingDigits1"</span>)</div><div class="line">  train_kd,_ = lowerDimension(U.T,<span class="string">"/Users/zcy/Desktop/study/git/mlearning/res/trainingDigits1"</span>)</div><div class="line"></div><div class="line">  <span class="comment"># 3 展示3维下的模型信息</span></div><div class="line">  <span class="comment">#show3D(train_3d)</span></div><div class="line"></div><div class="line">  <span class="comment"># 4 使用k邻域验证测试样本</span></div><div class="line">  <span class="comment">#test(u3.T, train_3d, "/Users/zcy/Desktop/study/git/mlearning/res/testDigits1")</span></div><div class="line">  test(U.T,train_kd,<span class="string">"/Users/zcy/Desktop/study/git/mlearning/res/testDigits1"</span>)</div></pre></td></tr></table></figure>
<p>500个测试样本中，有27个识别错误，具体识别率为94.6%。这与未使用PCA降维的完全一致。该例子似乎尚未体现到PCA有什么优势，以后有机会在分析。当然如果映射到三维空间，识别率仅仅为75.2%，因此不要过度降维。下面是一个展示到部分数据的三维图。</p>
<blockquote>
<p>考虑到颜色，图片只显示部分类别数据。</p>
</blockquote>
<p><img src="/images/机器学习/PCA手写数字三维展示图.png" width="50%" height="50%" text-align="center/"></p>
<h2 id="4-参考文献"><a href="#4-参考文献" class="headerlink" title="4 参考文献"></a>4 参考文献</h2><ul>
<li>cs229-note10</li>
<li>机器学习实战</li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-机器学习-2-8-监督学习之k近邻算法" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2017/10/15/机器学习-2-8-监督学习之k近邻算法/" class="article-date">
      <time datetime="2017-10-15T12:29:06.000Z" itemprop="datePublished">2017-10-15</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/10/15/机器学习-2-8-监督学习之k近邻算法/">机器学习-2.8-监督学习之k近邻算法</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <script type="text/javascript" src="/Users/zcy/Desktop/study/git/MathJax-master/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<h1 id="K近邻算法"><a href="#K近邻算法" class="headerlink" title="K近邻算法"></a>K近邻算法</h1><p>K近邻算法是一种常用的监督学习算法。其工作机制非常简单: 给定测试样本，基于某种距离度量找到与测试样本最近的k个训练样本，然后可以根据k个样本决定分类。譬如可以选择k个样本中最多的测试样本进行分类。</p>
<p>下面我们使用k近邻算法识别手写数字。手写数字为一个32*32维的向量，我们可以把它看成一个1024维的向量。然后任意两个样本的距离完全可以通过计算1024维上的两个点之间的距离获得，然后我们可以找到与测试样本距离最近的的10个样本，其中最多的分类我们认为他就是测试样本的分类。具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np<span class="keyword">import</span> os<span class="keyword">from</span> numpy <span class="keyword">import</span> linalg<span class="keyword">global</span> g_train<span class="keyword">global</span> g_label<span class="function"><span class="keyword">def</span> <span class="title">makeTrain</span><span class="params">(dir)</span>:</span>  dirs=os.listdir(dir)  files = filter(<span class="keyword">lambda</span> item:<span class="keyword">not</span> os.path.isdir(item), dirs)  mat = []  label = []  <span class="keyword">for</span> file <span class="keyword">in</span> files:    arr = []    f = open(dir+<span class="string">"/"</span>+file)    <span class="keyword">while</span> <span class="keyword">True</span>:      line = f.readline()      <span class="keyword">if</span> <span class="keyword">not</span> line:        <span class="keyword">break</span>      <span class="keyword">for</span> i <span class="keyword">in</span> range(len(line)<span class="number">-1</span>):        <span class="comment"># line[len(line)]='\n'</span>        arr.append(int(line[i]))    mat.append(arr)    label.append(int(file.split(<span class="string">"_"</span>)[<span class="number">0</span>]))  <span class="keyword">return</span> (np.array(mat),np.array(label))<span class="function"><span class="keyword">def</span> <span class="title">testClassify</span><span class="params">(dir)</span>:</span>  dirs=os.listdir(dir)  files = filter(<span class="keyword">lambda</span> item:<span class="keyword">not</span> os.path.isdir(item), dirs)  mat = []  right=<span class="number">0</span>  wrong=<span class="number">0</span>  <span class="keyword">for</span> file <span class="keyword">in</span> files:    arr = []    f = open(dir+<span class="string">"/"</span>+file)    <span class="keyword">while</span> <span class="keyword">True</span>:      line = f.readline()      <span class="keyword">if</span> <span class="keyword">not</span> line:        <span class="keyword">break</span>      <span class="keyword">for</span> i <span class="keyword">in</span> range(len(line)<span class="number">-1</span>):        <span class="comment"># line[len(line)]='\n'</span>        arr.append(int(line[i]))    mat.append(arr)    testLabel = file.split(<span class="string">"_"</span>)[<span class="number">0</span>]    label=classify(np.array(arr),<span class="number">10</span>)    <span class="keyword">if</span> testLabel == label:      right=right+<span class="number">1</span>    <span class="keyword">else</span>:      wrong=wrong+<span class="number">1</span>  print(<span class="string">"right="</span>,right,<span class="string">", wrong="</span>,wrong)<span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(vec,k)</span>:</span>  <span class="comment"># 计算各个训练数据与测试数据的距离</span>  m = len(g_label)  dis = []  <span class="keyword">for</span> i <span class="keyword">in</span> range(m):    dis.append([linalg.norm(vec-g_train[i]),g_label[i]])  dis = sorted(dis, key=<span class="keyword">lambda</span> v:v[<span class="number">0</span>])  <span class="comment"># 计算相似度最高的k个值，这里写入map做累积</span>  dic = &#123;&#125;  <span class="keyword">for</span> j <span class="keyword">in</span> range(k):    <span class="keyword">if</span> <span class="keyword">not</span> str(dis[j][<span class="number">1</span>]) <span class="keyword">in</span> dic:      dic[str(dis[j][<span class="number">1</span>])]=<span class="number">1</span>    <span class="keyword">else</span>:      dic[str(dis[j][<span class="number">1</span>])]=dic[str(dis[j][<span class="number">1</span>])]+<span class="number">1</span>  <span class="keyword">return</span> max(dic.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])[<span class="number">0</span>]<span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:  <span class="comment"># 1 formate trainning date</span>  (g_train,g_label) =makeTrain(<span class="string">"/Users/zcy/Desktop/study/git/mlearning/res/trainingDigits1"</span>)  <span class="comment"># 2 test</span>  testClassify(<span class="string">"/Users/zcy/Desktop/study/git/mlearning/res/testDigits1"</span>)</div></pre></td></tr></table></figure>
<p>计算946个测试样本集，有926个为正确计算，20个为错误计算，识别为97.89%</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-机器学习-3-1-非监督学习之聚类" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2017/09/24/机器学习-3-1-非监督学习之聚类/" class="article-date">
      <time datetime="2017-09-24T13:36:20.000Z" itemprop="datePublished">2017-09-24</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/09/24/机器学习-3-1-非监督学习之聚类/">机器学习-3.1-非监督学习之聚类.md</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <script type="text/javascript" src="/Users/zcy/Desktop/study/git/MathJax-master/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h1 id="聚类问题"><a href="#聚类问题" class="headerlink" title="聚类问题"></a>聚类问题</h1><p>本节主要介绍非监督学习的聚类算法。</p>
<h2 id="1-常见的聚类算法"><a href="#1-常见的聚类算法" class="headerlink" title="1 常见的聚类算法"></a>1 常见的聚类算法</h2><p>聚类问题中，我们给定训练集合\(\{ {x^{(1)}},{x^{(2)}},…,{x^{(m)}}\}\)，目的是将训练样本聚合几个类中。由于问题过程中，\(y\)并没有指定，所以这是一个非监督问题。</p>
<h3 id="1-1-k-means"><a href="#1-1-k-means" class="headerlink" title="1.1 k-means"></a>1.1 k-means</h3><p>下面介绍k-means算法。算法的主要流程如下：</p>
<ul>
<li>(1) 随机初始化重心\(\{ {x^{(1)}},{x^{(2)}},…,{x^{(m)}}\}\)(假设有k个分类)</li>
<li>(2) 根据当前的\(\{ {x^{(1)}},{x^{(2)}},…,{x^{(m)}}\}\)，计算距离每个样本距离最近的中心，即为所属类。</li>
<li>(3) 然后根据2中得到新的所属类关系，更新一组新的重心值。</li>
<li>(4) 重复2,3直到某个截止条件。</li>
</ul>
<p>下面我们随机制造以三个点为高斯分布的一组数据，试图从该组数据完成聚类操作，具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> sys</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeData</span><span class="params">()</span>:</span></div><div class="line">  <span class="comment"># 0 make data</span></div><div class="line">  mean_1 = [<span class="number">1</span>, <span class="number">1</span>]</div><div class="line">  mean_2 = [<span class="number">2</span>, <span class="number">2</span>]</div><div class="line">  mean_3 = [<span class="number">1</span>, <span class="number">2</span>]</div><div class="line">  cov = [[<span class="number">0.05</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0.05</span>]]</div><div class="line">  arr1 = np.random.multivariate_normal(mean_1, cov, <span class="number">100</span>)</div><div class="line">  arr2 = np.random.multivariate_normal(mean_2, cov, <span class="number">50</span>)</div><div class="line">  arr3 = np.random.multivariate_normal(mean_3, cov, <span class="number">50</span>)</div><div class="line">  figure, ax = plt.subplots()</div><div class="line">  ax.set_xlim(left=<span class="number">-1</span>, right=<span class="number">4</span>)</div><div class="line">  ax.set_ylim(bottom=<span class="number">-1</span>, top=<span class="number">4</span>)</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr1)):</div><div class="line">    plt.plot(arr1[i][<span class="number">0</span>], arr1[i][<span class="number">1</span>], <span class="string">'b--'</span>, marker=<span class="string">'+'</span>, color=<span class="string">'r'</span>)</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr2)):</div><div class="line">    plt.plot(arr2[i][<span class="number">0</span>], arr2[i][<span class="number">1</span>], <span class="string">'b--'</span>, marker=<span class="string">'o'</span>, color=<span class="string">'g'</span>)</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr3)):</div><div class="line">    plt.plot(arr3[i][<span class="number">0</span>], arr3[i][<span class="number">1</span>], <span class="string">'b--'</span>, marker=<span class="string">'*'</span>, color=<span class="string">'b'</span>)</div><div class="line">  plt.xlabel(<span class="string">"x1"</span>)</div><div class="line">  plt.ylabel(<span class="string">"x2"</span>)</div><div class="line">  <span class="keyword">return</span> np.vstack((arr1,arr2,arr3))</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateSingleLabel</span><span class="params">(x,center)</span>:</span></div><div class="line">  n = len(center)</div><div class="line">  min = sys.maxsize</div><div class="line">  minIndex = <span class="number">-1</span></div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</div><div class="line">    tmp = np.linalg.norm(x - center[i])</div><div class="line">    <span class="keyword">if</span> tmp &lt; min:</div><div class="line">      minIndex=i</div><div class="line">      min = tmp</div><div class="line">  <span class="keyword">return</span> minIndex</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateLable</span><span class="params">(data,label,center)</span>:</span></div><div class="line">  numChanged = <span class="number">0</span>;</div><div class="line">  n=len(data)</div><div class="line">  label_new = np.zeros(n);</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</div><div class="line">    label_new[i] = updateSingleLabel(data[i], center)</div><div class="line">    <span class="keyword">if</span> label_new[i] != label[i]:</div><div class="line">      numChanged = numChanged + <span class="number">1</span></div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</div><div class="line">    label[i] = label_new[i]</div><div class="line">  <span class="keyword">return</span> numChanged;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateCenter</span><span class="params">(data,label,center)</span>:</span></div><div class="line">  newCenter=np.array([[<span class="number">0.0</span>,<span class="number">0.0</span>],[<span class="number">0.0</span>,<span class="number">0.0</span>],[<span class="number">0.0</span>,<span class="number">0.0</span>]])</div><div class="line">  newCenterSum=np.array([<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>])</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(len(data)):</div><div class="line">    <span class="keyword">if</span> label[i]==<span class="number">0</span>:</div><div class="line">      newCenter[<span class="number">0</span>] = newCenter[<span class="number">0</span>] + data[i]</div><div class="line">      newCenterSum[<span class="number">0</span>] = newCenterSum[<span class="number">0</span>] + <span class="number">1</span></div><div class="line">    <span class="keyword">elif</span> label[i] == <span class="number">1</span>:</div><div class="line">      newCenter[<span class="number">1</span>] = newCenter[<span class="number">1</span>] + data[i]</div><div class="line">      newCenterSum[<span class="number">1</span>] = newCenterSum[<span class="number">1</span>] + <span class="number">1</span></div><div class="line">    <span class="keyword">elif</span> label[i] == <span class="number">2</span>:</div><div class="line">      newCenter[<span class="number">2</span>] = newCenter[<span class="number">2</span>] + data[i]</div><div class="line">      newCenterSum[<span class="number">2</span>] = newCenterSum[<span class="number">2</span>] + <span class="number">1</span></div><div class="line">  <span class="keyword">if</span> newCenterSum[<span class="number">0</span>] &gt; <span class="number">0</span> :</div><div class="line">    center[<span class="number">0</span>] = newCenter[<span class="number">0</span>]/newCenterSum[<span class="number">0</span>]</div><div class="line">  <span class="keyword">if</span> newCenterSum[<span class="number">1</span>] &gt; <span class="number">0</span> :</div><div class="line">    center[<span class="number">1</span>] = newCenter[<span class="number">1</span>]/newCenterSum[<span class="number">1</span>]</div><div class="line">  <span class="keyword">if</span> newCenterSum[<span class="number">2</span>] &gt; <span class="number">0</span> :</div><div class="line">    center[<span class="number">2</span>] = newCenter[<span class="number">2</span>]/newCenterSum[<span class="number">2</span>]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">showPic</span><span class="params">(x,label,label1)</span>:</span></div><div class="line">  figure, ax = plt.subplots()</div><div class="line">  ax.set_xlim(left=<span class="number">-1</span>, right=<span class="number">4</span>)</div><div class="line">  ax.set_ylim(bottom=<span class="number">-1</span>, top=<span class="number">4</span>)</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x)):</div><div class="line">    <span class="keyword">if</span> label[i] == <span class="number">0</span>:</div><div class="line">      plt.plot(x[i][<span class="number">0</span>], x[i][<span class="number">1</span>], <span class="string">'b--'</span>, marker=<span class="string">'+'</span>, color=<span class="string">'r'</span>)</div><div class="line">    <span class="keyword">elif</span> label[i] == <span class="number">1</span>:</div><div class="line">      plt.plot(x[i][<span class="number">0</span>], x[i][<span class="number">1</span>], <span class="string">'b--'</span>, marker=<span class="string">'o'</span>, color=<span class="string">'g'</span>)</div><div class="line">    <span class="keyword">elif</span> label[i] == <span class="number">2</span>:</div><div class="line">      plt.plot(x[i][<span class="number">0</span>], x[i][<span class="number">1</span>], <span class="string">'b--'</span>, marker=<span class="string">'*'</span>, color=<span class="string">'b'</span>)</div><div class="line">  figure, ax = plt.subplots()</div><div class="line">  ax.set_xlim(left=<span class="number">-1</span>, right=<span class="number">4</span>)</div><div class="line">  ax.set_ylim(bottom=<span class="number">-1</span>, top=<span class="number">4</span>)</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x)):</div><div class="line">    <span class="keyword">if</span> label1[i] == <span class="number">0</span>:</div><div class="line">      plt.plot(x[i][<span class="number">0</span>], x[i][<span class="number">1</span>], <span class="string">'b--'</span>, marker=<span class="string">'+'</span>, color=<span class="string">'r'</span>)</div><div class="line">    <span class="keyword">elif</span> label1[i] == <span class="number">1</span>:</div><div class="line">      plt.plot(x[i][<span class="number">0</span>], x[i][<span class="number">1</span>], <span class="string">'b--'</span>, marker=<span class="string">'o'</span>, color=<span class="string">'g'</span>)</div><div class="line">    <span class="keyword">elif</span> label1[i] == <span class="number">2</span>:</div><div class="line">      plt.plot(x[i][<span class="number">0</span>], x[i][<span class="number">1</span>], <span class="string">'b--'</span>, marker=<span class="string">'*'</span>, color=<span class="string">'b'</span>)</div><div class="line">  plt.show()</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</div><div class="line"></div><div class="line">  <span class="comment"># 0 make data</span></div><div class="line">  data=makeData()</div><div class="line"></div><div class="line">  <span class="comment"># 1 initial</span></div><div class="line">  n = len(data)</div><div class="line">  label = np.zeros(n)       <span class="comment"># 0,1,2 represent center[0],center[1],center[2]</span></div><div class="line">  center = np.array([[<span class="number">0.0</span>,<span class="number">3.0</span>],[<span class="number">3.0</span>,<span class="number">3.0</span>],[<span class="number">0.0</span>,<span class="number">0.0</span>]])</div><div class="line">  label1 = np.zeros(n)</div><div class="line">  center1 = np.array([[<span class="number">0.0</span>,<span class="number">10.0</span>],[<span class="number">2.0</span>,<span class="number">3.0</span>],[<span class="number">0.5</span>,<span class="number">5.0</span>]])</div><div class="line"></div><div class="line">  <span class="comment"># 2. trainning</span></div><div class="line">  <span class="comment"># 2.1 trainning for good initial</span></div><div class="line">  <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    <span class="comment"># update label</span></div><div class="line">    numChanged = updateLable(data,label,center)</div><div class="line">    <span class="comment"># update center</span></div><div class="line">    updateCenter(data,label,center)</div><div class="line">    <span class="keyword">if</span> numChanged==<span class="number">0</span>:</div><div class="line">      <span class="keyword">break</span></div><div class="line"></div><div class="line">  <span class="comment"># 2.1 trainning for bad initial</span></div><div class="line">  <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    <span class="comment"># update label</span></div><div class="line">    numChanged = updateLable(data,label1,center1)</div><div class="line">    <span class="comment"># update center</span></div><div class="line">    updateCenter(data,label1,center1)</div><div class="line">    <span class="keyword">if</span> numChanged==<span class="number">0</span>:</div><div class="line">      <span class="keyword">break</span></div><div class="line"></div><div class="line">  <span class="comment"># 3. showData</span></div><div class="line">  showPic(data,label,label1)</div></pre></td></tr></table></figure>
<p>下面是随机产生的基于(1，1),(2，2),(1,2)的高斯分布。</p>
<p><img src="/images/机器学习/非监督学习-kmeans实验原始数据.png" width="50%" height="50%" text-align="center/"></p>
<p>然后我们从(0.0,3.0),(3.0,3.0),(0.0,0.0)开始迭代得到如下效果，可以看出结果还是非常理想的。</p>
<p><img src="/images/机器学习/非监督学习-kmeans实验结果1.png" width="50%" height="50%" text-align="center/"></p>
<p>然后我们试图从(0.0,10.0),(2.0,3.0),(0.5,5.0)开始迭代,则会得到这样的结果。</p>
<p><img src="/images/机器学习/非监督学习-kmeans实验结果2.png" width="50%" height="50%" text-align="center/"></p>
<p>可以从上文中看出，k-means对初始值的敏感度很高。对于现实问题，也许我们并不知道训练样本中本身存在多个分类，我们可以设置多个分类，如果某一个分类里面的样本过少，就删除分类，这样就不再依赖于事先知道分类的数量了。</p>
<h3 id="1-2-高斯混合聚类"><a href="#1-2-高斯混合聚类" class="headerlink" title="1.2 高斯混合聚类"></a>1.2 高斯混合聚类</h3><p>假设我们的分类都服从于各自的高斯分布，我们试图从样本中对其分类。该问题与之前的高斯判别分析类似，区别仅仅在于该问题没有样本标签。<br>我们设置z表示样本的距离分类，可以知道他服从一个多项式分布, \({z^{(i)}} \sim Multinomial(\phi )\)，其中。然后已经\(z\)之后，\(x\)服从的是一个高斯分布，\({x^{(i)}}|{z^{(i)}} = j \sim N({\mu _j},{\Sigma _j})\)。</p>
<p>下面直接写出算法过程(具体的算法推导见EM算法小节)：</p>
<ul>
<li>(1)    随机初始化\(\phi\),\(\mu\),\(\Sigma\)。</li>
<li>(2)    遍历样本，计算得\([w _j^{(i)}\),如下：</li>
</ul>

$$w _j^{(i)} = p({z^{(i)}} = j|{x^{(i)}},\phi ,\mu ,\Sigma )$$$$w _j^{(i)} = p({z^{(i)}} = j|{x^{(i)}},\phi ,\mu ,\Sigma ) = \frac{{p({z^{(i)}} = j,{x^{(i)}})}}{{p({x^{(i)}})}} = \frac{{p({z^{(i)}} = j,{x^{(i)}})}}{{\sum\limits _{j = 1}^k {p({x^{(i)}},{z^{(i)}} = j)} }}$$
<ul>
<li>(3)    根据计算得到的\(w\)重新更新各个分类的分布，如下：</li>
</ul>

$${\phi _j} = \frac{1}{m}\sum\limits _{i = 1}^m {1\{ {z^{(i)}} = j\} } $$
$${\mu _j} = \frac{{\sum\limits _{i = 1}^m {1\{ {z^{(i)}} = j\} } {x^{(i)}}}}{{\sum\limits _{i = 1}^m {1\{ {z^{(i)}} = j\} } }}$$

$${\Sigma _j} = \frac{{\sum\limits _{i = 1}^m {1\{ {z^{(i)}} = j\} } ({x^{(i)}} - {\mu _j}){{({x^{(i)}} - {\mu _j})}^T}}}{{\sum\limits _{i = 1}^m {1\{ {z^{(i)}} = j\} } }}$$

<ul>
<li>(4)    重复2,3知道达到截止条件。</li>
</ul>
<p>下面我们制作一组由三个高斯分布组成的样本数据，对其进行聚类。代码如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># coding=utf-8</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> multivariate_normal</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_data</span><span class="params">()</span>:</span></div><div class="line">  mean_1 = [<span class="number">1</span>,<span class="number">1</span>]</div><div class="line">  mean_2 = [<span class="number">4</span>,<span class="number">4</span>]</div><div class="line">  mean_3 = [<span class="number">1</span>,<span class="number">4</span>]</div><div class="line">  cov1 = [[<span class="number">0.1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0.1</span>]]</div><div class="line">  cov2 = [[<span class="number">0.2</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0.2</span>]]</div><div class="line">  cov3 = [[<span class="number">0.6</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0.6</span>]]</div><div class="line">  arr1 = np.random.multivariate_normal(mean_1, cov1, <span class="number">100</span>)</div><div class="line">  arr2 = np.random.multivariate_normal(mean_2, cov2, <span class="number">50</span>)</div><div class="line">  arr3 = np.random.multivariate_normal(mean_3, cov3, <span class="number">50</span>)</div><div class="line">  figure, ax = plt.subplots()</div><div class="line">  ax.set_xlim(left=<span class="number">-4</span>, right=<span class="number">8</span>)</div><div class="line">  ax.set_ylim(bottom=<span class="number">-4</span>, top=<span class="number">8</span>)</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr1)):</div><div class="line">    plt.plot(arr1[i][<span class="number">0</span>], arr1[i][<span class="number">1</span>], <span class="string">'b--'</span>, marker=<span class="string">'+'</span>, color=<span class="string">'r'</span>)</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr2)):</div><div class="line">    plt.plot(arr2[i][<span class="number">0</span>], arr2[i][<span class="number">1</span>], <span class="string">'b--'</span>, marker=<span class="string">'o'</span>, color=<span class="string">'g'</span>)</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr3)):</div><div class="line">    plt.plot(arr3[i][<span class="number">0</span>], arr2[i][<span class="number">1</span>], <span class="string">'b--'</span>, marker=<span class="string">'*'</span>, color=<span class="string">'b'</span>)</div><div class="line">  plt.xlabel(<span class="string">"x1"</span>)</div><div class="line">  plt.ylabel(<span class="string">"x2"</span>)</div><div class="line">  plt.plot()</div><div class="line">  plt.title(<span class="string">"sample"</span>)</div><div class="line">  <span class="comment">#plt.show()</span></div><div class="line">  <span class="keyword">return</span> np.vstack((arr1, arr2, arr3))</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">updatePhi</span><span class="params">(w)</span>:</span></div><div class="line">  n1 = len(w)</div><div class="line">  phi=np.zeros(n1)</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(n1):</div><div class="line">    sum = <span class="number">0</span></div><div class="line">    n2 = len(w[i])</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n2):</div><div class="line">      sum = sum + w[i][j]</div><div class="line">    phi[i] = sum/n2</div><div class="line">  <span class="keyword">return</span> phi</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateW</span><span class="params">(data,w,phi,mu,sigma)</span>:</span></div><div class="line">  n1=len(w)           <span class="comment"># 3</span></div><div class="line">  n2 = len(w[<span class="number">0</span>])      <span class="comment"># 200</span></div><div class="line">  var = []</div><div class="line">  <span class="keyword">for</span> k <span class="keyword">in</span> range(n1):</div><div class="line">    var.append(multivariate_normal(mean=mu[k].tolist(), cov=sigma[k].tolist()))</div><div class="line"></div><div class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> range(n2):</div><div class="line">    sum = <span class="number">0</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n1):</div><div class="line">      sum = sum + var[i].pdf(data[j])*phi[i]</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n1):</div><div class="line">      w[i][j] = var[i].pdf(data[j])*phi[i]/sum</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateMu</span><span class="params">(data,w,mu)</span>:</span></div><div class="line">  changed=<span class="keyword">False</span></div><div class="line">  n1 = len(w)  <span class="comment"># 3</span></div><div class="line">  n2 = len(w[<span class="number">0</span>])  <span class="comment"># 200</span></div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(n1):</div><div class="line">    sumW = <span class="number">0.0</span></div><div class="line">    sumX = np.array([<span class="number">0.0</span>,<span class="number">0.0</span>])</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n2):</div><div class="line">      sumW = sumW + w[i][j]</div><div class="line">      sumX = sumX + w[i][j]*data[j]</div><div class="line">    mu_new = sumX / sumW</div><div class="line">    <span class="keyword">if</span> np.dot(mu_new-mu[i],mu_new-mu[i]) &gt; <span class="number">0.001</span>:</div><div class="line">      changed = <span class="keyword">True</span></div><div class="line">    mu[i] = mu_new</div><div class="line">  <span class="keyword">return</span> changed</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateSigma</span><span class="params">(data,w,mu,sigma)</span>:</span></div><div class="line">  n1 = len(w)  <span class="comment"># 3</span></div><div class="line">  n2 = len(w[<span class="number">0</span>])  <span class="comment"># 200</span></div><div class="line">  sum=np.array([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0</span>]])</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(n1):</div><div class="line">    sumW = <span class="number">0.0</span></div><div class="line">    sumX = np.array([<span class="number">0.0</span>,<span class="number">0.0</span>])</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n2):</div><div class="line">      sumW = sumW + w[i][j]</div><div class="line">      z0 = np.array([data[j] - mu[i]])</div><div class="line">      z0T = np.array([data[j] - mu[i]]).transpose()</div><div class="line">      sumX = sumX + w[i][j]*np.dot(z0T, z0)</div><div class="line">    sigma[i] = sumX/sumW</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(data,mu,sigma)</span>:</span></div><div class="line">  n1=len(w)           <span class="comment"># 3</span></div><div class="line">  n2 = len(w[<span class="number">0</span>])      <span class="comment"># 200</span></div><div class="line">  var=[]</div><div class="line">  <span class="keyword">for</span> k <span class="keyword">in</span> range(n1):</div><div class="line">    var.append(multivariate_normal(mean=mu[k].tolist(), cov=sigma[k].tolist()))</div><div class="line"></div><div class="line">  figure, ax = plt.subplots()</div><div class="line">  ax.set_xlim(left=<span class="number">-4</span>, right=<span class="number">8</span>)</div><div class="line">  ax.set_ylim(bottom=<span class="number">-4</span>, top=<span class="number">8</span>)</div><div class="line"></div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(n2):</div><div class="line">    tmp_arr=np.array([])</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n1):</div><div class="line">      tmp_arr=np.append(tmp_arr,var[j].pdf(data[i]))</div><div class="line">    index = tmp_arr.argmax()</div><div class="line">    <span class="keyword">if</span> index == <span class="number">0</span>:</div><div class="line">      plt.plot(data[i][<span class="number">0</span>], data[i][<span class="number">1</span>], <span class="string">'b--'</span>, marker=<span class="string">'+'</span>, color=<span class="string">'r'</span>)</div><div class="line">    <span class="keyword">elif</span> index == <span class="number">1</span>:</div><div class="line">      plt.plot(data[i][<span class="number">0</span>], data[i][<span class="number">1</span>], <span class="string">'b--'</span>, marker=<span class="string">'o'</span>, color=<span class="string">'g'</span>)</div><div class="line">    <span class="keyword">elif</span> index == <span class="number">2</span>:</div><div class="line">      plt.plot(data[i][<span class="number">0</span>], data[i][<span class="number">1</span>], <span class="string">'b--'</span>, marker=<span class="string">'o'</span>, color=<span class="string">'b'</span>)</div><div class="line">  plt.xlabel(<span class="string">"x1"</span>)</div><div class="line">  plt.ylabel(<span class="string">"x2"</span>)</div><div class="line">  plt.title(<span class="string">"result"</span>)</div><div class="line">  plt.plot()</div><div class="line">  plt.show()</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">  <span class="comment"># 1 make data</span></div><div class="line">  classN=<span class="number">3</span></div><div class="line">  data=make_data()</div><div class="line">  n=len(data)</div><div class="line">  <span class="keyword">for</span> k <span class="keyword">in</span> range(n):</div><div class="line">    w = np.array([np.zeros(n), np.zeros(n), np.zeros(n)])</div><div class="line">  <span class="comment"># w = array[classN][n] =array[3][200]</span></div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(classN):</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</div><div class="line">      w[i][j]= <span class="number">1.0</span>/classN</div><div class="line"></div><div class="line">  phi=updatePhi(w)</div><div class="line">  mu = np.array([[<span class="number">6.0</span>, <span class="number">6.0</span>], [<span class="number">4.0</span>, <span class="number">-1.0</span>], [<span class="number">-2.0</span>, <span class="number">2.0</span>]])</div><div class="line">  sigma=np.array([[[<span class="number">0.1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0.1</span>]],[[<span class="number">0.1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0.1</span>]],[[<span class="number">0.1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0.1</span>]]])</div><div class="line"></div><div class="line">  <span class="comment"># 2 training</span></div><div class="line">  <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    updateW(data,w,phi,mu,sigma)</div><div class="line">    updatePhi(w)</div><div class="line">    updateSigma(data,w,mu,sigma)</div><div class="line">    changed = updateMu(data,w,mu)</div><div class="line">    <span class="keyword">if</span> changed == <span class="keyword">False</span>:</div><div class="line">      print(<span class="string">"迭代完成"</span>)</div><div class="line">      <span class="keyword">break</span></div><div class="line"></div><div class="line">  <span class="comment"># 3 show</span></div><div class="line">  print(<span class="string">"mu="</span>,mu,<span class="string">", sigma="</span>,sigma)</div><div class="line">  classify(data,mu,sigma)</div></pre></td></tr></table></figure>
<p>我们分别以[6.0, 6.0], [4.0, -1.0], [-2.0, 2.0]为均值，以[[0.1,0],[0,0.1]],[[0.1,0],[0,0.1]],[[0.1,0],[0,0.1]]为标准差生成一组高斯分布如下：</p>
<p><img src="/images/机器学习/非监督学习-高斯混合聚类原始数据.png" width="50%" height="50%" text-align="center/"></p>
<p>然后通过训练的到的训练均值分别为[4.00729834, 3.99848889], [1.01089497,  1.05225006], [0.92949217, 4.18380895]]。标准差为[[0.23196114,-0.01896477],[-0.01896477, 0.17165715]], [[0.10205901, 0.00157192], [0.00157192,  0.11477843]], [[0.7010517, 0.04783335], [0.04783335, 0.51147277]]。具体结果如下:</p>
<p><img src="/images/机器学习/非监督学习-高斯混合聚类实验结果.png" width="50%" height="50%" text-align="center/"></p>
<h3 id="2-1-Jensen不等式"><a href="#2-1-Jensen不等式" class="headerlink" title="2.1 Jensen不等式"></a>2.1 Jensen不等式</h3><p>对于一个严格凸函数,即\(f’’(x) \geqslant 0\)，我们容易得到下式:</p>
$$E[f(x)] \geqslant f(E[x])$$

<p>当且仅当x为常数的时候，上式等号成立。对于严格凹函数，则正好相反。</p>
<h3 id="2-2-EM算法模型建立"><a href="#2-2-EM算法模型建立" class="headerlink" title="2.2 EM算法模型建立"></a>2.2 EM算法模型建立</h3><p>假定我们的数据有\(k\)个分类。我们聚类的目标是，样本在自己分类中出现的概率最大。或者换句话说，让其在所属分类的分布(可以用高斯分类假想该问题)中出现的概率最大。可是对于非监督学习问题，我们不知道具体的分类。因此，我们可以将模型假定为找到给定参数\(\theta\)对应分布，是的\(x\)在分布中出现的概率足够大，这说明(\theta\)对应的分布能够充分的表示某一组分类。因此可以构造如下的最大释然函数：</p>

$$\ell (\theta ) = \sum\limits _{i = 1}^m {\ln p({x^{(i)}};\theta )} $$
<p>进入推导最大释然函数：</p>
$$\ell (\theta ) = \sum\limits _{i = 1}^m {\ln \sum\limits _{j = 1}^k {p({x^{(i)}},{z^{(j)}};\theta )} }  = \sum\limits _{i = 1}^m {\ln \sum\limits _{j = 1}^k {{Q _i}({z^{(i)}} = j)\frac{{p({x^{(i)}},{z^{(i)}} = j;\theta )}}{{{Q _i}({z^{(i)}} = j)}}} } $$

<p>上面\(k\)为分类的个数。对上面的\({Q _i}\)为一个概率分布，有:</p>

$$\sum\limits _{j=1} ^k {{Q _i}({z^{(i)}}=j)}=1$$

<p>对于\(f(x) = \ln x\),我们知道\(f’’(x) =  - \frac{1}{x^2}\),可知其为一个严格凹函数。上式可以写成：</p>

$$\ell (\theta ) = \sum\limits _{i = 1}^m {f(E[\frac{{p({x^{(i)}},{z^{(i)}} = j;\theta )}}{{{Q _i}({z^{(i)}} = j)}}])}$$

<p>根据Jensen不等式，我们有:</p>
$$\ell (\theta ) = \sum\limits _{i = 1}^m {f(E[\frac{p({x^{(i)},{z^{(i)}};\theta )}}{{Q _i}({z^{(i)}} = j)}])} \geqslant \sum\limits _{i = 1}^m {E[f(\frac{p({x^{(i)}},{z^{(i)}} = j;\theta )}{{Q _i}({z^{(i)}} = j)})]} = \sum\limits _{i = 1}^m {\sum\limits _{j = 1}^k {{Q _i}({z^{(i)}} = j)\ln (\frac{p({x^{(i)}},{z^{(i)}} = j;\theta )}{{Q _i}({z^{(i)}} = j)})} }$$

<p>因此我们得到最大释然函数的下确定，即为上式子中后面的部分。我们只要保证下确定随着迭代的方向单调递增即可，具体的要保证\(\ell ({\theta ^{(t + 1)}}) \geqslant \ell ({\theta ^{(t)}})\)。这里我们设\(low(\theta )\)是已知\({Q _i}\)的情况下，最大释然函数的下确定关于\(\theta\)的函数。我们看如下公式:</p>

$$\ell ({\theta ^{(t + 1)}}) \geqslant low({\theta ^{(t + 1)}}) \geqslant low({\theta ^{(t)}}) = \ell ({\theta ^{(t)}})$$
<p>事实上，我们只要保证上面的式子我们就可以保证迭代方向是正确的。其中，第一个不等号是必然成立的。那我们分头来构造条件是后面的式子成立。<br>首先如果保证最后一个等号的成立，根据Jensen不等式，只有自变量为常数才能事等号成立，这样我们设置如下式子(其中\(c\)为常数):</p>

$$\frac{{p({x^{(i)}},{z^{(i)}} = j;\theta )}}{{{Q _i}({z^{(i)}} = j)}} = c$$

<p>然后对上面的式子按照分类累积求和得:</p>
$$\sum\limits _{j = 1}^k {p({x^{(i)}},{z^{(i)}} = j;\theta )}  = c\sum\limits _{j = 1}^k {{Q _i}({z^{(i)}} = j)}  = c$$
<p>因此得:</p>

$${Q _i}({z^{(i)}} = j) = \frac{{p({x^{(i)}},{z^{(i)}} = j;\theta )}}{{\sum\limits _{j = 1}^k {p({x^{(i)}},{z^{(i)}} = j;\theta )} }} = p({z^{(i)}}|{x^{(i)}};\theta)$$

<p>这样我们完成EM算法的E步骤。然后解决中间大于等于号的问题。这个就比较好解决了，我们只需要计算下确定函数关于\(\theta\)求最大值，最大值对应的参数，可保证不等号的成立，即为迭代的正确方向。具体公式如下：</p>

$$\theta  = \arg {\max _\theta }\sum\limits _{i = 1}^m {\sum\limits _{j = 1}^k {{Q _i}({z^{(i)}} = j)\ln \frac{{p({x^{(i)}},{z^{(i)}} = j;\theta )}}{{{Q _i}({z^{(i)}} = j)}}} }$$
<p>综上，我们来重新整理一下EM算法，具体如下：</p>
<ul>
<li>(1) 初始化相关参数</li>
<li>(2) E步骤:计算\({Q _i}\)，如下：</li>
</ul>

$${Q _i}({z^{(i)}} = j) = p({z^{(i)}}|{x^{(i)}};\theta)$$
<ul>
<li>(3) M步骤：更新参数\(\theta\)，如下：</li>
</ul>

$$\theta  = \arg {\max _\theta }\sum\limits _{i = 1}^m {\sum\limits _{j = 1}^k {{Q _i}({z^{(i)}} = j)\ln \frac{{p({x^{(i)}},{z^{(i)}} = j;\theta )}}{{{Q _i}({z^{(i)}} = j)}}} } $$
<p>(4)    重复2,3直到截止条件</p>
<p>注: 上面的例子是不断的更新迭代\({Q _i}\)和\(\theta\)。我们完全可以使用梯度上升发不断从各个方向更新\({Q _i}\)和\(\theta\)，来完成最大值的逼近。</p>
<h3 id="2-3-混合高斯分布的公式推导"><a href="#2-3-混合高斯分布的公式推导" class="headerlink" title="2.3 混合高斯分布的公式推导"></a>2.3 混合高斯分布的公式推导</h3><p>对比之前的内容，我们可以发现混合高斯分布的聚类问题为EM算法的一个特例。可以通过通用的EM算法来证明，下面我们来证明这一过程。</p>
<p>注： 这里只简单地提示计算，不展开了，因为与之前的高斯判别分析类似。<br>根据上一节，我们已得到E步骤的公式：</p>

$$w _j^{(i)}={Q _i}({z^{(i)}}=j) = p({z^{(i)}}|{x^{(i)}};\theta )$$

<p>然后对于M步骤，我们将\({Q _i}\)为一个已知值的方式对\(\theta\)进行求导，从而求得下确定的最大值，记为下一个迭代值。将最大释然函数展开记为如下函数：</p>
$$\ell (\theta ) = \sum\limits _{i = 1}^m {\sum\limits _{j = 1}^k {{Q _i}({z^{(i)}} = j)\ln (\frac{{p({x^{(i)}},{z^{(i)}} = j;\theta )}}{{{Q _i}({z^{(i)}} = j)}})} }  = \sum\limits _{i = 1}^m {\sum\limits _{j = 1}^k {w _j^{(i)}\ln (\frac{{\frac{1}{{{{(2\pi )}^{\frac{n}{2}}}|\Sigma {|^{\frac{1}{2}}}}}\exp ( - \frac{1}{2}{{({x^{(i)}} - {\mu _j})}^T}{\Sigma ^{ - 1}}({x^{(i)}} - {\mu _j})){\phi _j}}}{{w _j^{(i)}}})} } $$
<p>然后对\(\phi\),\(\mu\),\(\Sigma\), 即可得到M步骤的更新公式。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-机器学习-2-7-监督学习之支持向量机" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2017/09/08/机器学习-2-7-监督学习之支持向量机/" class="article-date">
      <time datetime="2017-09-08T08:18:25.000Z" itemprop="datePublished">2017-09-08</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/09/08/机器学习-2-7-监督学习之支持向量机/">机器学习-2.7-监督学习之支持向量机</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <script type="text/javascript" src="/Users/zcy/Desktop/study/git/MathJax-master/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h1 id="支持向量机-SVM"><a href="#支持向量机-SVM" class="headerlink" title="支持向量机(SVM)"></a>支持向量机(SVM)</h1><h2 id="1-Margins-间隔"><a href="#1-Margins-间隔" class="headerlink" title="1 Margins(间隔)"></a>1 Margins(间隔)</h2><p>对于一个逻辑回归问题，\(p(y = 1|x, \theta )\)可以通过下面的式子表示：</p>

$${h _ \theta }(x) = g({\theta ^T}x)$$

<p>\(\theta^Tx\)越大， \(g({\theta ^T}x)\)就越大， 我们就有可高的信心认为\(y=1\)。 同理假如\({ h _\theta }(x) &lt;&lt; 0\), 我们就有很强的信息认为\(y=0\)。 (注: &gt;&gt; 表示远大于，&lt;&lt;表示远小于)</p>
<p><img src="/images/机器学习/监督学习-支持向量机图1.png" width="50%" height="50%" text-align="center/"></p>
<p>上面这张图，x为正样本，o为负样本。中间的直线是分割超平面。上面图中，我们可以知道对于样本A，我们有很强的信心认为\(y=1\)。然后对于C却接近边界，如果稍微改变分割超平面，就可能导致\(y=0\)。因此，越远离分割超平面的样本，我们越有信息获得他的分类。</p>
<h2 id="2-函数化与几何间隔"><a href="#2-函数化与几何间隔" class="headerlink" title="2 函数化与几何间隔"></a>2 函数化与几何间隔</h2><p>区别与之前的逻辑回归。引入支持向量机，我们需要重新定义符号。对于SVM问题, \(y \in \{  - 1,1\} \),定义函数为:</p>
$${h_{w,b}}(x) = g({w^T}x + b)$$
<p>对于\(z = {w^T}x + b &gt; 0\), 有\({h _ \theta }(x) = 1\)。 否则\({h _ \theta }(x) = -1\)。</p>
<p>函数化几何间隔，有如下公式:</p>

$${\hat \gamma ^{(i)}} = {y^{(i)}}({w^T}x + b)$$
    
<p>上式很容易理解，对于正样本\(y=1\),对于负样本\(y=-1\)。因此可以得到\({\hat \gamma ^{(i)}}\)为一个正值。我们可以就可以定义其为几何间隔，可以理解为距离。根据上一节的分析，一个超平面对样本分割的好坏，取决于距离超平面最近的样本。因为我们定义最小间隔如下：</p>

$$\hat \gamma  = {\min _{i = 1,...,m}}{\hat \gamma ^{(i)}}$$
<p><img src="/images/机器学习/监督学习-支持向量机图2.png" width="50%" height="50%" text-align="center/"></p>
<p>根据上图，我们知道向量\(w\)与超平面\({w^T}x + b = 0\)垂直。</p>
<blockquote>
<p>在超平面找任意两点做直线，易证其与法向量內积为零。因此可知法向量与平面垂直。</p>
</blockquote>
<p>我们可以得知B点的坐标为\({x^{(i)}} - {r^{(i)}}\frac{w}{||w||}\)，我们将坐标代入超平面方程，有:</p>

$${w^T}({x^{(i)}} - {r^{(i)}}\frac{w}{||w||}) + b = 0$$
<p>由于知道\(||w|| = {w^T}w\)，有：</p>

$${r^{(i)}} = {(\frac{w}{||w||})^T}{x^{(i)}} + \frac{b}{||w||}$$
<p>上面公式对应于正样本，对于负样本则需要加一个符号。因此我们可以综合得到如下公式:</p>

$${r^{(i)}} = {y^{(i)}}({(\frac{w}{||w||})^T}{x^{(i)}} + \frac{b}{||w||})$$
<p>对于公式\({\hat \gamma ^{(i)}} = {y^{(i)}}({w^T}x + b)\)中，\(||w|| = 1\)的时候为几何间隔。上面的式子实际上就是定义了几何间隔。我们通过定义了向量\(w\)的尺度，这样不会因为选择w比例的不同而产生不同间隔的问题。</p>
<h2 id="3-最优间隔分类器"><a href="#3-最优间隔分类器" class="headerlink" title="3 最优间隔分类器"></a>3 最优间隔分类器</h2><blockquote>
<p>以下假定我们样本数据是严格线性可分的，否则通过间隔最大化来计算就毫无意义了。</p>
</blockquote>
<p>根据前面的分析，我们只需要找到超平面，使得间隔\(\hat \gamma \)最大即可。因此我们可以得到如下最优化问题:</p>
$$\begin{gathered}   & {\max _{w,b}}\gamma  \hfill \\\  s.t.: & {y^{(i)}}({w^T}{x^{(i)}} + b) \geqslant \gamma , & i = 1,...,m \hfill \\\   & ||w|| = 1 \hfill \\\ \end{gathered} $$
<p>由于\(||w|| - 1 = 0\)不是凸函数，所以需要转化问题。<br>注&gt; 为什么凸函数如此重要？因为对于一个凸优化问题，认为局部极小值点必为全局极小值点。对\(f(\lambda x + (1 - \lambda )y) \leqslant \lambda f(x) + (1 - \lambda )f(y)\),其中\(0 \leqslant \lambda  \leqslant 1\),则\(f(x)\)为凸函数。可以理解为类似于\(y = {x^2}\)的图形。然而对于\(||w|| - 1 = 0\)，具体为\(\sqrt {w _1^2 + w _2^2 + … + w _n^2}  = 1\)，二维的情景可以理解为一个圆，三维的话则为一个球。几何图形中，可以发现对于球或圆的上半部分正好与凸函数相反，因此不是凸函数。可以代入公式证明。<br>进一步转化问题，如下：</p>

$$\begin{gathered}   & {\max _{w,b}}\frac{\gamma }{||w||} \hfill \\\  s.t.: & {y^{(i)}}({w^T}{x^{(i)}} + b) \geqslant \gamma , & i = 1,...,m \hfill \\\\end{gathered} $$
<p>我们知道\(\gamma \)大小，取决于\(w\)和\(b\)的尺度，但是\(w\)和\(b\)的尺度的改变不会影响分配效果。因此我们固定\(\gamma \)为1。将问题转化为:</p>

$$\begin{gathered}   & {\min _{w,b}}\frac{1}{2}||w|{|^2} \hfill \\  s.t.: & {y^{(i)}}({w^T}{x^{(i)}} + b) \geqslant 1, & i = 1,...,m \hfill \\ \end{gathered}$$
<p>事实上对于这个问题，我们可以换一种几何意义的解释。如下图所示，我们需要找到\({w^T}x + b = 1\)和\({w^T}x + b = -1\)能够有效分割样本，并且保证是两个超平面之间间隔最大，即使\(\frac{2}{||w||}\)最大，也就意味着使\(\frac{1}{2}||w|{|^2}\)最小。同样我们可以得到上面的优化问题。</p>
<p><img src="/images/机器学习/监督学习-支持向量机图3.png" width="50%" height="50%" text-align="center/"></p>
<p>另外，我们距离超平面最近的点，即\(\gamma  = 1\)的点，我们称之为支持向量。</p>
<h2 id="4-拉格朗日对偶问题"><a href="#4-拉格朗日对偶问题" class="headerlink" title="4 拉格朗日对偶问题"></a>4 拉格朗日对偶问题</h2><p>在对计算上一级得到的优化问题之前，我们介绍一下拉格朗日对偶问题与KKT条件，以便于更容易解决问题。考虑一个通用的优化问题，如下：</p>
$$\begin{gathered}   & {\min _x}f(x) \hfill \\\  s.t.: & {g _i}(x) \leqslant 0, & i = 1,...,k \hfill \\\   & {h _j}(x) = 0, & j = 1,...,l \hfill \\\ \end{gathered} $$

<p>然后得到拉格朗日函数，如下：</p>
$$L(x,\alpha ,\beta ) = f(x) + \sum\limits _{i = 1}^k {{\alpha _i}{g _i}(x)}  + \sum\limits _{j = 1}^l {{\beta _i}{h _i}(x)} $$$${\alpha _i} \geqslant 0$$
<p>对于我们的原始问题如何用拉格朗日函数表达呢？我们知道上面的拉格朗日后面两项的最大值为零。因此我们就可以将原始问题转化为以\({\alpha _i}\)和\({\beta _i}\)为参数情况下，求拉格朗日函数的最大值。具体转化为如下形式:</p>

$${\theta _P}(x) = {\max _{\alpha ,\beta ;{\alpha _i} \geqslant 0}}L(x,\alpha ,\beta )$$
<p>然后我们上面的式子，关于x取极小值，就与目标问题一致了。原始为的最优解最终可以通过如下方式表述:</p>

$${p^*} = {\min _x}{\theta _P}(x) = {\min _x}{\max _{\alpha ,\beta ;{\alpha _i} \geqslant 0}}L(x,\alpha ,\beta )$$
<p>我们这里可以写出其对偶问题：</p>
$${\theta _D}(\alpha ,\beta ) = {\min _x}L(x,\alpha ,\beta )$$

<p>对偶问题的最优解如下：</p>
$${d^*} = {\max _{\alpha ,\beta ;{\alpha _i} \geqslant 0}}{\theta _D}(\alpha ,\beta ) = {\max _{\alpha ,\beta ;{\alpha _i} \geqslant 0}}{\min _x}L(x,\alpha ,\beta )$$
<p>我们知道maxmin&lt;minmax,所以我们得到如下式子:</p>
$${d^*} = {\max _{\alpha ,\beta ;{\alpha _i} \geqslant 0}}{\min _x}L(x,\alpha ,\beta ) \leqslant {\min _x}{\max _{\alpha ,\beta ;{\alpha _i} \geqslant 0}}L(x,\alpha ,\beta ) = {p^\*}$$
<p>对于上面的式子只要\(f _i\)和\(g _i\)为凸函数，\(h _i\)为仿射函数，即可使等式两端相等。</p>
<blockquote>
<p>仿射函数为线性函数加截距。<br>另外对于该问题的最优值必满足KKT条件。另外，上述拉格朗日函数对相应参数需要取得极值。最终得到如下条件:</p>
</blockquote>

$$\frac{\partial }{{\partial {x _i}}}L({x^*},{\alpha^*},{\beta^*}) = 0$$
$$\frac{\partial }{{\partial \beta }}L({x^*},{\alpha ^*},{\beta ^*}) = 0$$$${\alpha ^*}{g _i}(x) = 0$$
$${g _i}(x) \leqslant 0$$$${\alpha ^*} \geqslant 0$$
<blockquote>
<p>KKT条件的原理暂时不深入，目前处于应用阶段，有时间再考虑具体原理。</p>
</blockquote>
<h1 id="5-最优边界分类器"><a href="#5-最优边界分类器" class="headerlink" title="5 最优边界分类器"></a>5 最优边界分类器</h1><p>将我们的优化问题转化为如下标准型：</p>
$$\begin{gathered}   & {\min _{w,b}}\frac{1}{2}||w|{|^2} \hfill \\\  s.t.: &  - {y^{(i)}}({w^T}{x^{(i)}} + b) + 1 \leqslant 0, & i = 1,...,n \hfill \\\\end{gathered} $$
<p>根据前面的说明，我们可以通过解对偶问题最优解来获得该问题的最优解。<br>首先写出拉格朗日对偶函数：</p>
$$L(w,b,\alpha ) = \frac{1}{2}||w|{|^2} - \sum\limits _{i = 1}^m {{\alpha _i}({y^{(i)}}({w^T}{x^{(i)}} + b) - 1)} $$
<p>参数上一节的公式，这里\(w\)和\(b\)对应于\(x\)。我们需要关于\(w\)，\(b\)求极小值，然后求得的极值点代入拉格朗日函数，然后求转化后的拉格朗日函数的极大值即可。</p>
$$\frac{\partial }{{\partial w}}L(w,b,\alpha ) = w - \sum\limits _{i = 1}^m {{\alpha _i}{y^{(i)}}{x^{(i)}}}  = 0$$$$\frac{\partial }{{\partial b}}L(w,b,\alpha ) = \sum\limits _{i = 1}^m {{\alpha _i}{y^{(i)}}}  = 0$$
<p>得到\(w = \sum\limits _{i = 1}^m {\alpha _i}{y^{(i)}{x^{(i)}}} \)，\(\sum\limits _{i = 1}^m {\alpha _i}{y^{(i)}}  = 0\)。然后将其代入拉格朗日函数：</p>

$$L(\alpha ) = \frac{1}{2}{w^T}w - \sum\limits _{i = 1}^m {{\alpha _i}({y^{(i)}}({w^T}{x^{(i)}} + b) - 1)} $$
$$L(\alpha ) =  - \frac{1}{2}{(\sum\limits _{i = 1}^m {{\alpha _i}{y^{(i)}}{x^{(i)}}} )^T}(\sum\limits _{i = 1}^m {{\alpha _i}{y^{(i)}}{x^{(i)}}} ) + \sum\limits _{i = 1}^m {{\alpha _i}} $$

<p>由于\({\alpha _i}\)和\({y^{(i)}}\)为变量，实际上面的式子就是一个\((Ax + By + Cz)(Ax + By + Cz)\)的问题。因此可以归纳为如下公式：</p>
$$L(\alpha ) =  - \frac{1}{2}\sum\limits _{i = 1}^m {\sum\limits _{j = 1}^m {{\alpha _i}{\alpha _j}{y^{(i)}}{y^{(j)}} < {x^{(i)}},{x^{(j)}} > } }  + \sum\limits _{i = 1}^m {{\alpha _i}} $$
<p>问题就转化为：</p>

$$\begin{gathered}   & {\max _\alpha }L(\alpha ) =  - \frac{1}{2}\sum\limits _{i = 1}^m {\sum\limits _{j = 1}^m {{\alpha _i}{\alpha _j}{y^{(i)}}{y^{(j)}} < {x^{(i)}},{x^{(j)}} > } }  + \sum\limits _{i = 1}^m {{\alpha _i}}  \hfill \\\  s.t.: & {\alpha _i} \geqslant 0 \hfill \\\   & \sum\limits _{i = 1}^n {{\alpha _i}{y^{(i)}}}  = 0 \hfill \\\\end{gathered} $$
<p>由于我们知道支持向量的间隔必须为1，因此我们可以根据其计算\(b\)。设支持向量的集合为S,对属于结合S的样本有\({y^{(i)}}({w^T}{x^{(i)}} + b) = 1\)。由于\(w = \sum\limits _{i = 1}^m {\alpha _i}{y^{(i)}}{x^{(i)}} \)，又由于对所有的非支持向量，有\({\alpha _i} = 0\)。因此我们可以综合均值得到：</p>
$$b = \frac{1}{{|S|}}\sum\limits _{i = 1}^S {({y^{(i)}} - \sum\limits _{j = 1}^S {{\alpha _j}{y^{(j)}} < {x^{(i)}},{x^{(j)}} > } )} $$

<h2 id="6-正则化"><a href="#6-正则化" class="headerlink" title="6 正则化"></a>6 正则化</h2><p>关于之前的问题我们假定样本严格可分。但是实际上需要容忍一些误差。因此我们将公式修正为如下形式(C为常数)：</p>

$$\begin{gathered}   & {\min _{w,b,\xi }}\frac{1}{2}||w|{|^2} + C\sum\limits _{i = 1}^m {{\xi _i}}  \hfill \\\  s.t.: & {y^{(i)}}({w^T}{x^{(i)}} + b) \geqslant 1 - {\xi _i} &  & i = 1,...,m \hfill \\\   & {\xi _i} \geqslant 0 &  &  &  & i = 1,...,m \hfill \\\\end{gathered} $$
<p>我们允许\({y^{(i)}}({w^T}{x^{(i)}} + b)\)小于1，但是不希望小太多。所以，我们需要保证\({\xi _i}\)的总和尽可能小，因此得上面的式子。然后我们可以得到拉格朗日公式如下：</p>
$$L(w,b,\xi ,\alpha ,r) = \frac{1}{2}||w|{|^2} + C\sum\limits _{i = 1}^m {{\xi _i}}  - \sum\limits _{i = 1}^m {{\alpha _i}({y^{(i)}}({w^T}{x^{(i)}} + b) - 1 + {\xi _i})}  - \sum\limits _{i = 1}^m {{r _i}{\xi _i}} $$

<p>对\(w\)，\(b\)，\(\xi \), 求导得:</p>
$$\frac{\partial }{{\partial w}}L(w,b,\xi ,\alpha ,r) = w - \sum\limits _{i = 1}^m {{\alpha _i}{y^{(i)}}{x^{(i)}}}  = 0$$
$$\frac{\partial }{{\partial b}}L(w,b,\xi ,\alpha ,r) =  - \sum\limits _{i = 1}^m {{\alpha _i}{y^{(i)}}}  = 0$$
$$\frac{\partial }{{\partial {\xi _i}}}L(w,b,\xi ,\alpha ,r) = C - {r _i} - {\alpha _i} = 0$$
<p>可以得到\(w = \sum\limits _{i = 1}^m {\alpha _i}{y^{(i)}}{x^{(i)}}\),\(\sum\limits _{i = 1}^m {\alpha _i}{y^{(i)}} = 0\),\(C = {r _i} + {\alpha _i}\)。其中我们知道\({r _i} \geqslant 0\)，所以也可得到\(0 \leqslant {\alpha _i} \leqslant C\)。带入拉格朗日函数得:</p>
$$L(\alpha ) =  - \frac{1}{2}\sum\limits _{i = 1}^m {\sum\limits _{j = 1}^m {{\alpha _i}{\alpha _j}{y^{(i)}}{y^{(j)}} < {x^{(i)}},{x^{(j)}} > } }  + \sum\limits _{i = 1}^m {({r _i} + {\alpha _i}){\xi _i}}  - \sum\limits _{i = 1}^m {{\alpha _i}({\xi _i}}  - 1) - \sum\limits _{i = 1}^m {{r _i}{\xi _i}} $$
<p>我们对上面的式子乘以-1，转化为求最小值的问题，可以得到最终的优化问题：</p>
$$\begin{gathered}   & \min L(\alpha ) = \min \frac{1}{2}\sum\limits _{i = 1}^m {\sum\limits _{j = 1}^m {{\alpha _i}{\alpha _j}{y^{(i)}}{y^{(j)}} < {x^{(i)}},{x^{(j)}} > } }  - \sum\limits _{i = 1}^m {{\alpha _i}}  \hfill \\\  s.t.: & \sum\limits _{i = 1}^m {{\alpha _i}{y^{(i)}}}  = 0 \hfill \\\   & 0 \leqslant {\alpha _i} \leqslant C \hfill \\\\end{gathered} $$

<p>可以得到如下的KKT条件：</p>

$${\alpha _i},{r _i} \geqslant 0$$$${y^{(i)}}({w^T}{x^{(i)}} + b) - 1 + {\xi _i} \geqslant 0$$$${\alpha _i}({y^{(i)}}({w^T}{x^{(i)}} + b) - 1 + {\xi _i}) = 0$$$${\xi _i} \geqslant 0,{r _i}{\xi _i} = 0$$
<p>对于上述KKT条件我们可以转换为如下形式：</p>
$${y^{(i)}}({w^T}{x^{(i)}} + b) \geqslant 1 , {\alpha _i} = 0$$
$${y^{(i)}}({w^T}{x^{(i)}} + b) \leqslant 1 , {\alpha _i} = C$$
$${y^{(i)}}({w^T}{x^{(i)}} + b) = 1 , 0 \leqslant {\alpha _i} \leqslant C$$
<p>很容易转化上面的式子，三个条件分别表示在比支持向量距离分割超平面远的样本，比支持向量距离分割超平面近的可容忍的误差样本，支持向量对应的样本。</p>
<h2 id="7-SMO算法理论"><a href="#7-SMO算法理论" class="headerlink" title="7 SMO算法理论"></a>7 SMO算法理论</h2><p>这一节使用SMO算法解决上一节归纳出来的优化问题。<br>SMO算法的思想来自于坐标上升算法，坐标上升算法的主要思想是一次遍历一个变量，然后把其他变量当做是常亮，进在一个维度上优化。<br>然后对于我们之前的问题，有\(\sum\limits _{i = 1}^m {\alpha _i}{y^{(i)}}  = 0\)。设置我们设\(\zeta  = {\alpha _1}{y^{(1)}} + {\alpha _2}{y^{(2)}} = \sum\limits _{i = 3}^m {\alpha _i}{y^{(i)}} \)，将\({\alpha _1}\)用\({\alpha _2}\)表达，然后得到关于\({\alpha _2}\)的二次函数，这样很容易取得极值。当所有样本满足KKT条件，且无法继续增加，我们就可以认为此刻取得最优值。</p>
<p>由于我们知道\(0 \leqslant {\alpha _i} \leqslant C\)，所以我们可以求的其取值范围，我们可以将二维变量表述为一个方格内，具体如下：</p>
<p><img src="/images/机器学习/监督学习-支持向量机图4.png" width="50%" height="50%" text-align="center/"></p>
<p>最多四种情况代入，经过求截距等一系列操作，可以将的取值范围归纳为下面的公式，其中L表示上界，H表示上界。<br>同号时有：</p>
$$L = \max (0,\alpha _1^{old} + \alpha _2^{old} - C)$$
$$H = \min (C,\alpha _1^{old} + \alpha _2^{old})$$

<p>异号的时候有：</p>
$$L = \max (0,\alpha _2^{old} - \alpha _2^{old})$$
$$H = \min (C,C + \alpha _2^{old} - \alpha _1^{old})$$
<p>进一步求解二次规划问题：</p>

$$\psi ({\alpha _1},{\alpha _2}) = \frac{1}{2}\alpha _1^2k(1,1) + \frac{1}{2}\alpha _2^2k(2,2) + {y^{(1)}}{y^{(2)}}{\alpha _1}{\alpha _2}k(1,2) - {\alpha _1} - {\alpha _2} + {y^{(1)}}{\alpha _1}{v _1} + {y^{(2)}}{\alpha _2}{v _2} + M$$
<p>上式中\(k(i,j) =  &lt; {x _i},{x _j} &gt; \)具体是核函数的简写，下节会介绍。\(M\)为与\(\alpha _1\),\(\alpha _2\)无关的参数。另外，\({v _1} = \sum\limits _{i = 3}^m {\alpha _i}{y^{(i)}}k(1,i) \),\({v _2} = \sum\limits _{i = 3}^m {\alpha _i}{y^{(i)}}k(2,i) \)。</p>
<p>我们设\(\zeta  = {\alpha _1}{y^{(1)}} + {\alpha _2}{y^{(2)}}\)，所以有\({\alpha _1} = {y^{(1)}}(\zeta  - {\alpha _2}{y^{(2)}})\),代入上式:</p>

$$\psi ({\alpha _2}) = \frac{1}{2}{(\zeta  - {\alpha _2}{y^{(2)}})^2}k(1,1) + \frac{1}{2}\alpha _2^2k(2,2) + {y^{(2)}}(\zeta  - {\alpha _2}{y^{(2)}}){\alpha _2}k(1,2) - (\zeta  - {\alpha _2}{y^{(2)}}){y^{(1)}} - {\alpha _2} + (\zeta  - {\alpha _2}{y^{(2)}}){v _1} + {y^{(2)}}{\alpha _2}{v _2} + M$$
<p>然后求导数:</p>
$$\frac{\partial }{{\partial {\alpha _2}}}\psi ({\alpha _2}) = ({\alpha _2}{y^{(2)}} - \zeta ){y^{(2)}}k(1,1) + {\alpha _2}k(2,2) + {y^{(2)}}\zeta k(1,2) - 2{\alpha _2}k(1,2) + {y^{(1)}}{y^{(2)}} - {y^{(1)}}{y^{(2)}} - {y^{(2)}}{v _1} + {y^{(2)}}{v _2} = 0$$$$\frac{\partial }{{\partial {\alpha _2}}}\psi ({\alpha _2}) = {\alpha _2}(k(1,1) + k(1,2) - 2k(1,2)) - \zeta {y^{(2)}}k(1,1) + \zeta {y^{(2)}}k(1,2) + {y^{(1)}}{y^{(2)}} - {y^{(1)}}{y^{(2)}} - {y^{(1)}}{v _1} + {y^{(2)}}{v _2} = 0$$
<p>我们设\(\eta  = k(1,1) + k(1,2) - 2k(1,2)\)，对\(\eta\)大于0的情况，导数为0的极值点就是极小值。对于\(\eta\)小于等于0的情况，最小值点肯定取自于边界，我们需要比较函数在\(L\)和\(H\)的大小。</p>
<p>让我们继续简化上面的式子，对于\(\eta\)大于0的情况下，取得极值。由于我们轻易得到下面的关系。</p>

$${v_1} = \sum\limits _{i = 1}^m {{\alpha _i}{y^{(2)}}k(1,i)}  + b - b - \sum\limits _{i = 1}^2 {{\alpha _i}{y^{(2)}}k(1,i)}  = f({x^{(1)}}) - b - \sum\limits _{i = 1}^2 {{\alpha _i}{y^{(2)}}k(1,i)} $$

$${v_2} = \sum\limits _{i = 1}^m {{\alpha _i}{y^{(2)}}k(2,i)}  + b - b - \sum\limits _{i = 1}^2 {{\alpha _i}{y^{(2)}}k(2,i)}  = f({x^{(2)}}) - b - \sum\limits _{i = 1}^2 {{\alpha _i}{y^{(2)}}k(2,i)} $$
<p>将上面的关系代入的如下极值：</p>
$$\alpha _2^{new} = \frac{{\zeta {y^{(2)}}k(1,1) - \zeta {y^{(2)}}k(1,2) + {y^{(2)}}({y^{(1)}} - f({x^{(1)}}) - ({y^{(2)}} - f({x^{(2)}})) - {y^{(2)}}\sum\limits _{i = 1}^2 {\alpha _i^{old}{y^{(2)}}k(1,i)}  + {y^{(2)}}\sum\limits _{i = 1}^2 {\alpha _i^{old}{y^{(2)}}k(2,i)} }}{\eta }$$

<p>我们将\(\zeta  = {\alpha _1}{y^{(1)}} + {\alpha _2}{y^{(2)}}\)代入上式，得:</p>
$$\alpha _2^{new} = \frac{{{\alpha _1}{y^{(1)}}{y^{(2)}}k(1,1) + {\alpha _2}k(1,1) - {\alpha _1}{y^{(1)}}{y^{(2)}}k(1,2) - {\alpha _2}k(1,2) - {\alpha _1}{y^{(1)}}{y^{(2)}}k(1,1) - {\alpha _2}k(1,2) + {\alpha _2}{y^{(1)}}{y^{(2)}}k(1,2) + {\alpha _2}k(2,2) + {y^{(2)}}({y^{(1)}} - f({x^{(1)}}) - ({y^{(2)}} - f({x^{(2)}}))}}{\eta }$$
<p>约掉部分选项，有:</p>
$$\alpha _2^{new} = \alpha _2^{old} + \frac{{{y^{(2)}}({y^{(1)}} - f({x^{(1)}}) - ({y^{(2)}} - f({x^{(2)}})))}}{\eta }$$
$$\alpha _2^{new} = \alpha _2^{old} + \frac{{{y^{(2)}}({e _1} - {e _2})}}{\eta }$$
<p>接下来就只剩下求\(b\)的问题了，根据上一节最后转化的KKT条件。对\(0 \leqslant {\alpha _1} \leqslant C\)的情况下，有\({y^{(1)}}({w^T}{x^{(1)}} + b) = 1\)。所以有：</p>

$$b = {y^{(1)}} - \sum\limits _{i = 1}^m {{\alpha _i}} {y^{(i)}}k(1,i)$$

<p>进一步展开有：</p>

$$b = {y^{(1)}} - \sum\limits _{i = 3}^m {\alpha _i^{new}{y^{(i)}}k(1,i)}  - \alpha _1^{new}{y^{(1)}}k(1,1) - \alpha _2^{new}{y^{(2)}}k(1,2)$$

<p>我们知道上面的式子中\({\alpha _3}\)到\({\alpha _m}\)并没有发生变化，因此有：</p>

$$\sum\limits _{i = 3}^m {\alpha _i^{new}{y^{(i)}}k(1,i)}  = \sum\limits _{i = 3}^m {\alpha _i^{old}{y^{(i)}}k(1,i)}  = \sum\limits _{i = 1}^m {\alpha _i^{old}{y^{(i)}}k(1,i)}  - \alpha _1^{old}{y^{(1)}}k(1,1) - \alpha _2^{old}{y^{(2)}}k(1,2) = f({x^{(1)}}) - b - \alpha _1^{old}{y^{(1)}}k(1,1) - \alpha _2^{old}{y^{(2)}}k(1,2)$$
<p>代入如上式得:</p>
$${b^{new}} =  - {e_1} + (\alpha _1^{old} - \alpha _1^{new}){y^{(1)}}k(1,1) + (\alpha _1^{old} - \alpha _1^{new}){y^{(2)}}k(1,2) + {b^{old}}$$

<p>对于\({\alpha _2}\)有同样的道理。</p>
<p>综上，假如\(0 \leqslant {\alpha _1} \leqslant C\)，有:</p>
$${b^{new}} =  - {e_1} + (\alpha _1^{old} - \alpha _1^{new}){y^{(1)}}k(1,1) + (\alpha _1^{old} - \alpha _1^{new}){y^{(2)}}k(1,2) + {b^{old}}$$

<p>假如\(0 \leqslant {\alpha _2} \leqslant C\)，有：</p>
$${b^{new}} =  - {e_2} + (\alpha _2^{old} - \alpha _2^{new}){y^{(1)}}k(2,1) + (\alpha _2^{old} - \alpha _2^{new}){y^{(2)}}k(2,2) + {b^{old}}$$

<p>假如\(0 \leqslant {\alpha _1},{\alpha _2} \leqslant C\),事实上上面两个公司的出来的结果是一样的，因此不用特殊计算。<br>如果不满足在\(0\)和\(C\)的范围，则去两个公式的中间值。(笔者认为没有必要更新)</p>
<h2 id="8-SMO算法实践"><a href="#8-SMO算法实践" class="headerlink" title="8 SMO算法实践"></a>8 SMO算法实践</h2><p>在SMO论文中有具体的伪代码，算法的主要逻辑就是要保证每个样本都满足KKT条件，且直到所有\(\alpha \)达到极值，即不需要更新为止。</p>
<p>然后是关于\(\alpha \)的选择，第一个\(\alpha \)我们可以随机选择一个违反KKT条件的，第二个我们选择能够最大程度更新\(\alpha \)的值，看上一节的公式，实际会选择\(|e _1-e _2|\)最大的样本点作为第二个\(\alpha \)。具体逻辑可以参考SMO的论文，或者下面代码的注释。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div><div class="line">238</div><div class="line">239</div><div class="line">240</div><div class="line">241</div><div class="line">242</div><div class="line">243</div><div class="line">244</div><div class="line">245</div><div class="line">246</div><div class="line">247</div><div class="line">248</div><div class="line">249</div><div class="line">250</div><div class="line">251</div><div class="line">252</div><div class="line">253</div><div class="line">254</div><div class="line">255</div><div class="line">256</div><div class="line">257</div><div class="line">258</div><div class="line">259</div><div class="line">260</div><div class="line">261</div><div class="line">262</div><div class="line">263</div><div class="line">264</div><div class="line">265</div><div class="line">266</div><div class="line">267</div><div class="line">268</div><div class="line">269</div><div class="line">270</div><div class="line">271</div><div class="line">272</div><div class="line">273</div><div class="line">274</div><div class="line">275</div><div class="line">276</div><div class="line">277</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> matplotlib.lines <span class="keyword">import</span> Line2D</div><div class="line"></div><div class="line"><span class="keyword">global</span> g_y_vec    <span class="comment"># g_y_vec为样本输出, 大小g_m</span></div><div class="line"><span class="keyword">global</span> g_x_mat    <span class="comment"># g_x_mat为样本的输入, 大小2*g_m</span></div><div class="line"><span class="keyword">global</span> g_m        <span class="comment"># g_m 为样本数目</span></div><div class="line"><span class="keyword">global</span> g_alpha    <span class="comment"># g_alpha 大小为g_m</span></div><div class="line"><span class="keyword">global</span> g_C</div><div class="line"><span class="keyword">global</span> g_w        <span class="comment"># 实际没有使用，而是通过alpha表示的</span></div><div class="line"><span class="keyword">global</span> g_b        <span class="comment"># y = wx+b</span></div><div class="line"><span class="keyword">global</span> g_y_now    <span class="comment"># 表示当前参数计算的y，即svmOutPut对应的g_y_now</span></div><div class="line"><span class="keyword">global</span> g_err      <span class="comment"># g_err 表示svmOutput - g_y_vec对应的序列</span></div><div class="line"></div><div class="line"><span class="comment"># g_arr0 means classify of -1</span></div><div class="line">g_arr0 = np.array([[ <span class="number">0.88235916</span> , <span class="number">1.01511634</span>],[ <span class="number">0.75243817</span> , <span class="number">0.76520033</span>],[ <span class="number">0.95710848</span> , <span class="number">1.41894337</span>],[ <span class="number">1.48682891</span> , <span class="number">0.78885043</span>],[ <span class="number">1.24047011</span> , <span class="number">0.71984948</span>],[ <span class="number">0.67611276</span> , <span class="number">1.07909452</span>],[ <span class="number">1.03243669</span> , <span class="number">1.08929695</span>],[ <span class="number">1.0296548</span>  , <span class="number">1.25023769</span>],[ <span class="number">1.54134008</span> , <span class="number">0.39564824</span>],[ <span class="number">0.34645057</span> , <span class="number">1.61499636</span>],[ <span class="number">0.77206174</span> , <span class="number">1.23613698</span>],[ <span class="number">0.91446988</span> , <span class="number">1.38537765</span>],[ <span class="number">0.99982962</span> , <span class="number">1.34448471</span>],[ <span class="number">0.78745962</span> , <span class="number">0.9046565</span> ],[ <span class="number">0.74946602</span> , <span class="number">1.07424473</span>],[ <span class="number">1.09294839</span> , <span class="number">1.14711993</span>],[ <span class="number">0.39266844</span> , <span class="number">0.78788004</span>],[ <span class="number">0.83112357</span> , <span class="number">1.2762774</span> ],[ <span class="number">1.05056188</span> , <span class="number">1.13351562</span>],[ <span class="number">1.62101523</span> , <span class="number">1.15035562</span>],[ <span class="number">0.70377517</span> , <span class="number">1.1136416</span> ],[ <span class="number">1.03715472</span> , <span class="number">0.47905693</span>],[ <span class="number">0.94598381</span> , <span class="number">0.8874837</span> ],[ <span class="number">0.94447128</span> , <span class="number">2.02796925</span>],[ <span class="number">0.72442242</span> , <span class="number">1.09835206</span>],[ <span class="number">0.69046731</span> , <span class="number">1.46232182</span>],[ <span class="number">1.20744606</span> , <span class="number">1.10280041</span>],[ <span class="number">0.70665746</span> , <span class="number">0.82139503</span>],[ <span class="number">1.08803887</span> , <span class="number">1.4450361</span> ],[ <span class="number">0.88530961</span> , <span class="number">0.75727475</span>],[ <span class="number">0.98418545</span> , <span class="number">0.80248161</span>],[ <span class="number">0.74970386</span> , <span class="number">1.13205709</span>],[ <span class="number">0.72586454</span> , <span class="number">1.06058385</span>],[ <span class="number">0.9071812</span>  , <span class="number">1.09975063</span>],[ <span class="number">0.75182835</span> , <span class="number">0.93570147</span>],[ <span class="number">0.80052289</span> , <span class="number">1.08168507</span>],[ <span class="number">0.40180652</span> , <span class="number">0.9526211</span> ],[ <span class="number">0.62312617</span> , <span class="number">0.84385058</span>],[ <span class="number">0.68212516</span> , <span class="number">1.25912717</span>],[ <span class="number">1.19773245</span> , <span class="number">0.16399654</span>],[ <span class="number">0.96093132</span> , <span class="number">0.43932091</span>],[ <span class="number">1.25471657</span> , <span class="number">0.92371829</span>],[ <span class="number">1.12330272</span> , <span class="number">1.26968747</span>],[ <span class="number">1.30361985</span> , <span class="number">0.99862123</span>],[ <span class="number">1.23477665</span> , <span class="number">1.1742804</span> ],[ <span class="number">0.28471876</span> , <span class="number">0.5806044</span> ],[ <span class="number">1.89355099</span> , <span class="number">1.19928671</span>],[ <span class="number">1.09081369</span> , <span class="number">1.28467312</span>],[ <span class="number">1.40488635</span> , <span class="number">0.90034427</span>],[ <span class="number">1.11672364</span> , <span class="number">1.49070515</span>],[ <span class="number">1.35385212</span> , <span class="number">1.35767891</span>],[ <span class="number">0.92746374</span> , <span class="number">1.79096697</span>],[ <span class="number">1.89142562</span> , <span class="number">0.98228303</span>],[ <span class="number">1.0555218</span>  , <span class="number">0.86070833</span>],[ <span class="number">0.69001255</span> , <span class="number">1.12874741</span>],[ <span class="number">0.98137315</span> , <span class="number">1.3398852</span> ],[ <span class="number">1.02525371</span> , <span class="number">0.77572865</span>],[ <span class="number">1.1354295</span>  , <span class="number">1.07098552</span>],[ <span class="number">1.50829164</span> , <span class="number">1.43065998</span>],[ <span class="number">1.09928764</span> , <span class="number">1.55540292</span>],[ <span class="number">0.64695084</span> , <span class="number">0.79920395</span>],[ <span class="number">0.82059034</span> , <span class="number">0.97533491</span>],[ <span class="number">0.56345455</span> , <span class="number">1.08168272</span>],[ <span class="number">1.06673215</span> , <span class="number">1.19448556</span>],[ <span class="number">0.96512548</span> , <span class="number">1.5268577</span> ],[ <span class="number">0.96914451</span> , <span class="number">1.00902985</span>],[ <span class="number">0.72879413</span> , <span class="number">0.92476415</span>],[ <span class="number">1.0931483</span>  , <span class="number">1.13572242</span>],[ <span class="number">1.34765121</span> , <span class="number">0.83841006</span>],[ <span class="number">1.57813788</span> , <span class="number">0.65915892</span>],[ <span class="number">0.59032608</span> , <span class="number">0.82747946</span>],[ <span class="number">0.83838504</span> , <span class="number">0.67588473</span>],[ <span class="number">1.35101322</span> , <span class="number">1.21027851</span>],[ <span class="number">0.71762153</span> , <span class="number">0.41839038</span>],[ <span class="number">0.61295604</span> , <span class="number">0.66555018</span>],[ <span class="number">0.64379346</span> , <span class="number">0.92925228</span>],[ <span class="number">1.1194968</span>  , <span class="number">0.65876736</span>],[ <span class="number">0.39495437</span> , <span class="number">0.67246734</span>],[ <span class="number">1.05223282</span> , <span class="number">0.17889116</span>],[ <span class="number">0.97810984</span> , <span class="number">1.12794664</span>],[ <span class="number">0.98392719</span> , <span class="number">0.73590255</span>],[ <span class="number">1.25587405</span> , <span class="number">1.21853038</span>],[ <span class="number">1.01150226</span> , <span class="number">1.01835571</span>],[ <span class="number">1.02251614</span> , <span class="number">0.72704228</span>],[ <span class="number">1.00261519</span> , <span class="number">0.95347185</span>],[ <span class="number">0.96362523</span> , <span class="number">0.8607009</span> ],[ <span class="number">0.88034659</span> , <span class="number">1.2307104</span> ],[ <span class="number">0.75907236</span> , <span class="number">0.92799796</span>],[ <span class="number">0.54898709</span> , <span class="number">1.69882285</span>],[ <span class="number">0.55032649</span> , <span class="number">0.98831566</span>],[ <span class="number">1.33360789</span> , <span class="number">1.19793298</span>],[ <span class="number">0.83231239</span> , <span class="number">0.8946538</span> ],[ <span class="number">1.05173094</span> , <span class="number">1.26324289</span>],[ <span class="number">0.81482231</span> , <span class="number">0.56198584</span>],[ <span class="number">1.03854797</span> , <span class="number">1.0553811</span> ],[ <span class="number">1.32669227</span> , <span class="number">1.61115811</span>],[ <span class="number">1.13322152</span> , <span class="number">1.68151695</span>],[ <span class="number">0.39754618</span> , <span class="number">1.19392967</span>],[ <span class="number">0.61344185</span> , <span class="number">1.05281434</span>],[ <span class="number">1.18415366</span> , <span class="number">0.864884</span>  ]])</div><div class="line"><span class="comment"># g_arr1 means classify of +1</span></div><div class="line">g_arr1 = np.array([[ <span class="number">2.15366548</span> , <span class="number">1.88035458</span>],[ <span class="number">2.36978774</span> , <span class="number">1.76550283</span>],[ <span class="number">2.46261387</span> , <span class="number">2.10568262</span>],[ <span class="number">1.90475526</span> , <span class="number">1.95242885</span>],[ <span class="number">1.77712677</span> , <span class="number">1.96004856</span>],[ <span class="number">1.5995514</span>  , <span class="number">2.1323943</span> ],[ <span class="number">1.52727223</span> , <span class="number">1.50295551</span>],[ <span class="number">1.80330407</span> , <span class="number">1.57942301</span>],[ <span class="number">1.86487049</span> , <span class="number">1.87234414</span>],[ <span class="number">1.9586354</span>  , <span class="number">1.96279729</span>],[ <span class="number">2.59668134</span> , <span class="number">2.414423</span>  ],[ <span class="number">2.818419</span>   , <span class="number">1.76280366</span>],[ <span class="number">2.01511628</span> , <span class="number">2.10858546</span>],[ <span class="number">2.15907962</span> , <span class="number">1.81593012</span>],[ <span class="number">1.63966834</span> , <span class="number">2.2209023</span> ],[ <span class="number">2.47220599</span> , <span class="number">1.70482956</span>],[ <span class="number">2.08760748</span> , <span class="number">2.51601971</span>],[ <span class="number">1.50547722</span> , <span class="number">1.8487145</span> ],[ <span class="number">1.68125583</span> , <span class="number">2.64968501</span>],[ <span class="number">2.01924282</span> , <span class="number">2.0953572</span> ],[ <span class="number">2.22563534</span> , <span class="number">2.18266325</span>],[ <span class="number">2.2684291</span>  , <span class="number">2.23581599</span>],[ <span class="number">2.13787557</span> , <span class="number">1.9999382</span> ],[ <span class="number">1.02638695</span> , <span class="number">1.68134967</span>],[ <span class="number">2.35614619</span> , <span class="number">1.32072125</span>],[ <span class="number">2.20054871</span> , <span class="number">1.47401445</span>],[ <span class="number">1.99454827</span> , <span class="number">1.71658741</span>],[ <span class="number">1.83269065</span> , <span class="number">2.47662909</span>],[ <span class="number">2.40097251</span> , <span class="number">2.21823862</span>],[ <span class="number">2.54404652</span> , <span class="number">1.85742018</span>],[ <span class="number">1.84150027</span> , <span class="number">2.06350351</span>],[ <span class="number">1.69490855</span> , <span class="number">1.70169334</span>],[ <span class="number">1.44745704</span> , <span class="number">1.88295233</span>],[ <span class="number">2.24376639</span> , <span class="number">1.67530495</span>],[ <span class="number">1.42911921</span> , <span class="number">1.81854548</span>],[ <span class="number">1.33789289</span> , <span class="number">2.27686128</span>],[ <span class="number">2.43509821</span> , <span class="number">1.95032131</span>],[ <span class="number">1.9512447</span>  , <span class="number">1.4595415</span> ],[ <span class="number">2.13041192</span> , <span class="number">1.79372755</span>],[ <span class="number">2.2753866</span>  , <span class="number">2.23781951</span>],[ <span class="number">2.26753401</span> , <span class="number">1.78149305</span>],[ <span class="number">2.06505449</span> , <span class="number">2.01939606</span>],[ <span class="number">2.44426826</span> , <span class="number">2.1437101</span> ],[ <span class="number">2.16607141</span> , <span class="number">2.31077167</span>],[ <span class="number">1.96097237</span> , <span class="number">2.49100193</span>],[ <span class="number">1.37255424</span> , <span class="number">1.60735016</span>],[ <span class="number">1.63947758</span> , <span class="number">2.17852314</span>],[ <span class="number">2.13722666</span> , <span class="number">2.00559707</span>],[ <span class="number">1.222696</span>   , <span class="number">1.67075059</span>],[ <span class="number">2.56982685</span> , <span class="number">2.51218813</span>]])</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcLH</span><span class="params">(id1,id2)</span>:</span></div><div class="line">  <span class="keyword">if</span> g_y_vec[id1] == g_y_vec[id2]:</div><div class="line">    L = max(<span class="number">0</span>,g_alpha[id1]+g_alpha[id2]-g_C)</div><div class="line">    H = min(g_C,g_alpha[id1]+g_alpha[id2])</div><div class="line">  <span class="keyword">else</span>:</div><div class="line">    L = max(<span class="number">0</span>,g_alpha[id2]-g_alpha[id1])</div><div class="line">    H = min(g_C,g_C+g_alpha[id2]-g_alpha[id1])</div><div class="line">  <span class="keyword">return</span> (L,H)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">svmOutput</span><span class="params">(id1)</span>:</span></div><div class="line">  <span class="comment"># 这个函数是svm的实际输出，计算当前参数(w,b)下, 计算得到的y</span></div><div class="line">  <span class="comment"># 由于w = sum (alpha*y*x), 对于第i个分量x_i所以输出结果应该为w*x_i+b, 也就是 sum (alpha*y*&lt;x_1-m,x_i&gt;)</span></div><div class="line">  <span class="comment"># 注: 待会分析下alpha的变化趋势与C的关系</span></div><div class="line">  <span class="keyword">global</span> g_b</div><div class="line">  sum = <span class="number">0</span>;</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(g_m):</div><div class="line">    sum += g_alpha[i]*g_y_vec[i]*kernel(i,id1)</div><div class="line">  <span class="keyword">return</span> sum+g_b</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">kernel</span><span class="params">(id1,id2)</span>:</span></div><div class="line">  <span class="comment"># 这里核函数为简单的内积</span></div><div class="line">  <span class="comment"># id1和id2为int类型，是g_x_mat中的索引</span></div><div class="line">  <span class="keyword">return</span> np.dot(g_x_mat[id1],g_x_mat[id2])</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">compareFun</span><span class="params">(id1,id2,L,H)</span>:</span></div><div class="line">  <span class="comment"># 如果返回1，表示L处去极小值。如果返回-1，H处去极小值。如果是0，表示这次不更新</span></div><div class="line">  <span class="comment"># 如果两者相等，这里略过, 说明无法有强力证据证明这个样本属于wx+b&gt;1还是wx+b&lt;1，所以等待下一轮迭代。因此，与L和H相等应该设置一个阀值，判断近似相等。</span></div><div class="line">  <span class="keyword">global</span> g_b</div><div class="line">  y_1 = g_y_vec[id1]</div><div class="line">  y_2 = g_y_vec[id2]</div><div class="line">  e_1 = g_err[id1]</div><div class="line">  e_2 = g_err[id2]</div><div class="line">  alpha_1 = g_alpha[id1]</div><div class="line">  alpha_2 = g_alpha[id2]</div><div class="line">  k11 = kernel(id1,id1)</div><div class="line">  k12 = kernel(id1,id2)</div><div class="line">  k22 = kernel(id2,id2)</div><div class="line">  s = y_1*y_2</div><div class="line">  f_1 = y_1*(e_1+g_b)-alpha_1*k11-s*alpha_2*k12</div><div class="line">  f_2 = y_2*(e_2+g_b)-s*alpha_1*k12-alpha_2*k22</div><div class="line">  L_1 = alpha_1+s*(alpha_2-L)</div><div class="line">  H_1 = alpha_1+s*(alpha_2-H)</div><div class="line">  phi_l = L_1*f_1+L*f_2+<span class="number">0.5</span>*L_1*L_1*k11+<span class="number">0.5</span>*L*L*k22+s*L*L_1*k12</div><div class="line">  phi_h = H_1*f_1+H*f_2+<span class="number">0.5</span>*H_1*H_1*k11+<span class="number">0.5</span>*H*H*k22+s*H*H_1*k12</div><div class="line">  <span class="keyword">if</span> L==H:</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span></div><div class="line">  <span class="keyword">if</span> phi_l &lt; phi_h:</div><div class="line">    <span class="keyword">return</span> <span class="number">1</span></div><div class="line">  <span class="keyword">else</span>:</div><div class="line">    <span class="keyword">return</span> <span class="number">-1</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeStep</span><span class="params">(id1,id2,err)</span>:</span></div><div class="line">  <span class="keyword">if</span> id1==id2:</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span></div><div class="line">  alpha_1 = g_alpha[id1]</div><div class="line">  alpha_2 = g_alpha[id2]</div><div class="line">  y_1 = g_y_vec[id1]</div><div class="line">  y_2 = g_y_vec[id2]</div><div class="line">  e1 = g_err[id1]</div><div class="line">  e2 = g_err[id2]</div><div class="line">  s = y_1*y_2</div><div class="line">  L,H=calcLH(id1,id2)</div><div class="line">  <span class="comment">#print("id1=",id1,", id2=",id2," L=",L,", H=",H)</span></div><div class="line">  <span class="keyword">if</span> L==H :</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span></div><div class="line">  k11 = kernel(id1,id1)</div><div class="line">  k12 = kernel(id1,id2)</div><div class="line">  k22 = kernel(id2,id2)</div><div class="line">  eta = k11+k22<span class="number">-2</span>*k12</div><div class="line">  <span class="comment">#print("kernel:",k11,k12,k22,", eta=",eta,"e1=",e1,"e2=",e2)</span></div><div class="line"></div><div class="line">  alpha_2_new = alpha_2</div><div class="line">  <span class="comment"># 如果eta大于0, 我们可知最小值在边界或极小值点上。事实上，如果极小值不在范围内，必在距离极小值近的那个边界上。</span></div><div class="line">  <span class="comment"># 如果eta小于0, 我们可知最小值则必在边界上。我们只需要比较两个边界点函数的大小即可。</span></div><div class="line">  <span class="keyword">if</span> eta&gt;<span class="number">0</span> :</div><div class="line">    alpha_2_new = alpha_2+y_2*(e1-e2)/eta</div><div class="line">    alpha_2_new = max(alpha_2_new,L)</div><div class="line">    alpha_2_new = min(alpha_2_new,H)</div><div class="line">  <span class="keyword">else</span>:</div><div class="line">    <span class="comment"># 由于我们知道可以直接这是一个关于alpha的二次函数，并且自变量的取值范围是[L,H], 事实上我们只需要比较alpha_2_new离L和H哪个远即可</span></div><div class="line">    <span class="comment"># 但是考虑到eta=0的一次函数特殊情况，我们还是老老实实的计算函数值吧。</span></div><div class="line">    ret = compareFun(id1, id2, L, H)</div><div class="line">    <span class="keyword">if</span> ret == <span class="number">0</span>:</div><div class="line">      alpha_2_new = alpha_2</div><div class="line">    <span class="keyword">elif</span> ret == <span class="number">1</span>:</div><div class="line">      alpha_2_new = L</div><div class="line">    <span class="keyword">elif</span> ret == <span class="number">1</span>:</div><div class="line">      alpha_2_new = H</div><div class="line">    print(<span class="string">"----------------eta&lt;=0----------------"</span>)</div><div class="line"></div><div class="line">  <span class="comment"># 归整化alpha_2_new</span></div><div class="line">  <span class="keyword">if</span> alpha_2_new &lt; err:</div><div class="line">    alpha_2_new = <span class="number">0</span></div><div class="line">  <span class="keyword">elif</span> alpha_2_new &gt; g_C - err:</div><div class="line">    alpha_2_new = g_C</div><div class="line">  <span class="keyword">if</span> abs(alpha_2_new-alpha_2) &lt; err:</div><div class="line">    print(<span class="string">"alpha_2 is no need to update"</span>)</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span></div><div class="line"></div><div class="line">  <span class="comment"># update b</span></div><div class="line">  <span class="keyword">global</span> g_b</div><div class="line">  alpha_1_new = alpha_1 + s * (alpha_2 - alpha_2_new)      <span class="comment"># 不必担心alpha_1_new不在[0,C]范围内，之前的公式已经保证了</span></div><div class="line">  b1_new = -e1-y_1*k11*(alpha_1_new-alpha_1)-y_2*k12*(alpha_2_new-alpha_2) + g_b</div><div class="line">  b2_new = -e2-y_1*k12*(alpha_1_new-alpha_1)-y_2*k22*(alpha_2_new-alpha_2) + g_b</div><div class="line">  <span class="keyword">if</span> alpha_1_new&gt;<span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> alpha_1_new&lt;g_C:</div><div class="line">    g_b = b1_new</div><div class="line">  <span class="keyword">elif</span> alpha_2_new&gt;<span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> alpha_2_new&lt;g_C:</div><div class="line">    g_b = b2_new</div><div class="line">  <span class="keyword">else</span>:</div><div class="line">    g_b = <span class="number">0.5</span>*(b1_new+b2_new)</div><div class="line"></div><div class="line">  <span class="comment"># update alpha</span></div><div class="line">  g_alpha[id1] = alpha_1_new</div><div class="line">  g_alpha[id2] = alpha_2_new</div><div class="line">  print(<span class="string">"g_alpha[id1]="</span>,g_alpha[id1],<span class="string">"g_alpha[id2]="</span>,g_alpha[id2],<span class="string">"s="</span>,s,<span class="string">", alpha_1="</span>,alpha_1,<span class="string">", alpha_2="</span>,alpha_2)</div><div class="line"></div><div class="line">  <span class="comment"># update err g_y_now</span></div><div class="line">  updateYAndErr()</div><div class="line"></div><div class="line">  <span class="keyword">return</span> <span class="number">1</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateYAndErr</span><span class="params">()</span>:</span></div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(g_m):</div><div class="line">    g_y_now[i] = svmOutput(i)</div><div class="line">    g_err[i] = svmOutput(i)-g_y_vec[i]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestAlphaIndex</span><span class="params">(id2)</span>:</span></div><div class="line">  maxIncr = <span class="number">0</span></div><div class="line">  maxIndex = <span class="number">-1</span>;</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(g_m):</div><div class="line">    incr = abs(g_err[i]-g_alpha[i])</div><div class="line">    <span class="keyword">if</span> incr &gt;= maxIncr:</div><div class="line">      maxIndex = i</div><div class="line">      maxIncr = incr</div><div class="line">  <span class="keyword">return</span> maxIndex</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sizeOfNonZerorAndNonC</span><span class="params">()</span>:</span></div><div class="line">  size=<span class="number">0</span></div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(g_m):</div><div class="line">    <span class="keyword">if</span> g_alpha[i]!=<span class="number">0</span> <span class="keyword">and</span> g_alpha[i]!=g_C:</div><div class="line">      size= size+<span class="number">1</span></div><div class="line">  <span class="keyword">return</span> size</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseRandomIndex</span><span class="params">(id2)</span>:</span></div><div class="line">  ret = id2;</div><div class="line">  <span class="keyword">while</span> ret==id2:</div><div class="line">    ret = random.randint(<span class="number">0</span>,g_m<span class="number">-1</span>)</div><div class="line">  <span class="keyword">return</span> ret</div><div class="line"></div><div class="line"><span class="comment"># 对于examineExample函数,我们一次进选择id2样本对应的alpha与如下规则选择的id1对应的alpha,然后相应跟新其值。</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">examineExample</span><span class="params">(id2)</span>:</span></div><div class="line">  y_2 = g_y_vec[id2]</div><div class="line">  tol = <span class="number">1e-2</span>        <span class="comment"># 是一个正数</span></div><div class="line">  alpha_2 = g_alpha[id2]</div><div class="line">  e_2 = svmOutput(id2)-g_y_vec[id2]</div><div class="line">  r_2 = e_2 * y_2</div><div class="line"></div><div class="line">  <span class="comment"># 我们只对当前违反kkt条件的样本对应的alpha进行更新</span></div><div class="line">  <span class="comment"># 关于违反kkt条件的说明:</span></div><div class="line">  <span class="comment"># (1) r_2 &lt; -tol 表示r_2小于0，即表示输出的预测结果与样本的y符号相反，因此应该属于误差案例的。所以，根据公式，alpha = C。如果alpha &lt; C ,比违反kkt条件</span></div><div class="line">  <span class="comment"># (2) r_2 &lt; -tol 表示r_2大于0，即表示输出的预测结果与样本的y符号相同，当误差大于一定的</span></div><div class="line">  <span class="keyword">if</span> r_2 &lt; -tol <span class="keyword">and</span> alpha_2 &lt; g_C <span class="keyword">or</span> r_2 &gt; tol <span class="keyword">and</span> alpha_2 &gt; <span class="number">0</span> :</div><div class="line">    <span class="comment"># 下面的程序逻辑是这样的:</span></div><div class="line">    <span class="comment"># 先遍历alpha非0或非C, 因为我们对于alpha为0和alpha为C的情况, 认为是处于非支持向量和处于误差样本的情况。我们只有根据支持向量下，找到最优的||w||才有意义</span></div><div class="line">    <span class="comment"># 首先，我们找到|e1-e2|最大的alpha, 从这里优化。如果优化结果不理想，我们就随机找一个alpha一起计算。如果还不行，就在整个范围alpha范围内计算</span></div><div class="line">    <span class="keyword">if</span> sizeOfNonZerorAndNonC()&gt;<span class="number">0</span>:</div><div class="line">      id1=chooseBestAlphaIndex(id2)</div><div class="line">      <span class="keyword">if</span> takeStep(id1,id2,<span class="number">1e-3</span>):</div><div class="line">        <span class="comment">#print("takeStep1, alpha[",id1,"]=",g_alpha[id1],", alphapp[",id2,"]=",g_alpha[id2])</span></div><div class="line">        <span class="keyword">return</span> <span class="number">1</span></div><div class="line">    r=chooseRandomIndex(id2)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(g_m):</div><div class="line">      id1 = (r+i)%g_m</div><div class="line">      <span class="keyword">if</span> id1!=id2 <span class="keyword">and</span> g_alpha[id1]!=<span class="number">0</span> <span class="keyword">and</span> g_alpha[id1]!=g_C:</div><div class="line">        <span class="keyword">if</span> takeStep(id1,id2,<span class="number">1e-3</span>):</div><div class="line">          <span class="comment">#print("takeStep2, alpha[", id1, "]=", g_alpha[id1], ", alphapp[", id2, "]=", g_alpha[id2])</span></div><div class="line">          <span class="keyword">return</span> <span class="number">1</span></div><div class="line">    r = chooseRandomIndex(id2)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(g_m):</div><div class="line">      id1 = (r + i) % g_m</div><div class="line">      <span class="keyword">if</span> id1 != id2:</div><div class="line">        <span class="keyword">if</span> takeStep(id1,id2,<span class="number">1e-3</span>):</div><div class="line">          <span class="comment">#print("takeStep3, alpha[", id1, "]=", g_alpha[id1], ", alphapp[", id2, "]=", g_alpha[id2])</span></div><div class="line">          <span class="keyword">return</span> <span class="number">1</span></div><div class="line">  <span class="keyword">return</span> <span class="number">0</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">showPic</span><span class="params">(w,b)</span>:</span></div><div class="line">  <span class="comment"># draw wx+b, x1为横轴，x2为纵轴</span></div><div class="line">  k = -w[<span class="number">0</span>]/w[<span class="number">1</span>]</div><div class="line">  b = -g_b/w[<span class="number">1</span>]</div><div class="line"></div><div class="line">  figure, ax = plt.subplots()</div><div class="line">  ax.set_xlim(left=<span class="number">-1</span>, right=<span class="number">4</span>)</div><div class="line">  ax.set_ylim(bottom=<span class="number">-1</span>, top=<span class="number">4</span>)</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(g_m):</div><div class="line">    x = g_x_mat[i]</div><div class="line">    <span class="keyword">if</span> abs(g_alpha[i]<span class="number">-0</span>)&lt;<span class="number">1e-3</span>:</div><div class="line">      plt.plot(x[<span class="number">0</span>], x[<span class="number">1</span>], <span class="string">'b--'</span>, marker=<span class="string">'+'</span>, color=<span class="string">'r'</span>)</div><div class="line">    <span class="keyword">elif</span> abs(g_alpha[i]-g_C)&lt;<span class="number">1e-3</span>:</div><div class="line">      plt.plot(x[<span class="number">0</span>], x[<span class="number">1</span>], <span class="string">'b--'</span>, marker=<span class="string">'o'</span>, color=<span class="string">'g'</span>)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">      plt.plot(x[<span class="number">0</span>], x[<span class="number">1</span>], <span class="string">'b--'</span>, marker=<span class="string">'o'</span>, color=<span class="string">'b'</span>)</div><div class="line">  <span class="comment"># draw y = -x+3</span></div><div class="line">  (line1_xs, line1_ys) = [(<span class="number">0</span>, <span class="number">3</span>), (b, <span class="number">3</span>*k+b)]</div><div class="line">  ax.add_line(Line2D(line1_xs, line1_ys, linewidth=<span class="number">1</span>, color=<span class="string">'blue'</span>))</div><div class="line">  plt.xlabel(<span class="string">"x1"</span>)</div><div class="line">  plt.ylabel(<span class="string">"x2"</span>)</div><div class="line">  plt.plot()</div><div class="line">  plt.show()</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</div><div class="line">  <span class="comment"># 0 make data</span></div><div class="line">  g_y_vec = np.array([])</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(len(g_arr0)):</div><div class="line">    g_y_vec=np.append(g_y_vec,<span class="number">-1</span>)</div><div class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> range(len(g_arr1)):</div><div class="line">    g_y_vec=np.append(g_y_vec,+<span class="number">1</span>)</div><div class="line">  g_x_mat = np.vstack((g_arr0,g_arr1))</div><div class="line"></div><div class="line">  <span class="comment"># 1 training</span></div><div class="line">  g_m = len(g_x_mat)</div><div class="line">  g_alpha = np.zeros(g_m)</div><div class="line">  g_y_now = np.zeros(g_m)</div><div class="line">  g_err = np.zeros(g_m)</div><div class="line">  <span class="keyword">global</span> g_b</div><div class="line">  g_b = <span class="number">0</span></div><div class="line">  numChanged = <span class="number">0</span></div><div class="line">  examineAll = <span class="number">1</span></div><div class="line">  g_C = <span class="number">10</span></div><div class="line">  err = <span class="number">0</span></div><div class="line"></div><div class="line">  updateYAndErr()       <span class="comment"># 实现更新下缓冲，即当前输出与误差值</span></div><div class="line"></div><div class="line">  <span class="keyword">while</span> numChanged&gt;<span class="number">0</span> <span class="keyword">or</span> examineAll:</div><div class="line">    numChanged = <span class="number">0</span></div><div class="line">    <span class="comment"># 循环处理，第一次对所有的样本进行一次处理。</span></div><div class="line">    <span class="comment"># 然后对所有非边界的数值进行处理。因为在当前参数下，非边界的样本，我们认为其是支持向量。对于优化||w||的大小，支持向量才有意义。</span></div><div class="line">    <span class="keyword">if</span> examineAll:</div><div class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> range(g_m):</div><div class="line">        numChanged += examineExample(i)</div><div class="line">        <span class="comment">#print("examineAll=1, numChanged=",numChanged)</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> range(g_m):</div><div class="line">        <span class="keyword">if</span> g_alpha[i]!=<span class="number">0</span> <span class="keyword">and</span> g_alpha[i]!=g_C:</div><div class="line">          numChanged += examineExample(i)</div><div class="line">    examineAll = abs(examineAll<span class="number">-1</span>)</div><div class="line">  <span class="comment">#print(g_alpha)</span></div><div class="line"></div><div class="line">  <span class="comment"># 3 show</span></div><div class="line">  <span class="comment"># 计算w</span></div><div class="line">  w = np.array([<span class="number">0</span>,<span class="number">0</span>])</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(g_m):</div><div class="line">    <span class="keyword">if</span> g_alpha[i]!=<span class="number">0</span>:</div><div class="line">      w=np.add(w,g_y_vec[i]*g_alpha[i]*g_x_mat[i])</div><div class="line"></div><div class="line">  showPic(w,g_b)</div><div class="line">  <span class="comment">#print(g_alpha)</span></div></pre></td></tr></table></figure>
<p>经过数轮迭代得到如下结果，其中直线为得到的分割曲线，蓝色点为支持向量，红色点是那些有良好分类的样本，绿色点为可容忍的误差样本。</p>
<p><img src="/images/机器学习/监督学习-支持向量机计算结果1.png" width="50%" height="50%" text-align="center/"></p>
<h2 id="9-核函数"><a href="#9-核函数" class="headerlink" title="9 核函数"></a>9 核函数</h2><p>SMO另个非常强大的地方上，它能够很好的解决非线性问题。我们之前的公式中有\(&lt;{x _i},{x _j}&gt;\)，他是两个向量的內积，代表着两个样本的相关性。我们把这个叫做核函数。核函数不仅仅是可以为简单的內积，还可以对样本进行多维展开，映射到高维地址空间。这样在低维地址空间线性不可分的样本，在高维空间就变得线性可分了。譬如,\(|x| &lt; 1\)不是线性可分的，但是对其进行\(x \to (x,{x^2})\)映射后，也就的到一个二次曲线，我们可以使用\(y = 1\)进行线性分割。</p>
<p>下面直接引入高斯核函数，它可以对函数进行无线维空间的映射。具体定义如下：</p>
$$k(x,z) = \exp ( - \frac{{||x - z|{|^2}}}{{2{\sigma ^2}}})$$
<blockquote>
<p>对于核函数的数学理论和几何意义，以及高斯核为啥可以向无限维空间映射，之后有时间需要详细研究。</p>
</blockquote>
<p>我们制造一组\(x _1^2 + x _2^2 = 1\)分割的样本，然后尝试对其进行分割，实际上与上一节程序的区别仅仅在于核函数的选取，这里我们使用高斯核函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div><div class="line">238</div><div class="line">239</div><div class="line">240</div><div class="line">241</div><div class="line">242</div><div class="line">243</div><div class="line">244</div><div class="line">245</div><div class="line">246</div><div class="line">247</div><div class="line">248</div><div class="line">249</div><div class="line">250</div><div class="line">251</div><div class="line">252</div><div class="line">253</div><div class="line">254</div><div class="line">255</div><div class="line">256</div><div class="line">257</div><div class="line">258</div><div class="line">259</div><div class="line">260</div><div class="line">261</div><div class="line">262</div><div class="line">263</div><div class="line">264</div><div class="line">265</div><div class="line">266</div><div class="line">267</div><div class="line">268</div><div class="line">269</div><div class="line">270</div><div class="line">271</div><div class="line">272</div><div class="line">273</div><div class="line">274</div><div class="line">275</div><div class="line">276</div><div class="line">277</div><div class="line">278</div><div class="line">279</div><div class="line">280</div><div class="line">281</div><div class="line">282</div><div class="line">283</div><div class="line">284</div><div class="line">285</div><div class="line">286</div><div class="line">287</div><div class="line">288</div><div class="line">289</div><div class="line">290</div><div class="line">291</div><div class="line">292</div><div class="line">293</div><div class="line">294</div><div class="line">295</div><div class="line">296</div><div class="line">297</div><div class="line">298</div><div class="line">299</div><div class="line">300</div><div class="line">301</div><div class="line">302</div><div class="line">303</div><div class="line">304</div><div class="line">305</div><div class="line">306</div><div class="line">307</div><div class="line">308</div><div class="line">309</div><div class="line">310</div><div class="line">311</div><div class="line">312</div><div class="line">313</div><div class="line">314</div><div class="line">315</div><div class="line">316</div><div class="line">317</div><div class="line">318</div><div class="line">319</div><div class="line">320</div><div class="line">321</div><div class="line">322</div><div class="line">323</div><div class="line">324</div><div class="line">325</div><div class="line">326</div><div class="line">327</div><div class="line">328</div><div class="line">329</div><div class="line">330</div><div class="line">331</div><div class="line">332</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> matplotlib.patches <span class="keyword">import</span> Circle</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">global</span> g_y_vec    <span class="comment"># g_y_vec为样本输出, 大小g_m</span></div><div class="line"><span class="keyword">global</span> g_x_mat    <span class="comment"># g_x_mat为样本的输入, 大小2*g_m</span></div><div class="line"><span class="keyword">global</span> g_m        <span class="comment"># g_m 为样本数目</span></div><div class="line"><span class="keyword">global</span> g_alpha    <span class="comment"># g_alpha 大小为g_m</span></div><div class="line"><span class="keyword">global</span> g_C</div><div class="line"><span class="keyword">global</span> g_w        <span class="comment"># 实际没有使用，而是通过alpha表示的</span></div><div class="line"><span class="keyword">global</span> g_b        <span class="comment"># y = wx+b</span></div><div class="line"><span class="keyword">global</span> g_y_now    <span class="comment"># 表示当前参数计算的y，即svmOutPut对应的g_y_now</span></div><div class="line"><span class="keyword">global</span> g_err      <span class="comment"># g_err 表示svmOutput - g_y_vec对应的序列</span></div><div class="line"></div><div class="line">g_x_mat = np.array([[<span class="number">0.602</span>, <span class="number">0.732</span>], [<span class="number">-0.599</span>, <span class="number">0.945</span>], [<span class="number">-0.337</span>, <span class="number">-0.677</span>], [<span class="number">0.459</span>, <span class="number">-0.486</span>], [<span class="number">1.056</span>, <span class="number">0.137</span>], [<span class="number">0.838</span>, <span class="number">0.623</span>], [<span class="number">1.022</span>, <span class="number">-0.685</span>], [<span class="number">0.504</span>, <span class="number">0.821</span>], [<span class="number">-0.977</span>, <span class="number">-0.724</span>], [<span class="number">1.116</span>, <span class="number">0.56</span>], [<span class="number">0.969</span>, <span class="number">0.151</span>], [<span class="number">-0.693</span>, <span class="number">0.077</span>], [<span class="number">1.042</span>, <span class="number">-0.146</span>], [<span class="number">0.705</span>, <span class="number">-0.215</span>], [<span class="number">1.024</span>, <span class="number">-0.322</span>], [<span class="number">1.025</span>, <span class="number">-0.172</span>], [<span class="number">0.306</span>, <span class="number">-1.12</span>], [<span class="number">-0.131</span>, <span class="number">0.008</span>], [<span class="number">-1.157</span>, <span class="number">-1.081</span>], [<span class="number">0.452</span>, <span class="number">-0.865</span>], [<span class="number">-1.117</span>, <span class="number">-0.533</span>], [<span class="number">-1.083</span>, <span class="number">-0.355</span>], [<span class="number">-0.982</span>, <span class="number">0.572</span>], [<span class="number">-1.053</span>, <span class="number">1.003</span>], [<span class="number">-0.553</span>, <span class="number">-0.434</span>], [<span class="number">-0.115</span>, <span class="number">0.283</span>], [<span class="number">0.785</span>, <span class="number">0.233</span>], [<span class="number">-0.926</span>, <span class="number">-0.299</span>], [<span class="number">-1.039</span>, <span class="number">0.581</span>], [<span class="number">0.869</span>, <span class="number">-1.033</span>], [<span class="number">0.754</span>, <span class="number">-1.091</span>], [<span class="number">-1.096</span>, <span class="number">-0.311</span>], [<span class="number">0.537</span>, <span class="number">0.508</span>], [<span class="number">-0.38</span>, <span class="number">-0.565</span>], [<span class="number">1.165</span>, <span class="number">0.219</span>], [<span class="number">-0.123</span>, <span class="number">0.431</span>], [<span class="number">1.048</span>, <span class="number">-0.896</span>], [<span class="number">-0.409</span>, <span class="number">0.299</span>], [<span class="number">0.537</span>, <span class="number">-0.126</span>], [<span class="number">0.985</span>, <span class="number">-0.577</span>], [<span class="number">-1.135</span>, <span class="number">1.025</span>], [<span class="number">-0.779</span>, <span class="number">0.81</span>], [<span class="number">0.547</span>, <span class="number">0.697</span>], [<span class="number">0.424</span>, <span class="number">-1.015</span>], [<span class="number">0.421</span>, <span class="number">-0.904</span>], [<span class="number">0.151</span>, <span class="number">-0.149</span>], [<span class="number">0.77</span>, <span class="number">-1.011</span>], [<span class="number">-0.401</span>, <span class="number">1.113</span>], [<span class="number">0.817</span>, <span class="number">0.573</span>], [<span class="number">0.87</span>, <span class="number">-0.266</span>], [<span class="number">-0.731</span>, <span class="number">0.418</span>], [<span class="number">-0.651</span>, <span class="number">0.063</span>], [<span class="number">0.731</span>, <span class="number">0.04</span>], [<span class="number">0.649</span>, <span class="number">0.677</span>], [<span class="number">-0.084</span>, <span class="number">-0.568</span>], [<span class="number">0.391</span>, <span class="number">-0.171</span>], [<span class="number">-1.07</span>, <span class="number">0.738</span>], [<span class="number">-0.307</span>, <span class="number">0.702</span>], [<span class="number">0.854</span>, <span class="number">1.125</span>], [<span class="number">0.093</span>, <span class="number">-0.148</span>], [<span class="number">-0.82</span>, <span class="number">0.969</span>], [<span class="number">0.11</span>, <span class="number">-1.011</span>], [<span class="number">0.672</span>, <span class="number">-0.261</span>], [<span class="number">0.6</span>, <span class="number">-0.262</span>], [<span class="number">0.28</span>, <span class="number">0.001</span>], [<span class="number">-0.005</span>, <span class="number">-0.544</span>], [<span class="number">-0.666</span>, <span class="number">0.046</span>], [<span class="number">-0.457</span>, <span class="number">-0.129</span>], [<span class="number">-1.02</span>, <span class="number">1.071</span>], [<span class="number">1.191</span>, <span class="number">0.121</span>], [<span class="number">0.665</span>, <span class="number">-0.884</span>], [<span class="number">0.412</span>, <span class="number">0.665</span>], [<span class="number">-0.992</span>, <span class="number">-1.165</span>], [<span class="number">-0.726</span>, <span class="number">-1.178</span>], [<span class="number">-0.886</span>, <span class="number">1.08</span>], [<span class="number">0.263</span>, <span class="number">0.481</span>], [<span class="number">-0.051</span>, <span class="number">0.668</span>], [<span class="number">0.933</span>, <span class="number">-0.008</span>], [<span class="number">-0.896</span>, <span class="number">-0.637</span>], [<span class="number">-0.605</span>, <span class="number">0.287</span>], [<span class="number">0.03</span>, <span class="number">-0.232</span>], [<span class="number">0.749</span>, <span class="number">0.012</span>], [<span class="number">1.175</span>, <span class="number">0.632</span>], [<span class="number">0.968</span>, <span class="number">1.106</span>], [<span class="number">-1.19</span>, <span class="number">0.82</span>], [<span class="number">0.641</span>, <span class="number">0.129</span>], [<span class="number">-0.375</span>, <span class="number">-1.079</span>], [<span class="number">-0.267</span>, <span class="number">-0.442</span>], [<span class="number">0.361</span>, <span class="number">-0.741</span>], [<span class="number">-0.475</span>, <span class="number">0.473</span>], [<span class="number">0.133</span>, <span class="number">1.18</span>], [<span class="number">1.146</span>, <span class="number">1.185</span>], [<span class="number">-0.293</span>, <span class="number">0.172</span>], [<span class="number">0.78</span>, <span class="number">-0.805</span>], [<span class="number">0.186</span>, <span class="number">-0.089</span>], [<span class="number">-0.068</span>, <span class="number">0.829</span>], [<span class="number">-0.621</span>, <span class="number">-0.778</span>], [<span class="number">0.407</span>, <span class="number">-0.523</span>], [<span class="number">0.415</span>, <span class="number">-0.01</span>], [<span class="number">-0.229</span>, <span class="number">0.002</span>], [<span class="number">-0.997</span>, <span class="number">-0.891</span>], [<span class="number">1.011</span>, <span class="number">-1.186</span>], [<span class="number">0.19</span>, <span class="number">-0.437</span>], [<span class="number">0.958</span>, <span class="number">0.669</span>], [<span class="number">-0.888</span>, <span class="number">-0.217</span>], [<span class="number">0.444</span>, <span class="number">0.05</span>], [<span class="number">-0.54</span>, <span class="number">-1.041</span>], [<span class="number">-0.314</span>, <span class="number">0.296</span>], [<span class="number">0.879</span>, <span class="number">-0.898</span>], [<span class="number">0.127</span>, <span class="number">-0.008</span>], [<span class="number">0.995</span>, <span class="number">-1.11</span>], [<span class="number">-0.878</span>, <span class="number">-0.843</span>], [<span class="number">-0.109</span>, <span class="number">0.189</span>], [<span class="number">0.859</span>, <span class="number">0.564</span>], [<span class="number">-0.023</span>, <span class="number">0.945</span>], [<span class="number">-0.878</span>, <span class="number">0.899</span>], [<span class="number">-0.062</span>, <span class="number">-1.051</span>], [<span class="number">0.394</span>, <span class="number">0.519</span>], [<span class="number">-1.139</span>, <span class="number">0.282</span>], [<span class="number">-0.494</span>, <span class="number">-0.075</span>], [<span class="number">-0.922</span>, <span class="number">1.11</span>], [<span class="number">0.753</span>, <span class="number">-1.018</span>], [<span class="number">0.816</span>, <span class="number">-1.106</span>], [<span class="number">0.03</span>, <span class="number">0.569</span>], [<span class="number">-1.11</span>, <span class="number">-0.289</span>], [<span class="number">0.777</span>, <span class="number">0.025</span>], [<span class="number">0.892</span>, <span class="number">0.784</span>], [<span class="number">0.91</span>, <span class="number">0.176</span>], [<span class="number">0.692</span>, <span class="number">0.099</span>], [<span class="number">0.97</span>, <span class="number">0.58</span>], [<span class="number">0.034</span>, <span class="number">1.151</span>], [<span class="number">-0.606</span>, <span class="number">-0.775</span>], [<span class="number">0.873</span>, <span class="number">-0.579</span>], [<span class="number">0.833</span>, <span class="number">-1.042</span>], [<span class="number">-0.251</span>, <span class="number">0.102</span>], [<span class="number">0.436</span>, <span class="number">-0.585</span>], [<span class="number">0.86</span>, <span class="number">-1.06</span>], [<span class="number">-1.118</span>, <span class="number">1.094</span>], [<span class="number">0.598</span>, <span class="number">-0.129</span>], [<span class="number">0.694</span>, <span class="number">0.281</span>], [<span class="number">1.048</span>, <span class="number">-1.036</span>], [<span class="number">-0.348</span>, <span class="number">0.639</span>], [<span class="number">1.046</span>, <span class="number">-1.124</span>], [<span class="number">-0.333</span>, <span class="number">-0.463</span>], [<span class="number">-0.447</span>, <span class="number">-0.009</span>], [<span class="number">0.344</span>, <span class="number">-0.852</span>], [<span class="number">-1.174</span>, <span class="number">0.196</span>], [<span class="number">0.701</span>, <span class="number">0.695</span>], [<span class="number">-0.916</span>, <span class="number">-0.128</span>], [<span class="number">-0.597</span>, <span class="number">-0.934</span>]])</div><div class="line">g_y_vec = np.array([<span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>])</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcLH</span><span class="params">(id1,id2)</span>:</span></div><div class="line">  <span class="keyword">if</span> g_y_vec[id1] == g_y_vec[id2]:</div><div class="line">    L = max(<span class="number">0</span>,g_alpha[id1]+g_alpha[id2]-g_C)</div><div class="line">    H = min(g_C,g_alpha[id1]+g_alpha[id2])</div><div class="line">  <span class="keyword">else</span>:</div><div class="line">    L = max(<span class="number">0</span>,g_alpha[id2]-g_alpha[id1])</div><div class="line">    H = min(g_C,g_C+g_alpha[id2]-g_alpha[id1])</div><div class="line">  <span class="keyword">return</span> (L,H)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">svmOutput</span><span class="params">(id1)</span>:</span></div><div class="line">  <span class="comment"># 这个函数是svm的实际输出，计算当前参数(w,b)下, 计算得到的y</span></div><div class="line">  <span class="comment"># 由于w = sum (alpha*y*x), 对于第i个分量x_i所以输出结果应该为w*x_i+b, 也就是 sum (alpha*y*&lt;x_1-m,x_i&gt;)</span></div><div class="line">  <span class="comment"># 注: 待会分析下alpha的变化趋势与C的关系</span></div><div class="line">  <span class="keyword">global</span> g_b</div><div class="line">  sum = <span class="number">0</span>;</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(g_m):</div><div class="line">    sum += g_alpha[i]*g_y_vec[i]*kernel(i,id1)</div><div class="line">  <span class="keyword">return</span> sum+g_b</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">kernel</span><span class="params">(id1,id2)</span>:</span></div><div class="line">  <span class="comment"># 这里核函数为简单的内积</span></div><div class="line">  <span class="comment"># id1和id2为int类型，是g_x_mat中的索引</span></div><div class="line">  x_1 = g_x_mat[id1]</div><div class="line">  x_2 = g_x_mat[id2]</div><div class="line">  val = np.subtract(x_1,x_2)</div><div class="line">  val = np.dot(val,val)</div><div class="line">  <span class="keyword">return</span> np.exp(-val)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">compareFun</span><span class="params">(id1,id2,L,H)</span>:</span></div><div class="line">  <span class="comment"># 如果返回1，表示L处去极小值。如果返回-1，H处去极小值。如果是0，表示这次不更新</span></div><div class="line">  <span class="comment"># 如果两者相等，这里略过, 说明无法有强力证据证明这个样本属于wx+b&gt;1还是wx+b&lt;1，所以等待下一轮迭代。因此，与L和H相等应该设置一个阀值，判断近似相等。</span></div><div class="line">  <span class="keyword">global</span> g_b</div><div class="line">  y_1 = g_y_vec[id1]</div><div class="line">  y_2 = g_y_vec[id2]</div><div class="line">  e_1 = g_err[id1]</div><div class="line">  e_2 = g_err[id2]</div><div class="line">  alpha_1 = g_alpha[id1]</div><div class="line">  alpha_2 = g_alpha[id2]</div><div class="line">  k11 = kernel(id1,id1)</div><div class="line">  k12 = kernel(id1,id2)</div><div class="line">  k22 = kernel(id2,id2)</div><div class="line">  s = y_1*y_2</div><div class="line">  f_1 = y_1*(e_1+g_b)-alpha_1*k11-s*alpha_2*k12</div><div class="line">  f_2 = y_2*(e_2+g_b)-s*alpha_1*k12-alpha_2*k22</div><div class="line">  L_1 = alpha_1+s*(alpha_2-L)</div><div class="line">  H_1 = alpha_1+s*(alpha_2-H)</div><div class="line">  phi_l = L_1*f_1+L*f_2+<span class="number">0.5</span>*L_1*L_1*k11+<span class="number">0.5</span>*L*L*k22+s*L*L_1*k12</div><div class="line">  phi_h = H_1*f_1+H*f_2+<span class="number">0.5</span>*H_1*H_1*k11+<span class="number">0.5</span>*H*H*k22+s*H*H_1*k12</div><div class="line">  <span class="keyword">if</span> L==H:</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span></div><div class="line">  <span class="keyword">if</span> phi_l &lt; phi_h:</div><div class="line">    <span class="keyword">return</span> <span class="number">1</span></div><div class="line">  <span class="keyword">else</span>:</div><div class="line">    <span class="keyword">return</span> <span class="number">-1</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeStep</span><span class="params">(id1,id2,err)</span>:</span></div><div class="line">  <span class="keyword">if</span> id1==id2:</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span></div><div class="line">  alpha_1 = g_alpha[id1]</div><div class="line">  alpha_2 = g_alpha[id2]</div><div class="line">  y_1 = g_y_vec[id1]</div><div class="line">  y_2 = g_y_vec[id2]</div><div class="line">  e1 = g_err[id1]</div><div class="line">  e2 = g_err[id2]</div><div class="line">  s = y_1*y_2</div><div class="line">  L,H=calcLH(id1,id2)</div><div class="line">  <span class="comment">#print("id1=",id1,", id2=",id2," L=",L,", H=",H)</span></div><div class="line">  <span class="keyword">if</span> L==H :</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span></div><div class="line">  k11 = kernel(id1,id1)</div><div class="line">  k12 = kernel(id1,id2)</div><div class="line">  k22 = kernel(id2,id2)</div><div class="line">  eta = k11+k22<span class="number">-2</span>*k12</div><div class="line">  <span class="comment">#print("kernel:",k11,k12,k22,", eta=",eta,"e1=",e1,"e2=",e2)</span></div><div class="line"></div><div class="line">  alpha_2_new = alpha_2</div><div class="line">  <span class="comment"># 如果eta大于0, 我们可知最小值在边界或极小值点上。事实上，如果极小值不在范围内，必在距离极小值近的那个边界上。</span></div><div class="line">  <span class="comment"># 如果eta小于0, 我们可知最小值则必在边界上。我们只需要比较两个边界点函数的大小即可。</span></div><div class="line">  <span class="keyword">if</span> eta&gt;<span class="number">0</span> :</div><div class="line">    alpha_2_new = alpha_2+y_2*(e1-e2)/eta</div><div class="line">    alpha_2_new = max(alpha_2_new,L)</div><div class="line">    alpha_2_new = min(alpha_2_new,H)</div><div class="line">  <span class="keyword">else</span>:</div><div class="line">    <span class="comment"># 由于我们知道可以直接这是一个关于alpha的二次函数，并且自变量的取值范围是[L,H], 事实上我们只需要比较alpha_2_new离L和H哪个远即可</span></div><div class="line">    <span class="comment"># 但是考虑到eta=0的一次函数特殊情况，我们还是老老实实的计算函数值吧。</span></div><div class="line">    ret = compareFun(id1, id2, L, H)</div><div class="line">    <span class="keyword">if</span> ret == <span class="number">0</span>:</div><div class="line">      alpha_2_new = alpha_2</div><div class="line">    <span class="keyword">elif</span> ret == <span class="number">1</span>:</div><div class="line">      alpha_2_new = L</div><div class="line">    <span class="keyword">elif</span> ret == <span class="number">1</span>:</div><div class="line">      alpha_2_new = H</div><div class="line">    print(<span class="string">"----------------eta&lt;=0----------------"</span>)</div><div class="line"></div><div class="line">  <span class="comment"># 归整化alpha_2_new</span></div><div class="line">  <span class="keyword">if</span> alpha_2_new &lt; err:</div><div class="line">    alpha_2_new = <span class="number">0</span></div><div class="line">  <span class="keyword">elif</span> alpha_2_new &gt; g_C - err:</div><div class="line">    alpha_2_new = g_C</div><div class="line">  <span class="keyword">if</span> abs(alpha_2_new-alpha_2) &lt; err:</div><div class="line">    print(<span class="string">"alpha_2 is no need to update"</span>)</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span></div><div class="line"></div><div class="line">  <span class="comment"># update b</span></div><div class="line">  <span class="keyword">global</span> g_b</div><div class="line">  alpha_1_new = alpha_1 + s * (alpha_2 - alpha_2_new)      <span class="comment"># 不必担心alpha_1_new不在[0,C]范围内，之前的公式已经保证了</span></div><div class="line">  b1_new = -e1-y_1*k11*(alpha_1_new-alpha_1)-y_2*k12*(alpha_2_new-alpha_2) + g_b</div><div class="line">  b2_new = -e2-y_1*k12*(alpha_1_new-alpha_1)-y_2*k22*(alpha_2_new-alpha_2) + g_b</div><div class="line">  <span class="keyword">if</span> alpha_1_new&gt;<span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> alpha_1_new&lt;g_C:</div><div class="line">    g_b = b1_new</div><div class="line">  <span class="keyword">elif</span> alpha_2_new&gt;<span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> alpha_2_new&lt;g_C:</div><div class="line">    g_b = b2_new</div><div class="line">  <span class="keyword">else</span>:</div><div class="line">    g_b = <span class="number">0.5</span>*(b1_new+b2_new)</div><div class="line"></div><div class="line">  <span class="comment"># update alpha</span></div><div class="line">  g_alpha[id1] = alpha_1_new</div><div class="line">  g_alpha[id2] = alpha_2_new</div><div class="line">  print(<span class="string">"g_alpha[id1]="</span>,g_alpha[id1],<span class="string">"g_alpha[id2]="</span>,g_alpha[id2],<span class="string">"s="</span>,s,<span class="string">", alpha_1="</span>,alpha_1,<span class="string">", alpha_2="</span>,alpha_2)</div><div class="line"></div><div class="line">  <span class="comment"># update err g_y_now</span></div><div class="line">  updateYAndErr()</div><div class="line"></div><div class="line">  <span class="keyword">return</span> <span class="number">1</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateYAndErr</span><span class="params">()</span>:</span></div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(g_m):</div><div class="line">    g_y_now[i] = svmOutput(i)</div><div class="line">    g_err[i] = svmOutput(i)-g_y_vec[i]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestAlphaIndex</span><span class="params">(id2)</span>:</span></div><div class="line">  maxIncr = <span class="number">0</span></div><div class="line">  maxIndex = <span class="number">-1</span>;</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(g_m):</div><div class="line">    incr = abs(g_err[i]-g_alpha[i])</div><div class="line">    <span class="keyword">if</span> incr &gt;= maxIncr:</div><div class="line">      maxIndex = i</div><div class="line">      maxIncr = incr</div><div class="line">  <span class="keyword">return</span> maxIndex</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sizeOfNonZerorAndNonC</span><span class="params">()</span>:</span></div><div class="line">  size=<span class="number">0</span></div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(g_m):</div><div class="line">    <span class="keyword">if</span> g_alpha[i]!=<span class="number">0</span> <span class="keyword">and</span> g_alpha[i]!=g_C:</div><div class="line">      size= size+<span class="number">1</span></div><div class="line">  <span class="keyword">return</span> size</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseRandomIndex</span><span class="params">(id2)</span>:</span></div><div class="line">  ret = id2;</div><div class="line">  <span class="keyword">while</span> ret==id2:</div><div class="line">    ret = random.randint(<span class="number">0</span>,g_m<span class="number">-1</span>)</div><div class="line">  <span class="keyword">return</span> ret</div><div class="line"></div><div class="line"><span class="comment"># 对于examineExample函数,我们一次进选择id2样本对应的alpha与如下规则选择的id1对应的alpha,然后相应跟新其值。</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">examineExample</span><span class="params">(id2)</span>:</span></div><div class="line">  y_2 = g_y_vec[id2]</div><div class="line">  tol = <span class="number">1e-2</span>        <span class="comment"># 是一个正数</span></div><div class="line">  alpha_2 = g_alpha[id2]</div><div class="line">  e_2 = svmOutput(id2)-g_y_vec[id2]</div><div class="line">  r_2 = e_2 * y_2</div><div class="line"></div><div class="line">  <span class="comment"># 我们只对当前违反kkt条件的样本对应的alpha进行更新</span></div><div class="line">  <span class="comment"># 关于违反kkt条件的说明:</span></div><div class="line">  <span class="comment"># (1) r_2 &lt; -tol 表示r_2小于0，即表示输出的预测结果与样本的y符号相反，因此应该属于误差案例的。所以，根据公式，alpha = C。如果alpha &lt; C ,比违反kkt条件</span></div><div class="line">  <span class="comment"># (2) r_2 &lt; -tol 表示r_2大于0，即表示输出的预测结果与样本的y符号相同，当误差大于一定的</span></div><div class="line">  <span class="keyword">if</span> r_2 &lt; -tol <span class="keyword">and</span> alpha_2 &lt; g_C <span class="keyword">or</span> r_2 &gt; tol <span class="keyword">and</span> alpha_2 &gt; <span class="number">0</span> :</div><div class="line">    <span class="comment"># 下面的程序逻辑是这样的:</span></div><div class="line">    <span class="comment"># 先遍历alpha非0或非C, 因为我们对于alpha为0和alpha为C的情况, 认为是处于非支持向量和处于误差样本的情况。我们只有根据支持向量下，找到最优的||w||才有意义</span></div><div class="line">    <span class="comment"># 首先，我们找到|e1-e2|最大的alpha, 从这里优化。如果优化结果不理想，我们就随机找一个alpha一起计算。如果还不行，就在整个范围alpha范围内计算</span></div><div class="line">    <span class="keyword">if</span> sizeOfNonZerorAndNonC()&gt;<span class="number">0</span>:</div><div class="line">      id1=chooseBestAlphaIndex(id2)</div><div class="line">      <span class="keyword">if</span> takeStep(id1,id2,<span class="number">1e-3</span>):</div><div class="line">        <span class="comment">#print("takeStep1, alpha[",id1,"]=",g_alpha[id1],", alphapp[",id2,"]=",g_alpha[id2])</span></div><div class="line">        <span class="keyword">return</span> <span class="number">1</span></div><div class="line">    r=chooseRandomIndex(id2)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(g_m):</div><div class="line">      id1 = (r+i)%g_m</div><div class="line">      <span class="keyword">if</span> id1!=id2 <span class="keyword">and</span> g_alpha[id1]!=<span class="number">0</span> <span class="keyword">and</span> g_alpha[id1]!=g_C:</div><div class="line">        <span class="keyword">if</span> takeStep(id1,id2,<span class="number">1e-3</span>):</div><div class="line">          <span class="comment">#print("takeStep2, alpha[", id1, "]=", g_alpha[id1], ", alphapp[", id2, "]=", g_alpha[id2])</span></div><div class="line">          <span class="keyword">return</span> <span class="number">1</span></div><div class="line">    r = chooseRandomIndex(id2)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(g_m):</div><div class="line">      id1 = (r + i) % g_m</div><div class="line">      <span class="keyword">if</span> id1 != id2:</div><div class="line">        <span class="keyword">if</span> takeStep(id1,id2,<span class="number">1e-3</span>):</div><div class="line">          <span class="comment">#print("takeStep3, alpha[", id1, "]=", g_alpha[id1], ", alphapp[", id2, "]=", g_alpha[id2])</span></div><div class="line">          <span class="keyword">return</span> <span class="number">1</span></div><div class="line">  <span class="keyword">return</span> <span class="number">0</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">kernel_test</span><span class="params">(id1,x)</span>:</span></div><div class="line">  <span class="comment"># 这里核函数为简单的内积</span></div><div class="line">  <span class="comment"># id1和id2为int类型，是g_x_mat中的索引</span></div><div class="line">  x_1 = g_x_mat[id1]</div><div class="line">  val = np.subtract(x_1,x)</div><div class="line">  val = np.dot(val,val)</div><div class="line">  <span class="keyword">return</span> np.exp(-val)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">svmOutput_test</span><span class="params">(alpha,b,x)</span>:</span></div><div class="line">  <span class="comment"># 这个函数是svm的实际输出，计算当前参数(w,b)下, 计算得到的y</span></div><div class="line">  <span class="comment"># 由于w = sum (alpha*y*x), 对于第i个分量x_i所以输出结果应该为w*x_i+b, 也就是 sum (alpha*y*&lt;x_1-m,x_i&gt;)</span></div><div class="line">  <span class="comment"># 注: 待会分析下alpha的变化趋势与C的关系</span></div><div class="line">  <span class="keyword">global</span> g_m</div><div class="line">  sum = <span class="number">0</span></div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(g_m):</div><div class="line">    <span class="keyword">if</span> alpha[i]!=<span class="number">0</span>:</div><div class="line">      sum += alpha[i]*g_y_vec[i]*kernel_test(i,x)</div><div class="line">  <span class="keyword">return</span> sum+\</div><div class="line">         b</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">showPic</span><span class="params">(alpha,b)</span>:</span></div><div class="line">  figure, ax = plt.subplots()</div><div class="line">  ax.set_xlim(left=<span class="number">-2</span>, right=<span class="number">2</span>)</div><div class="line">  ax.set_ylim(bottom=<span class="number">-2</span>, top=<span class="number">2</span>)</div><div class="line"></div><div class="line">  <span class="comment"># 生成100组测试数据</span></div><div class="line">  x=[]</div><div class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">100</span>):</div><div class="line">    x_0 = random.randint(<span class="number">-1200</span>,<span class="number">1200</span>)/<span class="number">1000</span></div><div class="line">    x_1 = random.randint(<span class="number">-1200</span>,<span class="number">1200</span>)/<span class="number">1000</span></div><div class="line">    x.append([x_0,x_1])</div><div class="line"></div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</div><div class="line">    <span class="keyword">if</span> svmOutput_test(alpha,b,x[i])&gt;<span class="number">0</span>:</div><div class="line">      plt.plot(x[i][<span class="number">0</span>], x[i][<span class="number">1</span>], <span class="string">'b--'</span>, marker=<span class="string">'+'</span>, color=<span class="string">'r'</span>)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">      plt.plot(x[i][<span class="number">0</span>], x[i][<span class="number">1</span>], <span class="string">'b--'</span>, marker=<span class="string">'o'</span>, color=<span class="string">'b'</span>)</div><div class="line"></div><div class="line">  <span class="comment"># draw x_0*x_0+x_1*x_1=0</span></div><div class="line">  cir1 = Circle(xy=(<span class="number">0.0</span>, <span class="number">0.0</span>), radius=<span class="number">1</span>)</div><div class="line">  ax.add_patch(cir1)</div><div class="line"></div><div class="line">  plt.xlabel(<span class="string">"x1"</span>)</div><div class="line">  plt.ylabel(<span class="string">"x2"</span>)</div><div class="line">  plt.plot()</div><div class="line">  plt.show()</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</div><div class="line">  SHOW_PIC = <span class="keyword">True</span></div><div class="line"></div><div class="line">  g_m = len(g_x_mat)</div><div class="line">  <span class="keyword">if</span> SHOW_PIC == <span class="keyword">False</span>:</div><div class="line">    <span class="comment"># 1 training</span></div><div class="line">    g_alpha = np.zeros(g_m)</div><div class="line">    g_y_now = np.zeros(g_m)</div><div class="line">    g_err = np.zeros(g_m)</div><div class="line">    <span class="keyword">global</span> g_b</div><div class="line">    g_b = <span class="number">0</span></div><div class="line">    numChanged = <span class="number">0</span></div><div class="line">    examineAll = <span class="number">1</span></div><div class="line">    g_C = <span class="number">10</span></div><div class="line">    err = <span class="number">0</span></div><div class="line">    updateYAndErr()       <span class="comment"># 实现更新下缓冲，即当前输出与误差值</span></div><div class="line">    <span class="keyword">while</span> numChanged&gt;<span class="number">0</span> <span class="keyword">or</span> examineAll:</div><div class="line">      numChanged = <span class="number">0</span></div><div class="line">      <span class="comment"># 循环处理，第一次对所有的样本进行一次处理。</span></div><div class="line">      <span class="comment"># 然后对所有非边界的数值进行处理。因为在当前参数下，非边界的样本，我们认为其是支持向量。对于优化||w||的大小，支持向量才有意义。</span></div><div class="line">      <span class="keyword">if</span> examineAll:</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(g_m):</div><div class="line">          numChanged += examineExample(i)</div><div class="line">          <span class="comment">#print("examineAll=1, numChanged=",numChanged)</span></div><div class="line">      <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(g_m):</div><div class="line">          <span class="keyword">if</span> g_alpha[i]!=<span class="number">0</span> <span class="keyword">and</span> g_alpha[i]!=g_C:</div><div class="line">            numChanged += examineExample(i)</div><div class="line">      examineAll = abs(examineAll<span class="number">-1</span>)</div><div class="line">    print(g_alpha, g_b)</div><div class="line">  <span class="keyword">else</span>:</div><div class="line">    <span class="comment"># 3 show</span></div><div class="line">    alpha= [  <span class="number">0.00000000e+00</span>,   <span class="number">7.59410972e-01</span>,  <span class="number">-2.22044605e-16</span>,   <span class="number">0.00000000e+00</span>,</div><div class="line">    <span class="number">1.00000000e+01</span>,   <span class="number">1.00000000e+01</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</div><div class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">1.00000000e+01</span>,   <span class="number">0.00000000e+00</span>,</div><div class="line">    <span class="number">5.67018243e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">1.00000000e+01</span>,</div><div class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">1.00000000e+01</span>,</div><div class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">2.45788486e+00</span>,   <span class="number">1.33286169e+00</span>,   <span class="number">0.00000000e+00</span>,</div><div class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">1.11022302e-16</span>,   <span class="number">1.00000000e+01</span>,</div><div class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">7.22541284e+00</span>,</div><div class="line">    <span class="number">0.00000000e+00</span>,  <span class="number">-1.38777878e-17</span>,   <span class="number">0.00000000e+00</span>,  <span class="number">-2.77555756e-17</span>,</div><div class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</div><div class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">8.66273771e-04</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">8.33442222e+00</span>,</div><div class="line">    <span class="number">1.00000000e+01</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</div><div class="line">    <span class="number">1.00000000e+01</span>,   <span class="number">1.00000000e+01</span>,  <span class="number">-2.77555756e-17</span>,   <span class="number">0.00000000e+00</span>,</div><div class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</div><div class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</div><div class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">1.00000000e+01</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</div><div class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</div><div class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">4.39461998e+00</span>,   <span class="number">0.00000000e+00</span>,</div><div class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</div><div class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">1.00000000e+01</span>,   <span class="number">1.00000000e+01</span>,   <span class="number">0.00000000e+00</span>,</div><div class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</div><div class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,  <span class="number">-2.22044605e-16</span>,   <span class="number">0.00000000e+00</span>,</div><div class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</div><div class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</div><div class="line">    <span class="number">1.00000000e+01</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</div><div class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,  <span class="number">-2.77555756e-17</span>,</div><div class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</div><div class="line">    <span class="number">-5.55111512e-17</span>,  <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">6.59194921e-17</span>,</div><div class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">1.00000000e+01</span>,   <span class="number">1.00000000e+01</span>,  <span class="number">-1.38777878e-17</span>,</div><div class="line">    <span class="number">5.21868376e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</div><div class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</div><div class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,  <span class="number">-2.22044605e-16</span>,   <span class="number">6.56986459e+00</span>,</div><div class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">5.19744490e+00</span>,   <span class="number">9.69510083e+00</span>,   <span class="number">1.00000000e+01</span>,</div><div class="line">    <span class="number">1.00000000e+01</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</div><div class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</div><div class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</div><div class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">8.69782724e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">1.00000000e+01</span>,</div><div class="line">    <span class="number">3.24960991e+00</span>,   <span class="number">8.23041098e+00</span>]</div><div class="line">    b = <span class="number">-2.68518972087</span></div><div class="line">    <span class="comment">#w = np.array([0, 0])</span></div><div class="line">    <span class="comment">#for i in range(g_m):</span></div><div class="line">    <span class="comment">#  if g_alpha[i] != 0:</span></div><div class="line">    <span class="comment">#    w = np.add(w, g_y_vec[i] * g_alpha[i] * g_x_mat[i])</span></div><div class="line">    showPic(alpha,b)</div></pre></td></tr></table></figure>
<p>经过数轮迭代之后，得到参数。然后在随机产生一些样本，通过训练集得到参数对随机测试样本进行分割，结果如下，发现分割效果还是很理想的。</p>
<p><img src="/images/机器学习/监督学习-支持向量机计算结果2.png" width="50%" height="50%" text-align="center/"></p>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>手写公式和word版博客地址:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">https://github.com/zhengchenyu/MyBlog/tree/master/doc/mlearning/支持向量机</div></pre></td></tr></table></figure>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li>cs229-notes3</li>
<li>机器学习 周志华版</li>
<li>Fast Training of Support Vector Machines Using Sequential Minimal Optimization</li>
<li><a href="http://blog.csdn.net/luoshixian099/article/details/51227754" target="_blank" rel="external">http://blog.csdn.net/luoshixian099/article/details/51227754</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-机器学习-2-6-监督学习之朴素贝叶斯算法" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2017/08/05/机器学习-2-6-监督学习之朴素贝叶斯算法/" class="article-date">
      <time datetime="2017-08-05T13:11:31.000Z" itemprop="datePublished">2017-08-05</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/08/05/机器学习-2-6-监督学习之朴素贝叶斯算法/">机器学习-2.6-监督学习之朴素贝叶斯算法</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <script type="text/javascript" src="/Users/zcy/Desktop/study/git/MathJax-master/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h1 id="朴素贝叶斯算法"><a href="#朴素贝叶斯算法" class="headerlink" title="朴素贝叶斯算法"></a>朴素贝叶斯算法</h1><h2 id="1-朴素贝叶斯算法"><a href="#1-朴素贝叶斯算法" class="headerlink" title="1 朴素贝叶斯算法"></a>1 朴素贝叶斯算法</h2><h3 id="1-1-垃圾邮件处理模型"><a href="#1-1-垃圾邮件处理模型" class="headerlink" title="1.1 垃圾邮件处理模型"></a>1.1 垃圾邮件处理模型</h3><p>以预测垃圾邮件为例子，介绍贝叶斯算法。词库中有5000个单词。我们有输入\(x\)，\(x \in {\{ 0,1\} ^{50000}}\)，是一个50000维的向量，其中\(x _i\)表示词库中第\(i\)个单次在该邮件中出现，为\(0\)表示词库中第\(i\)个单次在该邮件中没有出现。</p>
<p>已经邮件是否为邮件,样本\(({x _1},{x _2},…,{x _i})\)出现的概率为\(p({x _1},{x _2},…,{x _i}|y)\)，可以定义为下式:</p>
$$p({x _1},{x _2},...,{x _i}|y) = p({x _1}|y)p({x _2}|y,{x _1})...p({x _{50000}}|y,{x _1},...,{x _{49999}})$$

<p>注: 上式很好理解。右侧第一个式子为已知\(y\)，\(x _1\)出现的概率。第二个为一直\(y\)和\(x _1\)出现\(x _2\)的概率。因此两个式子相乘表示一直\(y\)初选\((x1,x2)\)的概率。依次类推，可以得到这个公式。</p>

$$p({x _1},{x _2},...,{x _i}|y) = p({x _1}|y)p({x _2}|y)...p({x _{50000}}|y) = \prod\limits _{i = 1}^{50000} {p({x _i}|y)}$$

<h3 id="1-2-公式推导"><a href="#1-2-公式推导" class="headerlink" title="1.2 公式推导"></a>1.2 公式推导</h3><p>关于最大释然函数如何设置，我们需要知道我们的目标是找到参数\(\theta\)，使得训练时候在给定\(x\)的情况下，得到最准确\(y\)的概率最大，最大释然函数就是这些概率的连乘，具体是需要保证下面的式子最大:</p>

$$\ln \prod\limits _{i = 1}^m {p(y = {y^i}|x = {x^i};\theta )p(x = {x^i})} $$

<p>在很多教材中都介绍最大释然函数是如下的表达，实际上是一样的。下面的式子的直接意义是在\(\theta\)已知情况下，出现\((x^i,y^i)\)的概率，实际上面的分析过程是一样的。</p>
$$\ln \prod\limits _{i = 1}^m {p(y = {y^i},x = {x^i};\theta )} $$
<blockquote>
<p>后面为了简写后面省略了\(\theta\)。</p>
</blockquote>
<p>我们做如下设置:</p>

$${\phi _y} = p(y = 1)$$
$${\phi _{i|y = 1}} = p({x _i} = 1|y = 1)$$
$${\phi _{i|y = 0}} = p({x _i} = 1|y = 0)$$

<p>然后得到最大释然函数:</p>

$$\ell ({\phi _y},{\phi _{i|y = 1}},{\phi _{i|y = 0}}) = \ln \prod\limits _{i = 1}^m {p(y = {y^i},x = {x^i})}  = \ln \prod\limits _{i = 1}^m {p({x^i}|{y^i})p({y^i})}$$
$$\ell ({\phi _y},{\phi _{i|y = 1}},{\phi _{i|y = 0}}) = \ln \prod\limits _{i = 1}^m {[[1\{ {y^i} = 1\} \ln ((\prod\limits _{j = 1}^n {p(x _j^i|{y^i})p({y^i} = 1)} ){\phi _y})][1\{ {y^i} = 1\} \ln ((\prod\limits _{j = 1}^n {p(x _j^i|{y^i})p({y^i} = 0)} )(1 - {\phi _y}))]} ]$$
$$\ell ({\phi _y},{\phi _{i|y = 1}},{\phi _{i|y = 0}}) = \sum\limits _{i = 1}^m {[[1\{ {y^i} = 1\} (\ln {\phi _y} + \ln \prod\limits _{j = 1}^n {({{({\phi _{j|y = 1}})}^{1\{ {x _j} = 1\} }}{{(1 - {\phi _{j|y = 1}})}^{1\{ {x _j} = 0\} }})} )][1\{ {y^i} = 0\} (\ln (1 - {\phi _y}) + \ln \prod\limits _{j = 1}^n {({{({\phi _{j|y = 0}})}^{1\{ {x _j} = 1\} }}{{(1 - {\phi _{j|y = 0}})}^{1\{ {x _j} = 0\} }})} )]]}$$
$$\ell ({\phi _y},{\phi _{i|y = 1}},{\phi _{i|y = 0}}) = \sum\limits  _{i = 1}^m {[[1\{ {y^i} = 1\} (\ln {\phi _y} + \sum\limits _{j = 1}^n {(1\{ x _j^i = 1\} \ln {\phi _{j|y = 1}} + 1\{ x _j^i = 0\} \ln (1 - {\phi _{j|y = 1}}))} )][1\{ {y^i} = 0\} (\ln (1 - {\phi _y}) + \sum\limits _{j = 1}^n {(1\{ x _j^i = 1\} \ln {\phi _{j|y = 0}} + 1\{ x _j^i = 0\} \ln (1 - {\phi _{j|y = 0}}))} )]]}$$

<p>然后我们对\(\phi _y\)求偏导数：</p>

$$\frac{{\partial \ell ({\phi _y},{\phi _{i|y = 1}},{\phi _{i|y = 0}})}}{\partial {\phi _y}} = \sum\limits _{i = 1}^m {(1\{ {y^i} = 1\} \frac{1}{\phi _y} + 1\{ {y^i} = 0\} \frac{ - 1}{1 - {\phi _y}})}  = \sum\limits _{i = 1}^m {\frac{1\{ {y^i} = 1\}  - {\phi _y}}{\phi _y}(1 - {\phi _y})} = 0$$
<p>所以有:</p>
$${\phi _y} = \frac{1}{m}\sum\limits _{i = 1}^m {1\{ {y^i} = 1\} }$$

<p>然后继续求偏导数:</p>
$$\frac{{\partial \ell ({\phi _y},{\phi _{i|y = 1}},{\phi _{i|y = 0}})}}{{\partial {\phi _{i|y = 1}}}} = \sum\limits _{i = 1}^m {1\{ {y^i} = 1\} (1\{ x _j^i = 1\} \frac{1}{{{\phi _{x _j^i = 1|y = 1}}}} + 1\{ x _j^i = 0\} \frac{{ - 1}}{{1 - {\phi _{x _j^i = 1|y = 1}}}})}=0$$

$$\frac{{\partial \ell ({\phi _y},{\phi _{i|y = 1}},{\phi _{i|y = 0}})}}{{\partial {\phi _{i|y = 1}}}} = \sum\limits _{i = 1}^m {1\{ {y^i} = 1\} (\frac{{1\{ x _j^i = 1\}  - {\phi _{i|y = 1}}}}{{{\phi _{i|y = 1}}(1 - {\phi _{i|y = 1}})}})}=0$$

<p>如果一个单词不再训练样本中，就会出现0/0的现象。避免这个事件发生，引入拉普拉斯平滑。所以有:</p>

$${\phi _{i|y = 1}} = \frac{{\sum\limits _{i = 1}^m {1\{ x _j^i = 1,{y^i} = 1\} }  + 1}}{{\sum\limits _{i = 1}^m {1\{ {y^i} = 1\} }  + 2}}$$

<p>同理有:</p>
$${\phi _{i|y = 0}} = \frac{{\sum\limits _{i = 1}^m {1\{ x _j^i = 1,{y^i} = 0\} }  + 1}}{{\sum\limits _{i = 1}^m {1\{ {y^i} = 1\} }  + 2}}$$

<p>注:试分析这样的公式，\(\phi <em>y\)就是总的垃圾邮件数目除以从样本数。\({\phi </em>{i|y = 1}}\)就是所有垃圾邮件中出现\(x _j^i\)对应的单词的数目除以所有垃圾邮件的数目。实际上这是个很容理解的道理。当然这就是数学的魅力，即便很简单的公司都是有着其理论依据的。</p>
<h3 id="1-3-算法实现"><a href="#1-3-算法实现" class="headerlink" title="1.3 算法实现"></a>1.3 算法实现</h3><p>我们使用样本得到各个单词的概率分布，然后预测邮件是否为垃圾邮件。运行下面的程序，可以得到了正确的垃圾邮件分类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> re</div><div class="line"></div><div class="line"><span class="keyword">global</span> DEBUG</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">constructDic</span><span class="params">()</span>:</span></div><div class="line">  file = open(<span class="string">"../res/NaiveBayes/dict.txt"</span>)</div><div class="line">  <span class="comment"># key is the word, value is the sequence number</span></div><div class="line">  dict = &#123;&#125;           <span class="comment"># In fact, treeMap is better than dict</span></div><div class="line">  count = <span class="number">0</span>;</div><div class="line">  <span class="keyword">while</span> <span class="number">1</span>:</div><div class="line">    line = file.readline()</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> line:</div><div class="line">      <span class="keyword">break</span></div><div class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> line.split():</div><div class="line">      <span class="keyword">if</span> word.lower() <span class="keyword">not</span> <span class="keyword">in</span> dict:</div><div class="line">        dict[word.lower()]=count</div><div class="line">        count = count+<span class="number">1</span></div><div class="line">  <span class="keyword">return</span> dict</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">findIndexInDict</span><span class="params">(word,dict)</span>:</span></div><div class="line">  <span class="keyword">if</span> word <span class="keyword">in</span> dict:</div><div class="line">    <span class="keyword">return</span> dict[word]</div><div class="line">  <span class="keyword">else</span>:</div><div class="line">    <span class="keyword">return</span> <span class="number">-1</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parseSpamOrHam</span><span class="params">(words)</span>:</span></div><div class="line">  <span class="keyword">if</span> words[<span class="number">0</span>] == <span class="string">"spam"</span>:</div><div class="line">    <span class="keyword">return</span> <span class="number">1</span></div><div class="line">  <span class="keyword">elif</span> words[<span class="number">0</span>] == <span class="string">"ham"</span>:</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span></div><div class="line">  <span class="keyword">else</span>:</div><div class="line">    <span class="keyword">if</span> DEBUG:</div><div class="line">      print(<span class="string">"error email type in training sample!"</span>)</div><div class="line">  <span class="keyword">return</span> <span class="number">-1</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitEmail</span><span class="params">(line)</span>:</span></div><div class="line">  regEx = re.compile(<span class="string">r'[^a-zA-Z]|\d'</span>)</div><div class="line">  <span class="keyword">return</span> list(filter(<span class="keyword">lambda</span> word: word!=<span class="string">""</span>, regEx.split(line)))</div><div class="line"></div><div class="line"><span class="comment"># spam is 1, ham is 0</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parseEmails</span><span class="params">(x_arr,y_arr,dict,dictLen)</span>:</span></div><div class="line">  file = open(<span class="string">"../res/NaiveBayes/emails.txt"</span>)</div><div class="line">  <span class="keyword">while</span> <span class="number">1</span>:</div><div class="line">    line = file.readline()</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> line:</div><div class="line">      <span class="keyword">break</span></div><div class="line">    words = splitEmail(line)</div><div class="line">    y = parseSpamOrHam(words)</div><div class="line">    x = np.zeros(dictLen)</div><div class="line">    <span class="keyword">if</span> y == <span class="number">-1</span>:</div><div class="line">      <span class="keyword">continue</span>;</div><div class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words[<span class="number">1</span>:]:</div><div class="line">      <span class="keyword">if</span> word.lower() <span class="keyword">in</span> dict:</div><div class="line">        index = dict[word.lower()]</div><div class="line">        x[index]=<span class="number">1</span></div><div class="line">      <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">if</span> DEBUG:</div><div class="line">          print(word,<span class="string">" is not in dic!"</span>)</div><div class="line">    x_arr.append(x)</div><div class="line">    y_arr.append(y)</div><div class="line">    <span class="comment">#if DE<span class="doctag">BUG:</span></span></div><div class="line">    <span class="comment"># for i in range(len(x)):</span></div><div class="line">    <span class="comment">#  if x[i] == 1:</span></div><div class="line">    <span class="comment">#    print(i)</span></div><div class="line">  <span class="keyword">return</span> dict</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calPhiY</span><span class="params">(y_arr)</span>:</span></div><div class="line">  <span class="comment"># p(y=1)</span></div><div class="line">  sum = <span class="number">0</span></div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> y_arr:</div><div class="line">    sum = sum + i</div><div class="line">  <span class="keyword">return</span> sum/len(y_arr)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calPhiXY</span><span class="params">(y_arr,x_arr,knownY,dictLen)</span>:</span></div><div class="line">  <span class="comment"># return a vector</span></div><div class="line">  sum = <span class="number">0</span></div><div class="line">  ret = np.zeros(dictLen)</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(len(y_arr)):</div><div class="line">    <span class="keyword">if</span> y_arr[i]!=knownY:</div><div class="line">      <span class="keyword">continue</span></div><div class="line">    sum=sum+<span class="number">1</span></div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(dictLen):</div><div class="line">      <span class="keyword">if</span> x_arr[i][j]==<span class="number">1</span>:</div><div class="line">       ret[j]=ret[j]+<span class="number">1</span></div><div class="line">  <span class="keyword">return</span> (ret+<span class="number">1</span>)/(sum+<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(phi,phi_y0,phi_y1,dictLen)</span>:</span></div><div class="line">  file = open(<span class="string">"../res/NaiveBayes/testEmails.txt"</span>)</div><div class="line">  <span class="keyword">while</span> <span class="number">1</span>:</div><div class="line">    line = file.readline()</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> line:</div><div class="line">      <span class="keyword">break</span></div><div class="line">    words = splitEmail(line)[<span class="number">1</span>:]</div><div class="line">    x = np.zeros(dictLen)</div><div class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</div><div class="line">      <span class="keyword">if</span> word.lower() <span class="keyword">in</span> dict:</div><div class="line">        index = dict[word.lower()]</div><div class="line">        x[index]=<span class="number">1</span></div><div class="line">    res = calcPro(phi,phi_y1,x)/(calcPro(phi,phi_y1,x)+calcPro(phi,phi_y0,x))</div><div class="line">    <span class="keyword">if</span> res &gt; <span class="number">0.5</span>:</div><div class="line">      print(<span class="string">"spam :"</span> + line)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">      print(<span class="string">"ham :"</span> + line)</div><div class="line">  <span class="keyword">return</span> dict</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcPro</span><span class="params">(phi,phi_y,x)</span>:</span></div><div class="line">  <span class="comment"># p(x|y=phi_y)</span></div><div class="line">  ret = <span class="number">1</span></div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x)):</div><div class="line">    <span class="keyword">if</span> x[i] == <span class="number">1</span>:</div><div class="line">      ret = ret * phi_y[i]</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">      ret = ret * (<span class="number">1</span>-phi_y[i])</div><div class="line">  <span class="keyword">return</span> ret*phi</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">  <span class="comment"># 0. debug options</span></div><div class="line">  DEBUG = <span class="keyword">False</span></div><div class="line"></div><div class="line">  <span class="comment"># 1. construct Dict</span></div><div class="line">  dict = constructDic()</div><div class="line">  dictLen = len(dict)</div><div class="line"></div><div class="line">  <span class="comment"># 2. parse the email to generate the sample, x and y</span></div><div class="line">  x_arr=[]</div><div class="line">  y_arr=[]</div><div class="line">  parseEmails(x_arr,y_arr,dict,dictLen)</div><div class="line"></div><div class="line">  <span class="comment"># 3. learn from the sample</span></div><div class="line">  phi = calPhiY(y_arr)            <span class="comment"># p(y)=1</span></div><div class="line">  phi_y1 = calPhiXY(y_arr, x_arr, <span class="number">1</span>, dictLen)</div><div class="line">  phi_y0 = calPhiXY(y_arr, x_arr, <span class="number">0</span>, dictLen)</div><div class="line"></div><div class="line">  <span class="comment"># 4. test classify</span></div><div class="line">  <span class="comment">#p(y=1|x)=p(x|y=1)p(y=1)/(p(x|y=1)p(y=1)+p(x|y=0)p(y=0))</span></div><div class="line">  <span class="comment">#p(x|y=1) = Pe(p(x=x^i|y=1))</span></div><div class="line">  classify(phi,phi_y0,phi_y1,dictLen)</div></pre></td></tr></table></figure>
<h2 id="2-多元伯努利事件模型"><a href="#2-多元伯努利事件模型" class="headerlink" title="2 多元伯努利事件模型"></a>2 多元伯努利事件模型</h2><h3 id="2-1-公式推导"><a href="#2-1-公式推导" class="headerlink" title="2.1 公式推导"></a>2.1 公式推导</h3><p>下面采用另外一种方式进行建模。对于\({x^T} = [{x _1},{x _2},…,{x _n}]\)，其中\({x _1} = 1\)代表邮件的第一个单词在词库中的索引为1。仍然有如下公式：</p>

$$p({x _1},{x _2},...,{x _i}|y) = \prod\limits _{i = 1}^n {p({x _i}|y)}$$

<p>我们设置\({\phi <em>y} = p(y = 1)\)，\({\phi </em>{i = k|y = 1}} = p({x _i} = k|y = 1)\)。</p>
<p>然后求最大释然函数：</p>

$$\ell ({\phi _y},{\phi _{i = k|y = 1}},{\phi _{i = k|y = 0}}) = \ln \prod\limits _{i = 1}^m {p({y^i}|{x^i})}  = \ln \prod\limits _{i = 1}^m {p({x^i}|{y^i})p({y^i})}$$$$\ell ({\phi _y},{\phi _{i = k|y = 1}},{\phi _{i = k|y = 0}}) = \ln \prod\limits _{i = 1}^m {{{(p({x^i}|{y^i} = 1){\phi _y})}^{1\{ {y^i} = 1\} }}p({x^i}|{y^i} = 0)(1 - {\phi _y}){)^{1\{ {y^i} = 0\} }})}$$
$$\ell ({\phi _y},{\phi _{i = k|y = 1}},{\phi _{i = k|y = 0}}) = \ln \prod\limits _{i = 1}^m {{{((\ln (\prod\limits _{j = 1}^n {p(} x _j^i|{y^i} = 1)){\phi _y})}^{1\{ {y^i} = 1\} }}(\ln (\prod\limits _{j = 1}^n {p(} x _j^i|{y^i} = 0))(1 - {\phi _y}){)^{1\{ {y^i} = 0\} }})}$$
$$\ell ({\phi _y},{\phi _{i = k|y = 1}},{\phi _{i = k|y = 0}}) = \sum\limits _{i = 1}^m {[1\{ {y^i} = 1\} (\ln {\phi _y} + \sum\limits _{j = 1}^n {\ln p(x _j^i|{y^i} = 1)} ) + 1\{ {y^i} = 0\} (\ln (1 - {\phi _y}) + \sum\limits _{j = 1}^n {\ln p(x _j^i|{y^i} = 0)} )]}$$

<p>我们对\({\phi _y}\)求偏导数，与之前相同，这里直接写出：</p>
$${\phi _y} = \frac{1}{m}\sum\limits _{i = 1}^m {1\{ {y^i} = 1\} }$$

<p>我们对\({\phi _{i = k|y = 0}}\)求偏导数，如下：</p>

$$\frac{{\partial \ell ({\phi _y},{\phi _{i = k|y = 1}},{\phi _{i = k|y = 0}})}}{{\partial {\phi _{i = k|y = 1}}}} = \frac{{\partial \sum\limits _{i = 1}^m {1\{ {y^i} = 1\} \sum\limits _{j = 1}^n {\ln p({x^i}|{y^i} = 1)} } }}{{\partial {\phi _{i|y = 1}}}}$$

<p>其中有：</p>
$$\ln p({x^i}|{y^i} = 1) = \ln [p{(x _j^i = k|{y^i} = 1)^{1\{ x _j^i = k\} }}p{(x _j^i \ne k|{y^i} = 1)^{1\{ x _j^i \ne k\} }}]$$
$$\ln p({x^i}|{y^i} = 1) = 1\{ x _j^i = k\} \ln {\phi _{i = k|y = 1}} + (1 - 1\{ x _j^i = k\} )\ln (1 - {\phi _{i = k|y = 1}})$$

<p>所以有：</p>
$$\frac{{\partial \ell ({\phi _y},{\phi _{i = k|y = 1}},{\phi _{i = k|y = 0}})}}{{\partial {\phi _{i = k|y = 1}}}} = \sum\limits _{i = 1}^m {1\{ {y^i} = 1\} \sum\limits _{j = 1}^n {\frac{{1\{ x _j^i = k\}  - {\phi _{i = k|y = 1}}}}{{{\phi _{i = k|y = 1}}(1 - {\phi _{i = k|y = 1}})}}} }  = 0$$

<p>根据拉普拉斯平滑，所以最后得到下面的公式，其中V为词库中单词的数目。</p>
$${\phi _{i = k|y = 1}} = \frac{{\sum\limits _{i = 1}^m {\sum\limits _{j = 1}^n {1\{ {y^i} = 1,x _j^i = k\} } }  + 1}}{{n\sum\limits _{i = 1}^m {1\{ {y^i} = 1\} }  + V}}$$

<p>同理有:</p>

$${\phi _{i = k|y = 0}} = \frac{{\sum\limits _{i = 1}^m {\sum\limits _{j = 1}^n {1\{ {y^i} = 0,x _j^i = k\} } }  + 1}}{{n\sum\limits _{i = 1}^m {1\{ {y^i} = 0\} }  + V}}$$

<h3 id="2-2-算法的实现"><a href="#2-2-算法的实现" class="headerlink" title="2.2 算法的实现"></a>2.2 算法的实现</h3><p>我们使用样本得到各个单词的概率分布，然后预测邮件是否为垃圾邮件。运行下面的程序，可以得到了正确的垃圾邮件分类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> re</div><div class="line"></div><div class="line"><span class="keyword">global</span> DEBUG</div><div class="line"><span class="keyword">global</span> SCALE</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">constructDic</span><span class="params">()</span>:</span></div><div class="line">  file = open(<span class="string">"../res/NaiveBayes/dict.txt"</span>)</div><div class="line">  <span class="comment"># key is the word, value is the sequence number</span></div><div class="line">  dict = &#123;&#125;           <span class="comment"># In fact, treeMap is better than dict</span></div><div class="line">  count = <span class="number">0</span>;</div><div class="line">  <span class="keyword">while</span> <span class="number">1</span>:</div><div class="line">    line = file.readline()</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> line:</div><div class="line">      <span class="keyword">break</span></div><div class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> line.split():</div><div class="line">      <span class="keyword">if</span> word.lower() <span class="keyword">not</span> <span class="keyword">in</span> dict:</div><div class="line">        dict[word.lower()]=count</div><div class="line">        count = count+<span class="number">1</span></div><div class="line">  <span class="keyword">return</span> dict</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">findIndexInDict</span><span class="params">(word,dict)</span>:</span></div><div class="line">  <span class="keyword">if</span> word <span class="keyword">in</span> dict:</div><div class="line">    <span class="keyword">return</span> dict[word]</div><div class="line">  <span class="keyword">else</span>:</div><div class="line">    <span class="keyword">return</span> <span class="number">-1</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parseSpamOrHam</span><span class="params">(words)</span>:</span></div><div class="line">  <span class="keyword">if</span> words[<span class="number">0</span>] == <span class="string">"spam"</span>:</div><div class="line">    <span class="keyword">return</span> <span class="number">1</span></div><div class="line">  <span class="keyword">elif</span> words[<span class="number">0</span>] == <span class="string">"ham"</span>:</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span></div><div class="line">  <span class="keyword">else</span>:</div><div class="line">    <span class="keyword">if</span> DEBUG:</div><div class="line">      print(<span class="string">"error email type in training sample!"</span>)</div><div class="line">  <span class="keyword">return</span> <span class="number">-1</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitEmail</span><span class="params">(line)</span>:</span></div><div class="line">  regEx = re.compile(<span class="string">r'[^a-zA-Z]|\d'</span>)</div><div class="line">  <span class="keyword">return</span> list(filter(<span class="keyword">lambda</span> word: word!=<span class="string">""</span>, regEx.split(line)))</div><div class="line"></div><div class="line"><span class="comment"># spam is 1, ham is 0</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parseEmails</span><span class="params">(x_arr,y_arr,dict,dictLen)</span>:</span></div><div class="line">  file = open(<span class="string">"../res/NaiveBayes/emails.txt"</span>)</div><div class="line">  <span class="keyword">while</span> <span class="number">1</span>:</div><div class="line">    line = file.readline()</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> line:</div><div class="line">      <span class="keyword">break</span></div><div class="line">    words = splitEmail(line)</div><div class="line">    y = int(parseSpamOrHam(words))</div><div class="line">    words = words[<span class="number">1</span>:]</div><div class="line">    x = np.zeros(len(words),int)</div><div class="line">    <span class="keyword">if</span> y == <span class="number">-1</span>:</div><div class="line">      <span class="keyword">continue</span>;</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(words)):</div><div class="line">      <span class="keyword">if</span> words[i].lower() <span class="keyword">in</span> dict:</div><div class="line">        index = dict[words[i].lower()]</div><div class="line">        x[i]=index</div><div class="line">      <span class="keyword">else</span>:</div><div class="line">        x[i]=dictLen+<span class="number">1</span></div><div class="line">    x_arr.append(x)</div><div class="line">    y_arr.append(y)</div><div class="line">    <span class="comment">#if DE<span class="doctag">BUG:</span></span></div><div class="line">    <span class="comment"># for i in range(len(x)):</span></div><div class="line">    <span class="comment">#    print(x[i])</span></div><div class="line">  <span class="keyword">return</span> dict</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calPhiY</span><span class="params">(y_arr)</span>:</span></div><div class="line">  <span class="comment"># p(y=1)</span></div><div class="line">  sum = <span class="number">0</span></div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> y_arr:</div><div class="line">    sum = sum + i</div><div class="line">  <span class="keyword">return</span> sum/len(y_arr)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calPhiXY</span><span class="params">(y_arr,x_arr,knownY,dictLen)</span>:</span></div><div class="line">  <span class="comment"># return a vector</span></div><div class="line">  sum = <span class="number">0</span></div><div class="line">  ret = np.zeros(dictLen)</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(len(y_arr)):</div><div class="line">    <span class="keyword">if</span> y_arr[i]!=knownY:</div><div class="line">      <span class="keyword">continue</span></div><div class="line">    sum=sum+len(x_arr[i])</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(x_arr[i])):</div><div class="line">      index = x_arr[i][j]</div><div class="line">      ret[index]=ret[index]+<span class="number">1</span></div><div class="line">  <span class="keyword">return</span> SCALE*(ret+<span class="number">1</span>)/(sum+dictLen)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(phi,phi_y0,phi_y1,dictLen)</span>:</span></div><div class="line">  file = open(<span class="string">"../res/NaiveBayes/testEmails.txt"</span>)</div><div class="line">  <span class="keyword">while</span> <span class="number">1</span>:</div><div class="line">    line = file.readline()</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> line:</div><div class="line">      <span class="keyword">break</span></div><div class="line">    words = splitEmail(line)[<span class="number">1</span>:]</div><div class="line">    x = np.zeros(len(words),int)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(words)):</div><div class="line">      <span class="keyword">if</span> words[i].lower() <span class="keyword">in</span> dict:</div><div class="line">        index = dict[words[i].lower()]</div><div class="line">        x[i]=index</div><div class="line">    res = calcPro(phi,phi_y1,x)/(calcPro(phi,phi_y1,x)+calcPro(phi,phi_y0,x))</div><div class="line"></div><div class="line">    <span class="keyword">if</span> res &gt; <span class="number">0.5</span>:</div><div class="line">      print(<span class="string">"spam :"</span> + line)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">      print(<span class="string">"ham :"</span> + line)</div><div class="line">  <span class="keyword">return</span> dict</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcPro</span><span class="params">(phi,phi_y,x)</span>:</span></div><div class="line">  <span class="comment"># p(x|y=phi_y)</span></div><div class="line">  ret = <span class="number">1</span></div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x)):</div><div class="line">    ret = ret*phi_y[x[i]]</div><div class="line">    <span class="comment">#print(ret)</span></div><div class="line">  <span class="keyword">return</span> ret*phi</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">  <span class="comment"># 0. debug options</span></div><div class="line">  DEBUG = <span class="keyword">True</span></div><div class="line">  SCALE = <span class="number">10e3</span></div><div class="line"></div><div class="line">  <span class="comment"># 1. construct Dict</span></div><div class="line">  dict = constructDic()</div><div class="line">  dictLen = len(dict)</div><div class="line"></div><div class="line">  <span class="comment"># 2. parse the email to generate the sample, x and y</span></div><div class="line">  x_arr=[]</div><div class="line">  y_arr=[]</div><div class="line">  parseEmails(x_arr,y_arr,dict,dictLen)</div><div class="line"></div><div class="line">  <span class="comment"># 3. learn from the sample</span></div><div class="line">  phi = calPhiY(y_arr)            <span class="comment"># p(y)=1</span></div><div class="line">  phi_y1 = calPhiXY(y_arr, x_arr, <span class="number">1</span>, dictLen)</div><div class="line">  phi_y0 = calPhiXY(y_arr, x_arr, <span class="number">0</span>, dictLen)</div><div class="line"></div><div class="line">  <span class="comment"># 4. test classify</span></div><div class="line">  <span class="comment">#p(y=1|x)=p(x|y=1)p(y=1)/(p(x|y=1)p(y=1)+p(x|y=0)p(y=0))</span></div><div class="line">  <span class="comment">#p(x|y=1) = Pe(p(x=x^i|y=1))</span></div><div class="line">  classify(phi,phi_y0,phi_y1,dictLen)</div></pre></td></tr></table></figure>
<blockquote>
<p>由于概率值过小，多次乘积会超过浮点数精度范围，所以程序这里乘以一个固定的比例系数</p>
</blockquote>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>手写公式和word版博客地址:</p>
<pre><code>https://github.com/zhengchenyu/MyBlog/tree/master/doc/mlearning/贝叶斯算法
</code></pre>
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-机器学习-2-5-监督学习之生成型学习算法" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2017/08/05/机器学习-2-5-监督学习之生成型学习算法/" class="article-date">
      <time datetime="2017-08-05T08:41:15.000Z" itemprop="datePublished">2017-08-05</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/08/05/机器学习-2-5-监督学习之生成型学习算法/">机器学习-2.5-监督学习之生成型学习算法</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <script type="text/javascript" src="/Users/zcy/Desktop/study/git/MathJax-master/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h1 id="生成型学习算法"><a href="#生成型学习算法" class="headerlink" title="生成型学习算法"></a>生成型学习算法</h1><h2 id="1-生成型学习算法简述"><a href="#1-生成型学习算法简述" class="headerlink" title="1 生成型学习算法简述"></a>1 生成型学习算法简述</h2><p>我们之前讲到的算法。譬如逻辑回归，试图从\(x\)直接学习得到一组映射关系到\(y\)，即通过样本\((x,y)\)学习得到,使得其能够准确的预测\(y\)。这类算法叫做判别性学习算法。</p>
<p>本节我们将介绍生成学习型算法。举个例子，对于一个大象，我们可以为大象构造一个关于其特征的模型。对于一条狗，同样可以为狗构造一个关于关于其特征的模型。对于一个新动物，我们可以根据前述构建的模型，判断这个动物是像大象多一点还是像狗多一点，来确定这动物是大象还是狗。</p>
<p>我们可以根据样本得到大象的特征模型\(p(x|y=0)\)和狗的特征模型\(p(x|y=1)\)，同时也可以得到\(p(y)\)。根据贝叶斯公式，有如下：</p>
$$\max (p(y|x)) = \max (\frac{p(x|y)p(y)}{p(x)}) = \max (\frac{p(x|y)p(y)}{p(x|y = 1)p(y = 1) + p(x|y = 0)p(y = 0)})$$

<p>对于我们做预测的时候，可以不考虑分母。</p>
$$\max (p(y|x)) = \max (p(x|y)p(y))$$

<blockquote>
<p>为什么不考虑分母?我们可以将\(p(x|y)p(y)\)简化为\(f(y)\),因此分母就是\(f(1)+f(0)\)。如果我们通过样本对完成了建模，因此分母就是一个常数了，因此没有计算的必要了。</p>
</blockquote>
<h2 id="2-高斯判别分析"><a href="#2-高斯判别分析" class="headerlink" title="2 高斯判别分析"></a>2 高斯判别分析</h2><h3 id="2-1-多维高斯分布介绍"><a href="#2-1-多维高斯分布介绍" class="headerlink" title="2.1 多维高斯分布介绍"></a>2.1 多维高斯分布介绍</h3><p>略，详见cs229-note2.pdf</p>
<h3 id="2-2-高斯判别分析模型"><a href="#2-2-高斯判别分析模型" class="headerlink" title="2.2 高斯判别分析模型"></a>2.2 高斯判别分析模型</h3><p>对于一个分类问题,\(y \in (0,1)\)。假设\(x\)是连续的随机变量，服从高斯分布。具体描述如下：</p>
$$y \sim Bernoulli(\phi)$$$$x|y = 0 \sim N({\mu _0},\Sigma)$$$$x|y = 1 \sim N({\mu _1},\Sigma)$$
$$p(x|y = 0) = \frac{1}{{{(2\pi )}^{\frac{n}{2}}}|\Sigma {|^{\frac{n}{2}}}}\exp ( - \frac{1}{2}{(x - {\mu _0})^T}{\Sigma ^{ - 1}}(x - {\mu _0}))$$
$$p(x|y = 1) = \frac{1}{{{(2\pi )}^{\frac{n}{2}}}|\Sigma {|^{\frac{n}{2}}}}\exp ( - \frac{1}{2}{(x - {\mu _1})^T}{\Sigma ^{ - 1}}(x - {\mu _1}))$$

<h3 id="2-3-公式推导"><a href="#2-3-公式推导" class="headerlink" title="2.3 公式推导"></a>2.3 公式推导</h3><p>我们设置\(\phi  = p(y = 0)\)。根据前面的分析，在给定样本的情况下，我们需要保证下式最大。</p>
$$\prod\limits _{i = 1}^m {p({y^i}|{x^i})}  = \prod\limits _{i = 1}^m {p({x^i}|{y^i})p({y^i})}$$

<p>这里为了简写，我们设置\({z _0} = x - {\mu _0}\) ，\({z _1} = x - {\mu _1}\)。取自然对数后，得到最大释然函数:</p>

$$\ell ({\mu _1},{\mu _1},\Sigma ,\phi ) = \sum\limits _{i = 1}^m {\ln \{ {{[\frac{1}{{{{(2\pi )}^{\frac{n}{2}}}|\Sigma {|^{\frac{n}{2}}}}}\exp ( - \frac{1}{2}z _0^T{\Sigma ^{ - 1}}{z _0})]}^{1\{ {y^i} = 0\} }}{{[\frac{1}{{{{(2\pi )}^{\frac{n}{2}}}|\Sigma {|^{\frac{n}{2}}}}}\exp ( - \frac{1}{2}z _1^T{\Sigma ^{ - 1}}{z _1})]}^{1\{ {y^i} = 1\} }}\} } $$
$$\ell ({\mu _0},{\mu _1},\Sigma ,\phi ) = \sum\limits _{i = 1}^m {\{ 1\{ {y^i} = 0\} [ - \frac{1}{2}z _0^T{\Sigma ^{ - 1}}{z _0} - \frac{n}{2}\ln 2\pi  - \frac{n}{2}\ln |\Sigma | + \ln \phi ] + 1\{ {y^i} = 1\} } [ - \frac{1}{2}z _1^T{\Sigma ^{ - 1}}{z _1} - \frac{n}{2}\ln 2\pi  - \frac{n}{2}\ln |\Sigma | + \ln (1 - \phi )]\}$$

<p>然后我们分别对各个参数求偏导数:</p>

$$\frac{{\partial \ell ({\mu _0},{\mu _1},\sum ,\phi )}}{{\partial \phi }} = \sum\limits _{i = 1}^m {(\frac{{1\{ {y^i} = 1\} }}{\phi } - \frac{{1\{ {y^i} = 0\} }}{{1 - \phi }})}  = \sum\limits _{i = 1}^n {(\frac{{1\{ {y^i} = 1\} }}{\phi } - \frac{{1 - 1\{ {y^i} = 1\} }}{{1 - \phi }})}  = 0$$

<p>所以有:</p>

$$\phi  = \frac{1}{m}\sum\limits _{i = 1}^n {1\{ {y^i} = 1\} }$$

<p>然后对\(\mu _0\)求导：</p>

$${\nabla _{{\mu _0}}}\ell ({\mu _0},{\mu _1},\Sigma ,\phi ) = \sum\limits _{i = 1}^m {{\nabla _{{\mu _0}}}( - \frac{1}{2}{{({x^i} - {\mu _0})}^T}{\Sigma ^{ - 1}}({x^i} - {\mu _0}))1\{ {y^i} = 0\} }$$$${\nabla _{{\mu _0}}}\ell ({\mu _0},{\mu _1},\Sigma ,\phi ) =  - \frac{1}{2}\sum\limits _{i = 1}^m {{\nabla _{{\mu _0}}}( - \mu _0^T{\Sigma ^{ - 1}}{x^i} - {{({x^i})}^T}{\Sigma ^{ - 1}}{\mu _0} + \mu _0^T{\Sigma ^{ - 1}}{\mu _0})1\{ {y^i} = 0\} }$$
<p>我们对上面的式子分头计算。</p>
<blockquote>
<p>\(\Sigma\)为对角阵。由于求偏微分的数为一个常数，因此该值与其迹的相同。另外，这里使用了一些关于迹的公式。具体详见附录。</p>
</blockquote>

$${\nabla _{{\mu _0}}}\mu _0^T{\Sigma ^{ - 1}}x = {\nabla _{{\mu _0}}}tr(\mu _0^T{\Sigma ^{ - 1}}{x^i}) = {[{\nabla _{\mu _0^T}}tr(\mu _0^T{\Sigma ^{ - 1}}{x^i})]^T} = {[{({\Sigma ^{ - 1}}{x^i})^T}]^T} = {\Sigma ^{ - 1}}{x^i}$$
$${\nabla _{{\mu _0}}}{({x^i})^T}{\Sigma ^{ - 1}}{\mu _0} = {\nabla _{{\mu _0}}}tr({({x^i})^T}{\Sigma ^{ - 1}}{\mu _0}) = {\nabla _{{\mu _0}}}{\mu _0}{({x^i})^T}{\Sigma ^{ - 1}} = {({({x^i})^T}{\Sigma ^{ - 1}})^T} = {\Sigma ^{ - 1}}{x^i}$$
$${\nabla _{{\mu _0}}}\mu _0^T{\Sigma ^{ - 1}}{\mu _0} = {[{\nabla _{\mu _0^T}}\mu _0^T{\Sigma ^{ - 1}}{\mu _0}E]^T} = [E\mu _0^T{\Sigma ^{ - 1}} + {E^T}\mu _0^T{({\Sigma ^{ - 1}})^T}] = 2{\Sigma ^{ - 1}}{\mu _0}$$

<p>所以有:</p>

$${\nabla _{{\mu _0}}}\ell ({\mu _0},{\mu _1},\Sigma ,\phi ) =  - \sum\limits _{i = 1}^m {{\nabla _{{\mu _0}}}({\Sigma ^{ - 1}}{\mu _0} - {\Sigma ^{ - 1}}{x^i})1\{ {y^i} = 0\} }  = 0$$
$${\mu _0} = \frac{{\sum\limits _{i = 1}^m {1\{ {y^i} = 0\} {x^i}} }}{{\sum\limits _{i = 1}^m {1\{ {y^i} = 0\} } }}$$

<p>同理有:</p>

$${\mu _1} = \frac{{\sum\limits _{i = 1}^m {1\{ {y^i} = 1\} {x^i}} }}{{\sum\limits _{i = 1}^m {1\{ {y^i} = 1\} } }}$$

<p>然后对\(\Sigma^{-1}\)求偏导数。</p>
$${\nabla _{{\Sigma ^{ - 1}}}}\ell ({\mu _0},{\mu _1},\Sigma ,\phi ) = \sum\limits _{i = 1}^m {{\nabla _{{\Sigma ^{ - 1}}}}[( - \frac{1}{2}z _0^T{\Sigma ^{ - 1}}{z _0} + \frac{1}{2}\ln \frac{1}{{|\Sigma |}})1\{ {y^i} = 0\}  + ( - \frac{1}{2}z _1^T{\Sigma ^{ - 1}}{z _1} + \frac{1}{2}\ln \frac{1}{{|\Sigma |}})1\{ {y^i} = 1\} ]}$$

<p>这里分开计算:</p>

$${\nabla _{\Sigma ^{ - 1}}}(z _0^T{\Sigma ^{ - 1}}{z _0}) = {\nabla _{\sum ^{ - 1}}}tr(z _0^T{\Sigma ^{ - 1}}{z _0}) = {\nabla _{\sum ^{ - 1}}}tr({\Sigma ^{ - 1}}{z _0}z _0^T) = {({z _0}z _0^T)^T} = {z _0}z _0^T$$
$${\nabla _{\Sigma ^{ - 1}}}\ln \frac{1}{|\Sigma |} = {\nabla _{\Sigma ^{ - 1}}}\ln |{\Sigma ^{ - 1}}| = \frac{1}{|{\Sigma ^{ - 1}}|}{\nabla _{\Sigma ^{ - 1}}}{\Sigma ^{ - 1}} = {({({\Sigma ^{ - 1}})^{ - 1}})^T}$$

<p>所以有:</p>

$${\nabla _{{\sum ^{ - 1}}}}\ell ({\mu _0},{\mu _1},\Sigma ,\phi ) = \sum\limits _{i = 1}^m {[( - \frac{1}{2}{z _0}z _0^T + \frac{1}{2}\Sigma )1\{ {y^i} = 0\}  + ( - \frac{1}{2}{z _1}z _1^T + \frac{1}{2}\Sigma )1\{ {y^i} = 1\} ]}  = 0$$
$$\Sigma  = \frac{1}{m}\sum\limits _{i = 1}^m {[({x^i} - {\mu _0}){{({x^i} - {\mu _0})}^T}1\{ {y^i} = 0\}  + ({x^i} - {\mu _1}){{({x^i} - {\mu _1})}^T}1\{ {y^i} = 1\} ]}$$
$$\Sigma  = \frac{1}{m}\sum\limits _{i = 1}^m {[({x^i} - {\mu _{{y^i}}}){{({x^i} - {\mu _{{y^i}}})}^T}]}$$
<h2 id="2-3-高斯型算法实例"><a href="#2-3-高斯型算法实例" class="headerlink" title="2.3 高斯型算法实例"></a>2.3 高斯型算法实例</h2><p>我们分别以 \((1,1)\)和\((2,2)\)为均值生成一组高斯分布。下面图是生成的样本。</p>
<p><img src="/images/机器学习/监督学习-生成型算法GDA样本.png" width="50%" height="50%" text-align="center/"></p>
<p>前提假设是我们知道两组分类是符合高斯分布的，切假定协方差相同。但我们不知道两组数据的均值和协方差。根据前面的公式有如下代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> multivariate_normal</div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">from</span> matplotlib.lines <span class="keyword">import</span> Line2D</div><div class="line"></div><div class="line"><span class="comment">## this is a program about GDA</span></div><div class="line"></div><div class="line"><span class="keyword">global</span> MAKE_DATA</div><div class="line"><span class="keyword">global</span> SHOW_PIC</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_data</span><span class="params">(x)</span>:</span></div><div class="line">  <span class="keyword">if</span> MAKE_DATA == <span class="keyword">True</span>:</div><div class="line">    mean_1 = [<span class="number">1</span>,<span class="number">1</span>]</div><div class="line">    mean_2 = [<span class="number">2</span>,<span class="number">2</span>]</div><div class="line">    cov = [[<span class="number">0.1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0.1</span>]]</div><div class="line">    arr1 = np.random.multivariate_normal(mean_1, cov, <span class="number">100</span>)</div><div class="line">    arr2 = np.random.multivariate_normal(mean_2, cov, <span class="number">50</span>)</div><div class="line">    print(arr1)</div><div class="line">    print(arr2)</div><div class="line">    x.append(arr1)</div><div class="line">    x.append(arr2)</div><div class="line">  <span class="keyword">if</span> MAKE_DATA == <span class="keyword">False</span>:</div><div class="line">    arr1 = np.array([[ <span class="number">0.88235916</span> , <span class="number">1.01511634</span>],[ <span class="number">0.75243817</span> , <span class="number">0.76520033</span>],[ <span class="number">0.95710848</span> , <span class="number">1.41894337</span>],[ <span class="number">1.48682891</span> , <span class="number">0.78885043</span>],[ <span class="number">1.24047011</span> , <span class="number">0.71984948</span>],[ <span class="number">0.67611276</span> , <span class="number">1.07909452</span>],[ <span class="number">1.03243669</span> , <span class="number">1.08929695</span>],[ <span class="number">1.0296548</span>  , <span class="number">1.25023769</span>],[ <span class="number">1.54134008</span> , <span class="number">0.39564824</span>],[ <span class="number">0.34645057</span> , <span class="number">1.61499636</span>],[ <span class="number">0.77206174</span> , <span class="number">1.23613698</span>],[ <span class="number">0.91446988</span> , <span class="number">1.38537765</span>],[ <span class="number">0.99982962</span> , <span class="number">1.34448471</span>],[ <span class="number">0.78745962</span> , <span class="number">0.9046565</span> ],[ <span class="number">0.74946602</span> , <span class="number">1.07424473</span>],[ <span class="number">1.09294839</span> , <span class="number">1.14711993</span>],[ <span class="number">0.39266844</span> , <span class="number">0.78788004</span>],[ <span class="number">0.83112357</span> , <span class="number">1.2762774</span> ],[ <span class="number">1.05056188</span> , <span class="number">1.13351562</span>],[ <span class="number">1.62101523</span> , <span class="number">1.15035562</span>],[ <span class="number">0.70377517</span> , <span class="number">1.1136416</span> ],[ <span class="number">1.03715472</span> , <span class="number">0.47905693</span>],[ <span class="number">0.94598381</span> , <span class="number">0.8874837</span> ],[ <span class="number">0.94447128</span> , <span class="number">2.02796925</span>],[ <span class="number">0.72442242</span> , <span class="number">1.09835206</span>],[ <span class="number">0.69046731</span> , <span class="number">1.46232182</span>],[ <span class="number">1.20744606</span> , <span class="number">1.10280041</span>],[ <span class="number">0.70665746</span> , <span class="number">0.82139503</span>],[ <span class="number">1.08803887</span> , <span class="number">1.4450361</span> ],[ <span class="number">0.88530961</span> , <span class="number">0.75727475</span>],[ <span class="number">0.98418545</span> , <span class="number">0.80248161</span>],[ <span class="number">0.74970386</span> , <span class="number">1.13205709</span>],[ <span class="number">0.72586454</span> , <span class="number">1.06058385</span>],[ <span class="number">0.9071812</span>  , <span class="number">1.09975063</span>],[ <span class="number">0.75182835</span> , <span class="number">0.93570147</span>],[ <span class="number">0.80052289</span> , <span class="number">1.08168507</span>],[ <span class="number">0.40180652</span> , <span class="number">0.9526211</span> ],[ <span class="number">0.62312617</span> , <span class="number">0.84385058</span>],[ <span class="number">0.68212516</span> , <span class="number">1.25912717</span>],[ <span class="number">1.19773245</span> , <span class="number">0.16399654</span>],[ <span class="number">0.96093132</span> , <span class="number">0.43932091</span>],[ <span class="number">1.25471657</span> , <span class="number">0.92371829</span>],[ <span class="number">1.12330272</span> , <span class="number">1.26968747</span>],[ <span class="number">1.30361985</span> , <span class="number">0.99862123</span>],[ <span class="number">1.23477665</span> , <span class="number">1.1742804</span> ],[ <span class="number">0.28471876</span> , <span class="number">0.5806044</span> ],[ <span class="number">1.89355099</span> , <span class="number">1.19928671</span>],[ <span class="number">1.09081369</span> , <span class="number">1.28467312</span>],[ <span class="number">1.40488635</span> , <span class="number">0.90034427</span>],[ <span class="number">1.11672364</span> , <span class="number">1.49070515</span>],[ <span class="number">1.35385212</span> , <span class="number">1.35767891</span>],[ <span class="number">0.92746374</span> , <span class="number">1.79096697</span>],[ <span class="number">1.89142562</span> , <span class="number">0.98228303</span>],[ <span class="number">1.0555218</span>  , <span class="number">0.86070833</span>],[ <span class="number">0.69001255</span> , <span class="number">1.12874741</span>],[ <span class="number">0.98137315</span> , <span class="number">1.3398852</span> ],[ <span class="number">1.02525371</span> , <span class="number">0.77572865</span>],[ <span class="number">1.1354295</span>  , <span class="number">1.07098552</span>],[ <span class="number">1.50829164</span> , <span class="number">1.43065998</span>],[ <span class="number">1.09928764</span> , <span class="number">1.55540292</span>],[ <span class="number">0.64695084</span> , <span class="number">0.79920395</span>],[ <span class="number">0.82059034</span> , <span class="number">0.97533491</span>],[ <span class="number">0.56345455</span> , <span class="number">1.08168272</span>],[ <span class="number">1.06673215</span> , <span class="number">1.19448556</span>],[ <span class="number">0.96512548</span> , <span class="number">1.5268577</span> ],[ <span class="number">0.96914451</span> , <span class="number">1.00902985</span>],[ <span class="number">0.72879413</span> , <span class="number">0.92476415</span>],[ <span class="number">1.0931483</span>  , <span class="number">1.13572242</span>],[ <span class="number">1.34765121</span> , <span class="number">0.83841006</span>],[ <span class="number">1.57813788</span> , <span class="number">0.65915892</span>],[ <span class="number">0.59032608</span> , <span class="number">0.82747946</span>],[ <span class="number">0.83838504</span> , <span class="number">0.67588473</span>],[ <span class="number">1.35101322</span> , <span class="number">1.21027851</span>],[ <span class="number">0.71762153</span> , <span class="number">0.41839038</span>],[ <span class="number">0.61295604</span> , <span class="number">0.66555018</span>],[ <span class="number">0.64379346</span> , <span class="number">0.92925228</span>],[ <span class="number">1.1194968</span>  , <span class="number">0.65876736</span>],[ <span class="number">0.39495437</span> , <span class="number">0.67246734</span>],[ <span class="number">1.05223282</span> , <span class="number">0.17889116</span>],[ <span class="number">0.97810984</span> , <span class="number">1.12794664</span>],[ <span class="number">0.98392719</span> , <span class="number">0.73590255</span>],[ <span class="number">1.25587405</span> , <span class="number">1.21853038</span>],[ <span class="number">1.01150226</span> , <span class="number">1.01835571</span>],[ <span class="number">1.02251614</span> , <span class="number">0.72704228</span>],[ <span class="number">1.00261519</span> , <span class="number">0.95347185</span>],[ <span class="number">0.96362523</span> , <span class="number">0.8607009</span> ],[ <span class="number">0.88034659</span> , <span class="number">1.2307104</span> ],[ <span class="number">0.75907236</span> , <span class="number">0.92799796</span>],[ <span class="number">0.54898709</span> , <span class="number">1.69882285</span>],[ <span class="number">0.55032649</span> , <span class="number">0.98831566</span>],[ <span class="number">1.33360789</span> , <span class="number">1.19793298</span>],[ <span class="number">0.83231239</span> , <span class="number">0.8946538</span> ],[ <span class="number">1.05173094</span> , <span class="number">1.26324289</span>],[ <span class="number">0.81482231</span> , <span class="number">0.56198584</span>],[ <span class="number">1.03854797</span> , <span class="number">1.0553811</span> ],[ <span class="number">1.32669227</span> , <span class="number">1.61115811</span>],[ <span class="number">1.13322152</span> , <span class="number">1.68151695</span>],[ <span class="number">0.39754618</span> , <span class="number">1.19392967</span>],[ <span class="number">0.61344185</span> , <span class="number">1.05281434</span>],[ <span class="number">1.18415366</span> , <span class="number">0.864884</span>  ]])</div><div class="line">    arr2 = np.array([[ <span class="number">2.15366548</span> , <span class="number">1.88035458</span>],[ <span class="number">2.36978774</span> , <span class="number">1.76550283</span>],[ <span class="number">2.46261387</span> , <span class="number">2.10568262</span>],[ <span class="number">1.90475526</span> , <span class="number">1.95242885</span>],[ <span class="number">1.77712677</span> , <span class="number">1.96004856</span>],[ <span class="number">1.5995514</span>  , <span class="number">2.1323943</span> ],[ <span class="number">1.52727223</span> , <span class="number">1.50295551</span>],[ <span class="number">1.80330407</span> , <span class="number">1.57942301</span>],[ <span class="number">1.86487049</span> , <span class="number">1.87234414</span>],[ <span class="number">1.9586354</span>  , <span class="number">1.96279729</span>],[ <span class="number">2.59668134</span> , <span class="number">2.414423</span>  ],[ <span class="number">2.818419</span>   , <span class="number">1.76280366</span>],[ <span class="number">2.01511628</span> , <span class="number">2.10858546</span>],[ <span class="number">2.15907962</span> , <span class="number">1.81593012</span>],[ <span class="number">1.63966834</span> , <span class="number">2.2209023</span> ],[ <span class="number">2.47220599</span> , <span class="number">1.70482956</span>],[ <span class="number">2.08760748</span> , <span class="number">2.51601971</span>],[ <span class="number">1.50547722</span> , <span class="number">1.8487145</span> ],[ <span class="number">1.68125583</span> , <span class="number">2.64968501</span>],[ <span class="number">2.01924282</span> , <span class="number">2.0953572</span> ],[ <span class="number">2.22563534</span> , <span class="number">2.18266325</span>],[ <span class="number">2.2684291</span>  , <span class="number">2.23581599</span>],[ <span class="number">2.13787557</span> , <span class="number">1.9999382</span> ],[ <span class="number">1.02638695</span> , <span class="number">1.68134967</span>],[ <span class="number">2.35614619</span> , <span class="number">1.32072125</span>],[ <span class="number">2.20054871</span> , <span class="number">1.47401445</span>],[ <span class="number">1.99454827</span> , <span class="number">1.71658741</span>],[ <span class="number">1.83269065</span> , <span class="number">2.47662909</span>],[ <span class="number">2.40097251</span> , <span class="number">2.21823862</span>],[ <span class="number">2.54404652</span> , <span class="number">1.85742018</span>],[ <span class="number">1.84150027</span> , <span class="number">2.06350351</span>],[ <span class="number">1.69490855</span> , <span class="number">1.70169334</span>],[ <span class="number">1.44745704</span> , <span class="number">1.88295233</span>],[ <span class="number">2.24376639</span> , <span class="number">1.67530495</span>],[ <span class="number">1.42911921</span> , <span class="number">1.81854548</span>],[ <span class="number">1.33789289</span> , <span class="number">2.27686128</span>],[ <span class="number">2.43509821</span> , <span class="number">1.95032131</span>],[ <span class="number">1.9512447</span>  , <span class="number">1.4595415</span> ],[ <span class="number">2.13041192</span> , <span class="number">1.79372755</span>],[ <span class="number">2.2753866</span>  , <span class="number">2.23781951</span>],[ <span class="number">2.26753401</span> , <span class="number">1.78149305</span>],[ <span class="number">2.06505449</span> , <span class="number">2.01939606</span>],[ <span class="number">2.44426826</span> , <span class="number">2.1437101</span> ],[ <span class="number">2.16607141</span> , <span class="number">2.31077167</span>],[ <span class="number">1.96097237</span> , <span class="number">2.49100193</span>],[ <span class="number">1.37255424</span> , <span class="number">1.60735016</span>],[ <span class="number">1.63947758</span> , <span class="number">2.17852314</span>],[ <span class="number">2.13722666</span> , <span class="number">2.00559707</span>],[ <span class="number">1.222696</span>   , <span class="number">1.67075059</span>],[ <span class="number">2.56982685</span> , <span class="number">2.51218813</span>]])</div><div class="line">    x.append(arr1)</div><div class="line">    x.append(arr2)</div><div class="line">  <span class="keyword">if</span> SHOW_PIC == <span class="number">1</span>:</div><div class="line">    figure, ax = plt.subplots()</div><div class="line">    ax.set_xlim(left=<span class="number">-1</span>, right=<span class="number">4</span>)</div><div class="line">    ax.set_ylim(bottom=<span class="number">-1</span>, top=<span class="number">4</span>)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr1)):</div><div class="line">      plt.plot(arr1[i][<span class="number">0</span>], arr1[i][<span class="number">1</span>], <span class="string">'b--'</span>, marker=<span class="string">'+'</span>, color=<span class="string">'g'</span>)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr2)):</div><div class="line">      plt.plot(arr2[i][<span class="number">0</span>], arr2[i][<span class="number">1</span>], <span class="string">'b--'</span>, marker=<span class="string">'o'</span>, color=<span class="string">'b'</span>)</div><div class="line">    plt.xlabel(<span class="string">"x1"</span>)</div><div class="line">    plt.ylabel(<span class="string">"x2"</span>)</div><div class="line">    plt.plot()</div><div class="line">    plt.show()</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcPhi</span><span class="params">(x)</span>:</span></div><div class="line">  <span class="keyword">return</span> len(x[<span class="number">1</span>])/(len(x[<span class="number">0</span>])+len(x[<span class="number">1</span>]))</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcMu</span><span class="params">(x,i)</span>:</span></div><div class="line">  <span class="keyword">return</span> x[i].mean(axis=<span class="number">0</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcSigma</span><span class="params">(x,mu_0,mu_1)</span>:</span></div><div class="line">  sum=np.array([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0</span>]])</div><div class="line">  x0=x[<span class="number">0</span>]</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x0)):</div><div class="line">    z0=np.array([x0[i]-mu_0])</div><div class="line">    z0T = np.array([x0[i]-mu_0]).transpose()</div><div class="line">    sum = sum + np.dot(z0T, z0)</div><div class="line">    print(np.dot(z0T, z0))</div><div class="line">  x1=x[<span class="number">1</span>]</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x1)):</div><div class="line">    z1=np.array([x1[i]-mu_1])</div><div class="line">    z1T = np.array([x1[i]-mu_1]).transpose()</div><div class="line">    sum = sum + np.dot(z1T, z1)</div><div class="line">    print(np.dot(z1T, z1))</div><div class="line">  <span class="keyword">return</span> sum/(len(x0)+len(x1))</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(x,mu_0,mu_1,sigma,phi)</span>:</span></div><div class="line">  var0 = multivariate_normal(mean=mu_0.tolist(), cov=sigma.tolist())</div><div class="line">  var1 = multivariate_normal(mean=mu_1.tolist(), cov=sigma.tolist())</div><div class="line">  <span class="comment"># p(y=1|x) = p(x|y=1)/p(y=1) / p(x) = p(x|y=1)p(y=1) / (p(x|y=1)p(y=1)+p(x|y=0)p(y=0))</span></div><div class="line">  <span class="keyword">return</span> var1.pdf(x)*phi/(var1.pdf(x)*phi+var0.pdf(x)*(<span class="number">1</span>-phi)) &gt; <span class="number">0.5</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">testClassify</span><span class="params">(mu_0,mu_1,sigma,phi)</span>:</span></div><div class="line">  <span class="keyword">if</span> SHOW_PIC != <span class="number">2</span>:</div><div class="line">    <span class="keyword">return</span></div><div class="line">  figure, ax = plt.subplots()</div><div class="line">  ax.set_xlim(left=<span class="number">-1</span>, right=<span class="number">4</span>)</div><div class="line">  ax.set_ylim(bottom=<span class="number">-1</span>, top=<span class="number">4</span>)</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</div><div class="line">    x1 = random.randint(<span class="number">0</span>,<span class="number">3000</span>)/<span class="number">1000</span></div><div class="line">    x2 = random.randint(<span class="number">0</span>,<span class="number">3000</span>)/<span class="number">1000</span></div><div class="line">    <span class="keyword">if</span>( classify([x1,x2],mu_0,mu_1,sigma,phi)) == <span class="keyword">True</span>:</div><div class="line">      plt.plot(x1, x2, <span class="string">'b--'</span>, marker=<span class="string">'+'</span>, color=<span class="string">'g'</span>)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">      plt.plot(x1, x2, <span class="string">'b--'</span>, marker=<span class="string">'o'</span>, color=<span class="string">'b'</span>)</div><div class="line">  <span class="comment"># draw y = -x+3</span></div><div class="line">  (line1_xs, line1_ys) = [(<span class="number">0</span>, <span class="number">3</span>), (<span class="number">3</span>, <span class="number">0</span>)]</div><div class="line">  ax.add_line(Line2D(line1_xs, line1_ys, linewidth=<span class="number">1</span>, color=<span class="string">'blue'</span>))</div><div class="line">  plt.xlabel(<span class="string">"x1"</span>)</div><div class="line">  plt.ylabel(<span class="string">"x2"</span>)</div><div class="line">  plt.plot()</div><div class="line">  plt.show()</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">  <span class="comment"># 0 debug option</span></div><div class="line">  MAKE_DATA = <span class="keyword">False</span></div><div class="line">  SHOW_PIC = <span class="number">1</span>        <span class="comment"># 0 - do not show   1 - show sample data    2 - show test data</span></div><div class="line"></div><div class="line">  <span class="comment"># 1 make the sample</span></div><div class="line">  <span class="comment"># the format of x is [ndarray0,ndarray1] , ndarray1 is the set of the first class, we set y=0.</span></div><div class="line">  <span class="comment"># ndarray1 is the set of the second class, we set y=1</span></div><div class="line">  x = []</div><div class="line">  make_data(x)</div><div class="line"></div><div class="line">  <span class="keyword">if</span> MAKE_DATA == <span class="keyword">True</span>:</div><div class="line">    exit(<span class="number">0</span>)</div><div class="line"></div><div class="line">  <span class="comment"># 2 learn from the sample</span></div><div class="line">  print(<span class="string">"hello"</span>)</div><div class="line">  phi = calcPhi(x)                <span class="comment"># phi  = p(y=1)  means close (2,2)</span></div><div class="line">  mu_0 = calcMu(x,<span class="number">0</span>)</div><div class="line">  mu_1 = calcMu(x,<span class="number">1</span>)</div><div class="line">  sigma = calcSigma(x,mu_0,mu_1)</div><div class="line">  print(phi,<span class="string">", "</span>,mu_0,<span class="string">", "</span>,mu_1)</div><div class="line">  print(sigma.tolist())</div><div class="line"></div><div class="line">  print(<span class="string">"---------"</span>)</div><div class="line">  <span class="comment"># 3 test classify</span></div><div class="line">  testClassify(mu_0,mu_1,sigma,phi)</div></pre></td></tr></table></figure>
<p>对于两个协方差相同的样本，我们知道两组数据有相同的概率分布曲线，具体都应该是一个个同心圆。因此，我们可以确定一条垂直于两点连线的曲线是对该样本的理论上正确的分割。因此，我们得到如下结果，并与理论分割的曲线，即\(y=-x+3\)比较，发现对我们随机产生的样本分类效果非常好。</p>
<p><img src="/images/机器学习/监督学习-生成型算法GDA结果.png" width="50%" height="50%" text-align="center/"></p>
<h2 id="附录-A-相关公式"><a href="#附录-A-相关公式" class="headerlink" title="附录 A 相关公式"></a>附录 A 相关公式</h2><p>Hessian矩阵定义:</p>
$${\nabla _A}f(A) = \left( {\begin{array}{*{20}{c}}  {\frac{{\partial f}}{{\partial {A _{11}}}}}& \ldots &{\frac{{\partial f}}{{\partial {A _{1n}}}}} \\\    \vdots & \ddots & \vdots  \\\   {\frac{{\partial f}}{{\partial {A _{n1}}}}}& \cdots &{\frac{{\partial f}}{{\partial {A _{nn}}}}} \end{array}} \right)$$

<p>矩阵的迹的定义:</p>

$$trA = \sum\limits _{i = 1}^n {{A _{ii}}}$$

<p>关于矩阵迹的公式:</p>

$$tr(a) = a$$
$$tr(aA) = atr(A)$$
$$tr(aA) = atr(A)$$
$$tr(ABC) = tr(CAB) + tr(BCA)$$
$$trA = tr{A^T}$$
$$tr(A + B) = trA + trB$$
$${\nabla _A}tr(AB) = {B^T}$$
$${\nabla _{A^T}}f(A) = {({\nabla _A}f(A))^T}$$
$${\nabla _{A^T}}trAB{A^T}C = CAB + {C^T}A{B^T}$$
$${\nabla _{A^T}}trAB{A^T}C = CAB + {C^T}A{B^T}$$

<blockquote>
<p>对矩阵做展开能证明上述大部分公式，这里暂时略。</p>
</blockquote>
<h2 id="附录B"><a href="#附录B" class="headerlink" title="附录B"></a>附录B</h2><p>手写公式和word版博客地址:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">https://github.com/zhengchenyu/MyBlog/tree/master/doc/mlearning/生成型学习算法</div></pre></td></tr></table></figure>
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-机器学习-2-4-监督学习之softmax" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2017/07/24/机器学习-2-4-监督学习之softmax/" class="article-date">
      <time datetime="2017-07-24T10:35:50.000Z" itemprop="datePublished">2017-07-24</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/07/24/机器学习-2-4-监督学习之softmax/">机器学习-2.4-监督学习之softmax</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <script type="text/javascript" src="/Users/zcy/Desktop/study/git/MathJax-master/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h1 id="Softmax-Regression"><a href="#Softmax-Regression" class="headerlink" title="Softmax Regression"></a>Softmax Regression</h1><p>下面介绍一下指数族分布的另外一个例子。之前的逻辑回归中，可以用来解决二分类的问题。对于有k中可能结果的时候，问题就转化为多分类问题了，也就是接下来要说明的softmax regression问题。<br>s</p>
<h2 id="1-函数定义"><a href="#1-函数定义" class="headerlink" title="1 函数定义"></a>1 函数定义</h2><p>在进行下一步推导前，我们先定义部分辅助函数。我们这里事先定义一个k-1维向量\(T(y)\)，这里具体对应于指数族分布的\(T(y)\),具体定义如下:</p>

$$T(1) = \left[ \begin{gathered}  1 \hfill \\\  0 \hfill \\\  0 \hfill \\\  ... \hfill \\\  0 \hfill \\ \end{gathered}  \right],T(2) = \left[ \begin{gathered}  0 \hfill \\\  1 \hfill \\\  0 \hfill \\\  ... \hfill \\\  0 \hfill \\ \end{gathered}  \right],T(3) = \left[ \begin{gathered}  0 \hfill \\\  0 \hfill \\\  1 \hfill \\\  ... \hfill \\\  0 \hfill \\\ \end{gathered}  \right],...,T(k - 1) = \left[ \begin{gathered}  0 \hfill \\\  0 \hfill \\\  0 \hfill \\\  ... \hfill \\\  1 \hfill \\ \end{gathered}  \right],T(k) = \left[ \begin{gathered}  0 \hfill \\\  0 \hfill \\\  0 \hfill \\\  ... \hfill \\\  0 \hfill \\ \end{gathered}  \right]$$

<p>我们定义\(T(y) _1\)表示的\(T(y)\)第一个元素，其他依次类推。</p>
<p>然后再引入一个函数，具体定义如下:</p>
$$1\{ true\} = 1$$
$$1\{ false\} = 0$$

<blockquote>
<p>可以通过带入值的方式验证上式。</p>
</blockquote>
<h2 id="2-模型推导"><a href="#2-模型推导" class="headerlink" title="2    模型推导"></a>2    模型推导</h2><p>在softmax中,我们知道\(y \in \{ 1,2,…,k\}\)。我们设置\(y=1\)的概率为\(\phi _1\),<br>类似的有\(y=k\)的概率为\(\phi _k\)，并且我们轻易得到如下公式：</p>
$$\sum\limits _{i = 1}^k {\phi _i}  = 1$$$${\phi _k} = \sum\limits _{i = 1}^{k - 1}{\phi _i}$$

<p>由于我们已经知道\(p(y = i) = {\phi _i}\)，综上可以得到:</p>

$$p(y) = \phi _1^{1\{ y = 1\} }\phi _2^{1\{ y = 2\} }...\phi _k^{1\{ y = k\} } = \phi _1^{1\{ y = 1\} }\phi _2^{1\{ y = 2\} }...\phi _k^{1 - \sum\limits _{i = 1}^{k - 1} {1\{ y = i\} } } = \phi _1^{T{(y)} _1}\phi _2^{T{{(y)} _2}}...\phi _k^{1 - \sum\limits _{i = 1}^{k - 1} {T{(y)} _i} }$$$$p(y) = \exp (T{(y) _1}\ln {\phi _1} + T{(y) _2}\ln {\phi _2} + ... + (1 - \sum\limits _{i = 1}^{k - 1} {T{{(y)} _i}} )\ln {\phi _k})$$$$p(y) = \exp (T{(y) _1}\ln \frac{\phi _1}{\phi _k} + T{(y) _2}\ln \frac{\phi _2}{\phi _k} + ... + T{(y) _{k - 1}}\ln \frac{\phi _{k - 1}}{\phi _k} + \ln {\phi _k})$$

<p>我们对照指数族分布，其中\(T(y)\)我们已经定义，可以得到其他参数如下:</p>

$$b(y) = 1$$
$$a(\eta ) =  - \ln {\phi _k}$$
$$\eta  = \left[ \begin{gathered}  \ln \frac{\phi _1}{\phi _k} \hfill \\\  \ln \frac{\phi _2}{\phi _k} \hfill \\\  ... \hfill \\\  \ln \frac{\phi _{k - 1}}{\phi _k} \hfill \\\ \end{gathered}  \right]$$

<p>我们用\(\eta\)来表示\(\phi\)，目标是将我们的问题归整为一个变量的问题，从而更容易计算。对上面向量拆开计算得到:</p>

$${\eta _i} = \ln \frac{\phi _i}{\phi _k}$$

<p>继而可以转化为:</p>

$${\eta _i} = \ln \frac{\phi _i}{\phi _k}$$

<p>又由于我们已知\(\sum\limits _{j = 1}^k {\phi _j}  = 1\)，所以有:</p>

$$\sum\limits _{i = 1}^k {e^{\eta _i}{\phi _k}}  = 1$$

<p>继而有:</p>

$${\phi _k} = \frac{1}{\sum\limits _{j = 1}^k {e^{\eta _j}}}$$

<p>最后我们可以得到:</p>

$${\phi _i} = \frac{e^{\eta _i}}{\sum\limits _{j = 1}^k {e^{\eta _j}}}$$

<p>由于指数组分布假设是关于输入的线性函数，所以得到在已知\(x\)和\(\theta\)的情况下\(y=i\)的概率的公式，如下：</p>
$$p(y = i|x,\theta ) = {\phi _i} = \frac{e^{\eta _i}}{\sum\limits _{j = 1}^k {e^{\eta _j}}} = \frac{e^{\theta _i^Tx}}{\sum\limits _{j = 1}^k {e^{\theta _j^Tx}}}$$

<p>这里我们可以得到\({h_\theta }(x)\),如下:</p>
$${h_\theta }(x) = E[T(y)|x,\theta ] = E[\left( \begin{gathered}  1\{ y = 1\}  \hfill \\\  1\{ y = 2\}  \hfill \\\  ... \hfill \\\  1\{ y = k - 1\}  \hfill \\\ \end{gathered}  \right)|x,\theta ]$$

$${h_ \theta }(x) = E[\left( \begin{gathered}  {\phi _1} \hfill \\\  {\phi _2} \hfill \\\  ... \hfill \\\  {\phi _{k - 1}} \hfill \\\ \end{gathered}  \right)|x,\theta ] = \left( \begin{gathered}  \frac{e^{\theta _1^Tx}}{\sum\limits _{j = 1}^k {e^{\theta  _j^Tx}}} \hfill \\\  \frac{e^{\theta _2^Tx}}{\sum\limits _{j = 1}^k {e^{\theta _j^Tx}}} \hfill \\\  ... \hfill \\\  \frac{e^{\theta _{k - 1}^Tx}}{\sum\limits _{j = 1}^k {e^{\theta _j^Tx}}} \hfill \\\\end{gathered}  \right)$$

<blockquote>
<p>其中,\(k\)为取值的可能集合的大小，\(\theta\)为一个\(k*n\)的矩阵。</p>
</blockquote>
<p>到这里我们可以定义我们的损失函数了，如下:</p>
$$J(x) = \frac{1}{2}\sum\limits _{i = 1}^m {{(T(y) - {h _\theta }(x))}^2}$$

<p>上式是一个向量，我们可以对这个向量继续求平方和，来衡量准确度，如下：</p>
$$J(x) = \frac{1}{2}\sum\limits _{i = 1}^m {|{{(T(y) - {h _\theta }(x))}^2}|}$$

<p>损失函数已经定义完成，我们就有了算法的截止条件了。接下来就是找到算法的最优迭代方向，也就是计算其偏导数了。</p>
<p>我们使用最大释然性来计算迭代方向。建模的原则是：对于一个已知\(y=i\)的样本\((x,y)\)，我们需要找到一个方向去迭代\(\theta\)，使得\(\phi _i\)尽可能大，使得其他\(\phi\)尽可能小。当然由于所有\(\phi\)之和为1,所以我们只需要保证\(\phi _i\)尽可能大即可。因此,在已经的\(m\)个样本的情况下，我们只需要保证下面的式子最大:</p>

$$\sum\limits _{i = 1}^m {p(y = {y^i}|x,\theta )}  = \sum\limits _{i = 1}^m {\prod\limits _{l = 1}^k {p{(y = l|x,\theta )}^{1\{ {y^i} = l\}}} }$$

<p>取自然对数后，我们定义最大似然函数:</p>

$$l(\theta ) = \sum\limits _{i = 1}^m {\ln \prod\limits _{l = 1}^k {(\frac{e^{\theta _l^Tx}}{\sum\limits _{j = 1}^k {e^{\theta _j^Tx}}})^{1\{{y^i} = l\}}}}$$
<p>当且仅当\({y^i} = l\)的时候, \(^{1{y^i=l\} = 1})。其他值为0，我们可以将连乘转化，如下：</p>
$$l(\theta ) = \sum\limits _{i = 1}^m {\ln (\frac{e^{\theta _{y^i}^Tx}}{\sum\limits _{j = 1}^k {e^{\theta _j^Tx}}})}  = \sum\limits _{i = 1}^m {\ln [\theta _{y^i}^Tx - \ln \sum\limits _{j = 1}^k {e^{\theta _j^Tx}}]} $$

<p>然后对其求偏导数,对\(y _i=f\)的时候后有：</p>
$$\frac{\partial l(\theta )}{\partial {\theta _f}} = \sum\limits _{i = 1}^m {\ln [{x _f} - \frac{e^{\theta _f^Tx}{x _f}}{\sum\limits _{j = 1}^k {e^{\theta _j^Tx}}}]}$$

<p>对\({y_i} \ne f\)的时候，有：</p>
$$\frac{\partial l(\theta )}{\partial {\theta _f}} = \sum\limits _{i = 1}^m {\ln [0 - \frac{e^{\theta _f^Tx}{x _f}}{\sum\limits _{j = 1}^k {e^{\theta _j^Tx}}}]}$$

<p>综上，可以得到:</p>
$$\frac{\partial l(\theta )}{\partial {\theta _f}} = \sum\limits _{i = 1}^m {\ln [1\{ {y^i} = f\}  - \frac{e^{\theta _f^Tx}}{\sum\limits _{j = 1}^k {e^{\theta _j^Tx}}}]{x _f}}  = \sum\limits _{i = 1}^m {\ln [1\{{y^i} = f\}  - {\phi _f}]{x _f}}$$

<blockquote>
<p>\(x _f\)是一个向量，因为公式一次迭代更新一个维度下的一组\(\theta\)。事实上，这里是一个向量求微分。如果我们对的某个元素\(\theta\)进行微分，我们依然能够得到这个公式。</p>
</blockquote>
<h2 id="3-实际问题的解决"><a href="#3-实际问题的解决" class="headerlink" title="3 实际问题的解决"></a>3 实际问题的解决</h2><p>我们随机制造一组数据，在\(2*2\)的空间内使用直线\({x _2} = 0.5*{x _1}\)和直线\({x _2} = -0.5*{x _1}+2\)具体的分类如下:</p>
<p><img src="/images/机器学习/监督学习-softmax样本.png" width="50%" height="50%" text-align="center/"></p>
<p>根据之前的推导，编写如下代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> math</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> matplotlib.lines <span class="keyword">import</span> Line2D</div><div class="line"></div><div class="line">g_x_arr=[[<span class="number">1</span>, <span class="number">0.035</span>, <span class="number">1.344</span>], [<span class="number">1</span>, <span class="number">0.662</span>, <span class="number">0.598</span>], [<span class="number">1</span>, <span class="number">1.791</span>, <span class="number">1.889</span>], [<span class="number">1</span>, <span class="number">0.158</span>, <span class="number">0.12</span>], [<span class="number">1</span>, <span class="number">1.55</span>, <span class="number">1.835</span>], [<span class="number">1</span>, <span class="number">2.0</span>, <span class="number">0.613</span>], [<span class="number">1</span>, <span class="number">1.176</span>, <span class="number">0.368</span>], [<span class="number">1</span>, <span class="number">0.564</span>, <span class="number">0.043</span>], [<span class="number">1</span>, <span class="number">1.559</span>, <span class="number">1.507</span>], [<span class="number">1</span>, <span class="number">1.998</span>, <span class="number">0.988</span>], [<span class="number">1</span>, <span class="number">0.082</span>, <span class="number">0.941</span>], [<span class="number">1</span>, <span class="number">0.542</span>, <span class="number">1.371</span>], [<span class="number">1</span>, <span class="number">0.542</span>, <span class="number">0.671</span>], [<span class="number">1</span>, <span class="number">1.34</span>, <span class="number">1.856</span>], [<span class="number">1</span>, <span class="number">0.049</span>, <span class="number">0.089</span>], [<span class="number">1</span>, <span class="number">1.933</span>, <span class="number">0.871</span>], [<span class="number">1</span>, <span class="number">1.753</span>, <span class="number">1.024</span>], [<span class="number">1</span>, <span class="number">0.315</span>, <span class="number">1.341</span>], [<span class="number">1</span>, <span class="number">0.829</span>, <span class="number">1.26</span>], [<span class="number">1</span>, <span class="number">0.686</span>, <span class="number">1.721</span>], [<span class="number">1</span>, <span class="number">1.222</span>, <span class="number">1.129</span>], [<span class="number">1</span>, <span class="number">0.55</span>, <span class="number">0.075</span>], [<span class="number">1</span>, <span class="number">0.767</span>, <span class="number">0.346</span>], [<span class="number">1</span>, <span class="number">1.516</span>, <span class="number">1.752</span>], [<span class="number">1</span>, <span class="number">1.347</span>, <span class="number">0.905</span>], [<span class="number">1</span>, <span class="number">0.127</span>, <span class="number">0.782</span>], [<span class="number">1</span>, <span class="number">1.169</span>, <span class="number">1.272</span>], [<span class="number">1</span>, <span class="number">1.301</span>, <span class="number">0.273</span>], [<span class="number">1</span>, <span class="number">0.081</span>, <span class="number">0.739</span>], [<span class="number">1</span>, <span class="number">0.203</span>, <span class="number">0.658</span>], [<span class="number">1</span>, <span class="number">0.347</span>, <span class="number">1.064</span>], [<span class="number">1</span>, <span class="number">0.793</span>, <span class="number">1.193</span>], [<span class="number">1</span>, <span class="number">1.428</span>, <span class="number">0.326</span>], [<span class="number">1</span>, <span class="number">0.509</span>, <span class="number">0.983</span>], [<span class="number">1</span>, <span class="number">0.12</span>, <span class="number">0.884</span>], [<span class="number">1</span>, <span class="number">0.251</span>, <span class="number">0.282</span>], [<span class="number">1</span>, <span class="number">0.73</span>, <span class="number">0.445</span>], [<span class="number">1</span>, <span class="number">1.889</span>, <span class="number">1.323</span>], [<span class="number">1</span>, <span class="number">1.314</span>, <span class="number">1.795</span>], [<span class="number">1</span>, <span class="number">1.297</span>, <span class="number">1.467</span>], [<span class="number">1</span>, <span class="number">1.669</span>, <span class="number">0.613</span>], [<span class="number">1</span>, <span class="number">0.753</span>, <span class="number">0.114</span>], [<span class="number">1</span>, <span class="number">0.94</span>, <span class="number">1.972</span>], [<span class="number">1</span>, <span class="number">0.738</span>, <span class="number">1.603</span>], [<span class="number">1</span>, <span class="number">1.508</span>, <span class="number">1.237</span>], [<span class="number">1</span>, <span class="number">0.979</span>, <span class="number">0.572</span>], [<span class="number">1</span>, <span class="number">0.128</span>, <span class="number">1.254</span>], [<span class="number">1</span>, <span class="number">0.569</span>, <span class="number">0.155</span>], [<span class="number">1</span>, <span class="number">0.88</span>, <span class="number">0.211</span>], [<span class="number">1</span>, <span class="number">0.405</span>, <span class="number">0.603</span>], [<span class="number">1</span>, <span class="number">1.02</span>, <span class="number">1.9</span>], [<span class="number">1</span>, <span class="number">0.438</span>, <span class="number">1.535</span>], [<span class="number">1</span>, <span class="number">1.506</span>, <span class="number">1.638</span>], [<span class="number">1</span>, <span class="number">1.712</span>, <span class="number">0.394</span>], [<span class="number">1</span>, <span class="number">0.556</span>, <span class="number">0.124</span>], [<span class="number">1</span>, <span class="number">0.444</span>, <span class="number">0.115</span>], [<span class="number">1</span>, <span class="number">0.595</span>, <span class="number">1.009</span>], [<span class="number">1</span>, <span class="number">0.165</span>, <span class="number">0.089</span>], [<span class="number">1</span>, <span class="number">1.57</span>, <span class="number">0.634</span>], [<span class="number">1</span>, <span class="number">1.429</span>, <span class="number">1.181</span>], [<span class="number">1</span>, <span class="number">0.8</span>, <span class="number">0.671</span>], [<span class="number">1</span>, <span class="number">1.914</span>, <span class="number">1.091</span>], [<span class="number">1</span>, <span class="number">0.594</span>, <span class="number">0.569</span>], [<span class="number">1</span>, <span class="number">0.935</span>, <span class="number">0.277</span>], [<span class="number">1</span>, <span class="number">0.47</span>, <span class="number">0.522</span>], [<span class="number">1</span>, <span class="number">0.94</span>, <span class="number">1.924</span>], [<span class="number">1</span>, <span class="number">0.194</span>, <span class="number">1.933</span>], [<span class="number">1</span>, <span class="number">0.612</span>, <span class="number">0.613</span>], [<span class="number">1</span>, <span class="number">0.236</span>, <span class="number">0.894</span>], [<span class="number">1</span>, <span class="number">1.888</span>, <span class="number">0.251</span>], [<span class="number">1</span>, <span class="number">1.548</span>, <span class="number">0.191</span>], [<span class="number">1</span>, <span class="number">1.543</span>, <span class="number">0.603</span>], [<span class="number">1</span>, <span class="number">1.521</span>, <span class="number">0.02</span>], [<span class="number">1</span>, <span class="number">0.923</span>, <span class="number">0.856</span>], [<span class="number">1</span>, <span class="number">0.649</span>, <span class="number">1.31</span>], [<span class="number">1</span>, <span class="number">0.379</span>, <span class="number">1.746</span>], [<span class="number">1</span>, <span class="number">1.345</span>, <span class="number">0.902</span>], [<span class="number">1</span>, <span class="number">0.937</span>, <span class="number">0.524</span>], [<span class="number">1</span>, <span class="number">1.018</span>, <span class="number">0.68</span>], [<span class="number">1</span>, <span class="number">1.738</span>, <span class="number">1.623</span>], [<span class="number">1</span>, <span class="number">1.534</span>, <span class="number">1.9</span>], [<span class="number">1</span>, <span class="number">0.139</span>, <span class="number">1.911</span>], [<span class="number">1</span>, <span class="number">1.508</span>, <span class="number">1.173</span>], [<span class="number">1</span>, <span class="number">0.798</span>, <span class="number">0.865</span>], [<span class="number">1</span>, <span class="number">0.451</span>, <span class="number">1.186</span>], [<span class="number">1</span>, <span class="number">1.63</span>, <span class="number">1.123</span>], [<span class="number">1</span>, <span class="number">0.82</span>, <span class="number">0.848</span>], [<span class="number">1</span>, <span class="number">1.213</span>, <span class="number">1.48</span>], [<span class="number">1</span>, <span class="number">0.894</span>, <span class="number">0.664</span>], [<span class="number">1</span>, <span class="number">1.456</span>, <span class="number">0.934</span>], [<span class="number">1</span>, <span class="number">0.59</span>, <span class="number">1.525</span>], [<span class="number">1</span>, <span class="number">0.522</span>, <span class="number">1.329</span>], [<span class="number">1</span>, <span class="number">1.179</span>, <span class="number">1.396</span>], [<span class="number">1</span>, <span class="number">0.527</span>, <span class="number">0.273</span>], [<span class="number">1</span>, <span class="number">1.399</span>, <span class="number">1.215</span>], [<span class="number">1</span>, <span class="number">0.966</span>, <span class="number">1.514</span>], [<span class="number">1</span>, <span class="number">1.341</span>, <span class="number">0.028</span>], [<span class="number">1</span>, <span class="number">0.479</span>, <span class="number">0.191</span>], [<span class="number">1</span>, <span class="number">1.193</span>, <span class="number">0.724</span>], [<span class="number">1</span>, <span class="number">0.714</span>, <span class="number">1.285</span>]]</div><div class="line">g_Ty_arr=[[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>]]</div><div class="line">g_y_arr=[<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">phi</span><span class="params">(i,theta,x)</span>:</span></div><div class="line">	theta_n = len(theta)</div><div class="line">	numerator = math.exp(np.dot(theta[i], x))</div><div class="line">	denominator = <span class="number">0</span>;</div><div class="line">	<span class="keyword">for</span> j <span class="keyword">in</span> range(theta_n):                      <span class="comment"># maybe need to optimize</span></div><div class="line">		denominator = denominator + math.exp(np.dot(theta[j],x))</div><div class="line">	<span class="keyword">return</span> numerator/denominator</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">h_theta</span><span class="params">(theta,x)</span>:</span></div><div class="line">	theta_n = len(theta)</div><div class="line">	ret = []</div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(theta_n):</div><div class="line">		ret.append(phi(i,theta,x))</div><div class="line">	<span class="keyword">return</span> ret</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">one</span><span class="params">(y,i)</span>:</span></div><div class="line">	<span class="keyword">if</span> y == i:</div><div class="line">		<span class="keyword">return</span> <span class="number">1</span></div><div class="line">	<span class="keyword">else</span>:</div><div class="line">		<span class="keyword">return</span> <span class="number">0</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">derl</span><span class="params">(f,x_arr,y_arr,theta)</span>:</span></div><div class="line">	<span class="comment"># 0.</span></div><div class="line">	<span class="comment"># f is the dimension to calc the partial derivative</span></div><div class="line">	<span class="comment"># 1. calc the size</span></div><div class="line">	<span class="comment"># theta_n is the size of theta, equals to the length of Ty' result set -1 .Here, equals lenght of &#123;0,1,2&#125; = 3-1 =2</span></div><div class="line">	<span class="comment"># m is the sum of samples</span></div><div class="line">	<span class="comment"># x_n is the dimension of the input variable 'x' + 1 (for x_0 =1). Here is 2 + 1 = 3</span></div><div class="line">	theta_n = len(theta)</div><div class="line">	m = len(x_arr)</div><div class="line">	x_n = len(x_arr[<span class="number">0</span>])</div><div class="line">	<span class="comment"># initial the output variable sum.Here is a vector, and the length of this vector is x_n</span></div><div class="line">	sum = []</div><div class="line">	<span class="keyword">for</span> x_dim_index <span class="keyword">in</span> range(x_n):</div><div class="line">			sum.append(<span class="number">0</span>)</div><div class="line">	<span class="comment"># 2. calc the partial derivative</span></div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(x_n):</div><div class="line">		sum[i] = <span class="number">0</span></div><div class="line">		<span class="keyword">for</span> j <span class="keyword">in</span> range(m):</div><div class="line">			sum[i]=sum[i]+(one(y_arr[j],f)-phi(f,theta,x_arr[j]))*x_arr[j][i]</div><div class="line">		sum[i]=sum[i]/m</div><div class="line">	<span class="comment">#sum=plus_vector(sum,theta[f],0.1)</span></div><div class="line">	<span class="keyword">return</span> sum</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plus_vector</span><span class="params">(arr1,arr2,a)</span>:</span></div><div class="line">	<span class="keyword">return</span> [x + a * y <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(arr1, arr2)]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(theta,a,x_arr,y_arr)</span>:</span></div><div class="line">	theta_n = len(theta)</div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(theta_n):</div><div class="line">		theta[i]=plus_vector(theta[i],derl(i, x_arr,y_arr,theta),a)</div><div class="line">	<span class="keyword">return</span> theta</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">judge1</span><span class="params">(theta,x_arr,y_arr_vector,limit,debug)</span>:</span></div><div class="line">	j_theta = J(theta,x_arr,y_arr_vector)</div><div class="line">	<span class="keyword">if</span> j_theta &lt; limit:</div><div class="line">		<span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">	<span class="keyword">if</span> debug:</div><div class="line">		print(<span class="string">"|J_theta(x)| = "</span>, j_theta,<span class="string">"\n"</span>)</div><div class="line">	<span class="keyword">return</span> <span class="keyword">False</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">J</span><span class="params">(theta,x_arr,y_arr_vector)</span>:</span></div><div class="line">	sum=<span class="number">0</span></div><div class="line">	m = len(x_arr)</div><div class="line">	y_len = len(y_arr_vector[<span class="number">0</span>])</div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(m):</div><div class="line">		<span class="keyword">for</span> j <span class="keyword">in</span> range(y_len):</div><div class="line">			sum = sum + (phi(j,theta,x_arr[i]) - y_arr_vector[i][j])**<span class="number">2</span></div><div class="line">	<span class="keyword">return</span> sum</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calTheVaule</span><span class="params">()</span>:</span></div><div class="line">	<span class="comment">#theta=[[theta_1_1,theta_1_2],[theta_2_1,theta_2_2],[theta_3_1,theta_3_2]]</span></div><div class="line">	theta = [[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]]</div><div class="line">	a = <span class="number">1</span></div><div class="line">	count = <span class="number">0</span></div><div class="line">	<span class="keyword">while</span> <span class="number">1</span>:</div><div class="line">		<span class="keyword">if</span> judge1(theta,g_x_arr,g_Ty_arr,<span class="number">1</span>,<span class="keyword">True</span>):</div><div class="line">			<span class="keyword">break</span></div><div class="line">		theta=update(theta,a,g_x_arr,g_y_arr)</div><div class="line">		count = count + <span class="number">1</span></div><div class="line">		print(<span class="string">"count="</span>,count,<span class="string">"theta="</span>,theta)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcIndex</span><span class="params">(theta,x)</span>:</span></div><div class="line">  phi0 = phi(<span class="number">0</span>,theta,x)</div><div class="line">  phi1 = phi(<span class="number">1</span>,theta,x)</div><div class="line">  phi2 = phi(<span class="number">2</span>,theta,x)</div><div class="line">  <span class="keyword">if</span> phi0 &gt;= phi1 <span class="keyword">and</span> phi0 &gt;=phi1:</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span></div><div class="line">  <span class="keyword">elif</span> phi1&gt;= phi0 <span class="keyword">and</span> phi1 &gt;= phi2:</div><div class="line">    <span class="keyword">return</span> <span class="number">1</span></div><div class="line">  <span class="keyword">else</span>:</div><div class="line">    <span class="keyword">return</span> <span class="number">2</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">testValue</span><span class="params">()</span>:</span></div><div class="line">  <span class="comment"># limit =1， count = 33844</span></div><div class="line">  theta = [[<span class="number">17.418683742474045</span>, <span class="number">13.989348587756815</span>, <span class="number">-37.731307869980014</span>],[<span class="number">-33.56035031194091</span>, <span class="number">1.3797244978249763</span>, <span class="number">34.095843086377855</span>],[<span class="number">19.97548491920147</span>, <span class="number">-11.65484983882828</span>, <span class="number">7.628008093823735</span>]]</div><div class="line">  samples = <span class="number">100</span></div><div class="line">  figure, ax = plt.subplots()</div><div class="line">  <span class="comment"># 设置x，y值域</span></div><div class="line">  ax.set_xlim(left=<span class="number">0</span>, right=<span class="number">2</span>)</div><div class="line">  ax.set_ylim(bottom=<span class="number">0</span>, top=<span class="number">2</span>)</div><div class="line">  <span class="comment"># 两条line的数据</span></div><div class="line">  (line1_xs, line1_ys) = [(<span class="number">0</span>, <span class="number">2</span>), (<span class="number">0</span>, <span class="number">1</span>)]</div><div class="line">  (line2_xs, line2_ys) = [(<span class="number">0</span>, <span class="number">2</span>), (<span class="number">2</span>, <span class="number">1</span>)]</div><div class="line">  <span class="comment"># 创建两条线，并添加</span></div><div class="line">  ax.add_line(Line2D(line1_xs, line1_ys, linewidth=<span class="number">1</span>, color=<span class="string">'blue'</span>))</div><div class="line">  ax.add_line(Line2D(line2_xs, line2_ys, linewidth=<span class="number">1</span>, color=<span class="string">'blue'</span>))</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(samples):</div><div class="line">    x_0 = random.randint(<span class="number">0</span>,<span class="number">2000</span>)/<span class="number">1000</span></div><div class="line">    x_1 = random.randint(<span class="number">0</span>,<span class="number">2000</span>)/<span class="number">1000</span></div><div class="line">    index = calcIndex(theta,[<span class="number">1</span>,x_0,x_1])</div><div class="line">    <span class="keyword">if</span> index == <span class="number">0</span>:</div><div class="line">      plt.plot(x_0, x_1, <span class="string">'b--'</span>, marker=<span class="string">'x'</span>, color=<span class="string">'r'</span>)</div><div class="line">    <span class="keyword">elif</span> index == <span class="number">1</span>:</div><div class="line">      plt.plot(x_0, x_1, <span class="string">'b--'</span>, marker=<span class="string">'+'</span>, color=<span class="string">'g'</span>)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">      plt.plot(x_0, x_1, <span class="string">'b--'</span>, marker=<span class="string">'o'</span>, color=<span class="string">'b'</span>)</div><div class="line">  plt.xlabel(<span class="string">"x1"</span>)</div><div class="line">  plt.ylabel(<span class="string">"x2"</span>)</div><div class="line">  plt.plot()</div><div class="line">  plt.show()</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__== <span class="string">"__main__"</span>:</div><div class="line">  <span class="comment">#calTheVaule()</span></div><div class="line">  testValue()</div></pre></td></tr></table></figure>
<p>经过33844次迭代之后，我们得到\(\theta\) = [[17.418683742474045, 13.989348587756815, -37.731307869980014],[-33.56035031194091, 1.3797244978249763, 34.095843086377855],[19.97548491920147, -11.65484983882828, 7.628008093823735]]</p>
<blockquote>
<p>迭代的次数越多数据会越准确，这里迭代了上三万次，实际上可以通过牛顿法来减少迭代的次数，这里就不再重新代码了，牛顿法可以参见前面的文章。</p>
</blockquote>
<p>然后，我们随机制造一组值，看看分类效果，具体如下：</p>
<p><img src="/images/机器学习/监督学习-softmax计算结果.png" width="50%" height="50%" text-align="center/"></p>
<h2 id="附录-公式推导手写版"><a href="#附录-公式推导手写版" class="headerlink" title="附录 公式推导手写版"></a>附录 公式推导手写版</h2><p><img src="/images/机器学习/监督学习-softmax公式推导手写版1.png" width="50%" height="50%" text-align="center/"></p>
<p><img src="/images/机器学习/监督学习-softmax公式推导手写版2.png" width="50%" height="50%" text-align="center/"></p>
<p><img src="/images/机器学习/监督学习-softmax公式推导手写版3.png" width="50%" height="50%" text-align="center/"></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-机器学习-2.3-监督学习之通用线性模型" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2017/07/16/机器学习-2.3-监督学习之通用线性模型/" class="article-date">
      <time datetime="2017-07-16T06:33:58.000Z" itemprop="datePublished">2017-07-16</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/07/16/机器学习-2.3-监督学习之通用线性模型/">机器学习-2.3-监督学习之通用线性模型</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <script type="text/javascript" src="/Users/zcy/Desktop/study/git/MathJax-master/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h1 id="Generalized-Linear-Models"><a href="#Generalized-Linear-Models" class="headerlink" title="Generalized Linear Models"></a>Generalized Linear Models</h1><p>回归分析中我们使用了高斯分布,分类问题服从伯努利分布。这两种方法都是第一个更通用的分布的特例，我们称其Generalized Linear Models (GLMs)</p>
<h2 id="1-指数族分布"><a href="#1-指数族分布" class="headerlink" title="1 指数族分布"></a>1 指数族分布</h2><p>我们定义如下形式为指数族分布：</p>
$$p(y;\eta ) = b(y)exp({\eta ^T}T(y) - a(\eta ))$$

<p>其中,\(\eta \)是自然参数(natural parameter )，\(T(y) \)叫做充分统计量(sufficient statistic),一般\(T(y)=y\)。\(a(\eta) \)是log partition function 。\({e^{ - a\left( \eta  \right)}}\)在归整化很重要，这个参数确保\(p(y;\eta )\)归整为1。</p>
<h2 id="2-指数族分布的特例"><a href="#2-指数族分布的特例" class="headerlink" title="2 指数族分布的特例"></a>2 指数族分布的特例</h2><p>其中伯努利分布和高斯分布都是指数族分布的一个特例。</p>
<h3 id="2-1-伯努利分布的指数族表达"><a href="#2-1-伯努利分布的指数族表达" class="headerlink" title="2.1 伯努利分布的指数族表达"></a>2.1 伯努利分布的指数族表达</h3><p>伯努利分布的指数表示:</p>
$$p(y,\varphi ) = {\varphi ^y}{(1 - \varphi )^{1 - y}} = \exp (y\log \frac{\varphi }{1 - \varphi } + \log (1 - \varphi ))$$

<p>对照指数函数族的表达式，有\(b(y) = 1\),\(T(y) = y\),\(\eta  = \log \frac{\varphi }{1 - \varphi}\)等价于\(\varphi=\frac{e^\eta}{e^\eta + 1}\),\(a(y) = \log \frac{1}{1 - \varphi } = \log ({e^\eta } + 1)\)</p>
<h3 id="2-2-高斯分布的指数族表达"><a href="#2-2-高斯分布的指数族表达" class="headerlink" title="2.2 高斯分布的指数族表达"></a>2.2 高斯分布的指数族表达</h3><p>高斯分布的指数表示:</p>
$$p(y;\mu ) = \frac{1}{{\sqrt {2\pi } }}\exp ( - \frac{1}{2}{(y - \mu )^2}) = \frac{1}{{\sqrt {2\pi } }}\exp ( - \frac{1}{2}{y^2})\exp (\mu y - \frac{1}{2}{\mu ^2})$$

<p>对照指数函数族的表达式，有\(b(y) = \frac{1}{\sqrt {2\pi }}\exp ( - \frac{1}{2}{y^2})\),\( \eta = \mu \),\(T(y) = y\),\(a(\eta ) = \frac{1}{2}{u^2} = \frac{1}{2}{\eta ^2}\)。</p>
<h2 id="3-指数函数组的三个假设"><a href="#3-指数函数组的三个假设" class="headerlink" title="3 指数函数组的三个假设"></a>3 指数函数组的三个假设</h2><p>三个假设分别为:</p>
<ul>
<li>\(y|x;\theta\)服从指数分布</li>
<li>给定\(x\)的情况下，我们的目标是预测\(T(y)\)。在大多数的例子中，\(T(y)=y\),因此就意味着我们通过学习得到函数h, 使得\(h(x)=E(y|x)\), 意思就是给定x，得到函数h使得其与训练样本的数学期望相等。</li>
<li>自然参数\(\theta\)与输入\(x\)是线性关系。</li>
</ul>
<p>对照上面的过程，我们可知伯努利分布和高斯分布都符合这三个假设</p>
<h2 id="4-GLMs公式推导"><a href="#4-GLMs公式推导" class="headerlink" title="4 GLMs公式推导"></a>4 GLMs公式推导</h2><p>还是之前的问题，已知有\(m\)个样本，通过学习模拟正确的\(h(x)\),使得其能够有效的推测\(y\)。(这里我们假定了\(T(y)=y\))</p>
<p>得到最大释然函数:</p>
$$l(\eta ) = \sum\limits_{i = 1}^m {b({y^i}){\text{ }}exp({\eta ^T}T({y^i}) - a(\eta ))} $$

<p>然后为了求其最大值，我们对其取自然对数后求倒数。</p>
$$\frac{{\partial \ln (l(\eta ))}}{{\partial \eta }} = \sum\limits _{i = 1}^m {\frac{{\partial (\ln b({y^i}) + {\eta ^T}T({y^i}) - a(\eta ))}}{{\partial \eta }}}  = \sum\limits _{i = 1}^m {(T({y^i})}  - \frac{{\partial a(\eta )}}{{\partial \eta }})$$

<p>在我们之前指数族分布的定义3中，\(\eta\)是各个维度\(x\)的线性表达。有\(\eta  = {\theta ^T}x\)。所以，我们对每一个维度求偏导数，有：</p>
$$\frac{{\partial \ln (l(\eta ))}}{{\partial {\theta _j}}} = \frac{{\partial \ln (l(\eta ))}}{{\partial \eta }}\frac{{\partial \eta }}{{\partial {\theta _j}}} = \sum\limits _{i = 1}^m {(T({y^i})}  - \frac{{\partial a(\eta )}}{{\partial \eta }})x _j^i$$
<p>如果指数分布为伯努利分布，我们将式子带入其中，能够轻易的关于\(\theta _j\)的偏导数，且与之前推导的结果一致，具体过程不详细说了。高斯分布同理。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-机器学习-2.2-监督学习之分类" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2017/06/13/机器学习-2.2-监督学习之分类/" class="article-date">
      <time datetime="2017-06-13T13:20:17.000Z" itemprop="datePublished">2017-06-13</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/06/13/机器学习-2.2-监督学习之分类/">机器学习 2.2 监督学习之逻辑回归</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <script type="text/javascript" src="/Users/zcy/Desktop/study/git/MathJax-master/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h1 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h1><p>本章继续研究监督学习的问题。</p>
<h2 id="1-回归问题"><a href="#1-回归问题" class="headerlink" title="1 回归问题"></a>1 回归问题</h2><p>我们随机制造一个样本空间\({x _2} \geqslant 2{x _1} + 1\)，对于对空间进行分类。具体例子如下：</p>
<p><img src="/images/机器学习/监督学习-分类问题样本.png" width="50%" height="50%" text-align="center/"></p>
<p>对于这些样本，假定可以通过二维线性曲线进行分类。然后模拟出某条曲线，使样本得到一个很好的分类。</p>
<p>这个分类问题的样本的y值只有0或1(0-1分布或伯努利分布)，即表示在直线上面或直线下面。如果用之前的线性回归结果将非常糟糕。因为对于分类问题，线性回归得到的线性曲线，得到的\(y &gt; 1\)或\(y &lt; 0\)这些是没有意义的，肯定是错误的。因此引入了logistic函数，转换为Logistic regression，logistic函数如下：</p>

$$h(z) = \frac{1}{{1 + {e^{ - z}}}}$$

<p>函数曲线如下：</p>
<p><img src="/images/机器学习/监督学习-分类问题辅助函数.png" width="50%" height="50%" text-align="center/"></p>
<p>如果我们设置\(z = {\theta _1}{x _1} + {\theta _0} - {x _2}\)。因此，对于分类问题就可以转化为：对于\({x _2} \geqslant {\theta _1}{x _1} + {\theta _0}\)就可以转化为h(z)趋向于0的程度，对于\({x _2} \leqslant {\theta _1}{x _1} + {\theta _0}\)就可以转化为h(z)趋向于1的程度。我们简化为如下公式:</p>

$$z = {\theta _1}{x _1} + {\theta _0}{x _0} + {\theta _2}{x _2} = {\theta ^T}x$$

<p>其中,\({x _0} = 1\)，\({\theta _2} =  - 1\)。</p>
<h2 id="2-回归问题推导"><a href="#2-回归问题推导" class="headerlink" title="2 回归问题推导"></a>2 回归问题推导</h2><p>我们求辅助函数的导数，以备后来使用：</p>

$$h'(z) = (\frac{1}{{1 + {e^{ - z}}}})' =  - \frac{{ - {e^{ - z}}}}{{{{(1 + {e^{ - z}})}^2}}} = \frac{1}{{1 + {e^{ - z}}}}(1 - 1 + {e^{ - z}}) = h(z)(1 - h(z))$$

<p>接下来我们从公式角度对问题进行建模分析与推导。</p>
<p>这里我们将我们的函数\({h_\theta }(x)\)转化为如下形式:</p>

$${h_\theta }(x) = \frac{1}{{1 + {e^{ - {\theta ^T}x}}}}$$

<p>对于0-1分布，由于\({h<em>\theta }(x)\)只能为0或1。所以，在已知\(x\)和\({\theta}\)的情况下，我们可以认为有\({h</em>\theta }(x)\)的概率得到\(y=1\)。因此有如下公式：<br>
$$p(y = 1|x,\theta ) = {h_\theta }(x)$$
$$p(y = 0|x,\theta ) = 1 - {h_\theta }(x)$$
</p>
<p>综合两个公式有如下公式：</p>

$$p(y|x,\theta ) = {h_\theta }{(x)^y}{(1 - {h _\theta }(x))^{(1 - y)}}$$

<p>即在已经样本的情况下得到准确的\(y\)，即要使用极大释然方法，得到样本概率乘积的最大值,即得到下式的最大值：</p>

$$L(\theta ) = \prod\limits _{i = 1}^m {{h _\theta }{{(x{}^i)}^{{y^i}}}{{(1 - {h _\theta }({x^i}))}^{(1 - {y^i})}}} $$

<p>取自然对数，对其中一个维度\({\theta _j}\)求偏导数，如下：</p>

$$\ell ({\theta _j}) = \frac{{\partial \ln (L(\theta ))}}{{\partial {\theta _j}}} = \frac{\partial } {{\partial {\theta _j}}}(\sum\limits _{i = 1}^m {({y^i}\ln ({h _\theta }(x{}^i)) + (1 - {y^i})} \ln (1 - {h _\theta }(x{}^i))))$$ 

<p>我们使用一个辅助函数\(z = {\theta ^T}x\), 并且\(\ln x\)的倒数是\(\frac{1}{x}\)。<br>又由于复合函数有如下求导法则：</p>

$$\frac{{df(g(x))}}{{dx}} = \frac{{df(z)}}{{dz}}g(x)'$$

<p>其中\(z = g(x)\)。。(这里再次强调下标识维度，上表是样本编号)。所以有：</p>

$$\ell ({\theta _j}) = \frac{{\partial L(\theta )}}{{\partial {\theta _j}}} = \frac{\partial }{{\partial {\theta _j}}}(\sum\limits _{i = 1}^m {({y^i}\ln ({h _\theta }(x{}^i)) + (1 - {y^i})} \ln (1 - {h _\theta }(x{}^i))))$$

<p>这里设\(z = {\theta ^T}x\)，有：</p>

$$\ell ({\theta _j}) = \sum\limits _{i = 1}^m {({y^i}\frac{1}{{h(z)}}h'(z) + (1 - {y^i})} \frac{1}{{1 - h(z)}}( - h'(z)))$$

<p>将之前的倒数公式带入其中,有:</p>

$$\ell ({\theta _j}) = \sum\limits _{i = 1}^m {(x _j^i(y - {h _\theta }({x^i})))} $$

<p>因此这样就可以使用如下的式子学习样本值：</p>

$${\theta _j} = {\theta _j} + \alpha \sum\limits _{i = 1}^m {(x _j^i(y - {h _\theta }({x^i})))} $$

<h2 id="3-代码"><a href="#3-代码" class="headerlink" title="3 代码"></a>3 代码</h2><p>从这一章开始讲代码从matlab转化更为通用的python。</p>
<p>我们选取合适的步长，当\(J({\theta})\)小于合适的阈值(这里配置为4)，就认为迭代完成。分类代码如下:</p>
<pre><code>import random
import math
import numpy as np
from matplotlib import pyplot as plt
k = 2
b = 1
x1_res = [-0.711, -0.2798, 0.4258, 0.4108, 0.0492, -0.842, -0.6036, -0.8394, 0.395, -0.8612, 0.806, 0.1476, 0.5464,
          0.3992,
          0.8326, 0.76, -0.7288, 0.0718, -0.4436, 0.8448, -0.017, -0.108, -0.4032, -0.9678, -0.4322, -0.9024, -0.0146,
          -0.5346, -0.3808, 0.0498, 0.6376, -0.3946, 0.5896, -0.9504, 0.2046, -0.0994, 0.468, -0.9236, 0.0996, 0.4366,
          -0.167, 0.9618, 0.5796, -0.383, 0.0254, -0.367, 0.0734, -0.8434, -0.9948, -0.0188]
x2_res = [-0.5108, -0.1254, 0.6166, 0.5296, 0.5758, -0.5972, 0.3994, 0.9808, 0.7134, -0.9414, -0.098, -0.2778, 0.667,
          0.1916, -0.0298, -0.5124, -0.509, -0.9754, 0.261, 0.4256, -0.7104, 0.8398, 0.7094, -0.5558, -0.799, 0.666,
          0.392,
          0.6688, 0.7832, 0.6372, -0.788, -0.1138, 0.0922, -0.6254, -0.1456, 0.3608, 0.4694, -0.8946, 0.9708, 0.8736,
          -0.5254, 0.0252, 0.6554, -0.2486, 0.9064, -0.35, -0.7724, 0.131, 0.4446, 0.2468]

def make_samples(x0_arr,x1_arr,x2_arr,y_arr):
  # classified by x_2 = 2*x_1 + 1 . (x_1,x_2) is the point of 2-D plane
  # y is 0 or 1, means &apos;x_2 &gt;= 2*x_1 + 1 &apos;  or &apos;x_2 &lt; 2*x_1 + 1&apos;
  for i in range(len(x1_res)):
    x0_arr.append(1)
    x1_arr.append(x1_res[i])
    x2_arr.append(x2_res[i])
    y_arr.append(0 if (x2_arr[i] &gt;= k * x1_arr[i] + b ) else 1)

def h(z):
  return 1/(1 + math.exp(-1*z))

def h_xita(x0,x1,x2,xita):
  return h(transvection([x0,x1,x2], xita))

def transvection(a,b):
  return np.dot(a, b)

def J(x0_arr,x1_arr,x2_arr,y_arr,xita):
  sum = 0;
  length = len(x1_arr)
  for i in range(length):
    sum = sum + ( h_xita(x0_arr[i],x1_arr[i],x2_arr[i],xita) - y_arr[0] )**2
  return sum/2

def updateXita(x0_arr,x1_arr,x2_arr,y_arr,xita,a,update_x):
  sum = 0;
  length = len(x0_arr)
  for i in range(length):
    sum = sum + ( y_arr[i] - h_xita(x0_arr[i],x1_arr[i],x2_arr[i],xita) ) * update_x[i]
  return sum*a

def showPic(k,b,point_x1_arr,point_x2_arr):
  ## show line
  X = np.arange(-1,1,0.001)
  Y=[]
  for i in range(X.size):
    Y.append(k*X[i]+b)
  plt.plot(X, Y, &apos;b--&apos;, label=&quot;logistic&quot;)
  ## show points
  for i in range(len(point_x1_arr)):
    if(point_x2_arr[i]&gt;2*point_x1_arr[i]+1):
      plt.plot(point_x1_arr[i],point_x2_arr[i],&apos;b--&apos;,marker = &apos;x&apos;, color = &apos;g&apos;)
    else:
      plt.plot(point_x1_arr[i],point_x2_arr[i],&apos;r--&apos;,marker = &apos;+&apos;, color = &apos;r&apos;)
  plt.xlabel(&quot;x1&quot;)
  plt.ylabel(&quot;x2&quot;)
  plt.figure(figsize=(8, 4))
  plt.show()

if __name__== &quot;__main__&quot;:
  x0_arr = []
  x1_arr = []
  x2_arr = []
  y_arr = []
  make_samples(x0_arr,x1_arr,x2_arr,y_arr)
  xita = [1,20,-1]
  a = 0.01
  limit = 4
  count = 0
  while 1:
    j = J(x0_arr, x1_arr, x2_arr, y_arr, xita)
    print(&quot;j=&quot;,j)
    if j &lt; limit:
      break;
    xita[0] = xita[0] + updateXita(x0_arr, x1_arr, x2_arr, y_arr, xita, a, x0_arr)
    xita[1] = xita[1] + updateXita(x0_arr, x1_arr, x2_arr, y_arr, xita, a, x1_arr)
    count = count + 1
    print(&quot;cout=&quot;,count)
    print(&quot;xita0=&quot;,xita[0],&quot; xita1=&quot;,xita[1])
  showPic(xita[1],xita[0],x1_arr,x2_arr)
  print(xita)
</code></pre><p>经过数论迭代分析，得到\( \theta _0 = 2.7841420917875297 \)，\( \theta _1 = 4.850051144306185 \)。</p>
<p>得到如下结果:</p>
<p><img src="/images/机器学习/监督学习-分类问题计算结果1.png" width="50%" height="50%" text-align="center/"></p>
<h2 id="4-代码的改进"><a href="#4-代码的改进" class="headerlink" title="4 代码的改进"></a>4 代码的改进</h2><p>经过试验发现上面的算法的问题是如果选取合适的阈值。这里将算法修改为两次的\(J({\theta})\)的差值小于0.00001，这样就任务函数已经几近收敛,然后结束迭代。另外，如果\(J({\theta})\)的值某一次迭代增大了,说明算法异常或者迭代参数设置过大。修改后的代码如下:</p>
<pre><code>while 1:                                                                             
  j = J(x0_arr, x1_arr, x2_arr, y_arr, xita)                                         
  print(&quot;j=&quot;,j)                                                                      
  if j &gt; j_last:                                                                     
    print(&quot;算法异常或选取迭代系数过大&quot;)                                                           
    break;                                                                           
  if j_last - j &lt; 0.00001:                                                           
    print(&quot;out &quot;,j_last - j)                                                         
    break;                                                                           
  j_last = j                                                                         
  xita[0] = xita[0] + updateXita(x0_arr, x1_arr, x2_arr, y_arr, xita, a, x0_arr)     
  xita[1] = xita[1] + updateXita(x0_arr, x1_arr, x2_arr, y_arr, xita, a, x1_arr)    
</code></pre><p>得到如下结果:</p>
<p><img src="/images/机器学习/监督学习-分类问题计算结果2.png" width="50%" height="50%" text-align="center/"></p>
<h2 id="5-牛顿法改进"><a href="#5-牛顿法改进" class="headerlink" title="5 牛顿法改进"></a>5 牛顿法改进</h2><p>这里说明另外一种快速求值的方法，牛顿法。<br><img src="/images/机器学习/牛顿法示意图.png" width="50%" height="50%" text-align="center/"></p>
<p>如上所示为牛顿法的简易过程。我们初始值为\(x _0\)，然后在\((x _0,f(x _0))\)做切线得到\(x _0\)，依次类推会接近于得到\(x _n\),使得\(f(x _n)\)接近于0。具体算法的证明请见文献2。</p>
<p>根据上面的算法，我们可以轻易的推导出</p>

$${x _{i + 1}} = {x _i} - \frac{{f({x _i})}}{{f'({x _i})}}$$

<p>联系之前的问题，我们可以转化快速得到\({\theta _j}\)使得\(\ell ({\theta _j}) = 0\)问题。（注: 之前的算法使找到最优下降方向使得\(J({\theta})\)最小，而牛顿法是直接得到最优的\({\theta _j}\)）</p>
<p>本问题的公式如下:</p>

$$\ell ({\theta _j}) = \sum\limits _{i = 1}^m {(x _j^i(y - {h _\theta }({x^i})))} $$


$$\ell '({\theta _j}) =  - \sum\limits _{i = 1}^m {{{(x _j^i)}^2}} $$


$${\theta _j}: = {\theta _j} - \frac{{\ell ({\theta _j})}}{{\ell '({\theta _j})}} = {\theta _j} - \frac{{\sum\limits _{i = 1}^m {(x _j^i(y - {h _\theta }({x^i})))} }}{{ - \sum\limits _{i = 1}^m {{{(x _j^i)}^2}} }}$$

<p>代码如下:</p>
<pre><code>import sys
import math
import numpy as np
from matplotlib import pyplot as plt
k = 2
b = 1
x1_res = [-0.711, -0.2798, 0.4258, 0.4108, 0.0492, -0.842, -0.6036, -0.8394, 0.395, -0.8612, 0.806, 0.1476, 0.5464,
          0.3992,
          0.8326, 0.76, -0.7288, 0.0718, -0.4436, 0.8448, -0.017, -0.108, -0.4032, -0.9678, -0.4322, -0.9024, -0.0146,
          -0.5346, -0.3808, 0.0498, 0.6376, -0.3946, 0.5896, -0.9504, 0.2046, -0.0994, 0.468, -0.9236, 0.0996, 0.4366,
          -0.167, 0.9618, 0.5796, -0.383, 0.0254, -0.367, 0.0734, -0.8434, -0.9948, -0.0188]
x2_res = [-0.5108, -0.1254, 0.6166, 0.5296, 0.5758, -0.5972, 0.3994, 0.9808, 0.7134, -0.9414, -0.098, -0.2778, 0.667,
          0.1916, -0.0298, -0.5124, -0.509, -0.9754, 0.261, 0.4256, -0.7104, 0.8398, 0.7094, -0.5558, -0.799, 0.666,
          0.392,
          0.6688, 0.7832, 0.6372, -0.788, -0.1138, 0.0922, -0.6254, -0.1456, 0.3608, 0.4694, -0.8946, 0.9708, 0.8736,
          -0.5254, 0.0252, 0.6554, -0.2486, 0.9064, -0.35, -0.7724, 0.131, 0.4446, 0.2468]

def make_samples(x0_arr,x1_arr,x2_arr,y_arr):
  # classified by x_2 = 2*x_1 + 1 . (x_1,x_2) is the point of 2-D plane
  # y is 0 or 1, means &apos;x_2 &gt;= 2*x_1 + 1 &apos;  or &apos;x_2 &lt; 2*x_1 + 1&apos;
  for i in range(len(x1_res)):
    x0_arr.append(1)
    x1_arr.append(x1_res[i])
    x2_arr.append(x2_res[i])
    y_arr.append(0 if (x2_arr[i] &gt;= k * x1_arr[i] + b ) else 1)

def h(z):
  return 1/(1 + math.exp(-1*z))

def h_xita(x0,x1,x2,xita):
  return h(transvection([x0,x1,x2], xita))

def transvection(a,b):
  return np.dot(a, b)

def l(x0_arr,x1_arr,x2_arr,y_arr,xita,a,update_x):
  sum = 0;
  for i in range(len(x0_arr)):
    sum = sum + ( y_arr[i] - h_xita(x0_arr[i],x1_arr[i],x2_arr[i],xita) ) * update_x[i]
  return sum*a

def derl(update_x):
  sum = 0;
  for i in range(len(update_x)):
    sum = sum + update_x[i]*update_x[i];
  return -1 * sum

def showPic(k,b,point_x1_arr,point_x2_arr):
  ## show line
  X = np.arange(-1,1,0.001)
  Y=[]
  for i in range(X.size):
    Y.append(k*X[i]+b)
  plt.plot(X, Y, &apos;b--&apos;, label=&quot;logistic&quot;)
  ## show points
  for i in range(len(point_x1_arr)):
    if(point_x2_arr[i]&gt;2*point_x1_arr[i]+1):
      plt.plot(point_x1_arr[i],point_x2_arr[i],&apos;b--&apos;,marker = &apos;x&apos;, color = &apos;g&apos;)
    else:
      plt.plot(point_x1_arr[i],point_x2_arr[i],&apos;r--&apos;,marker = &apos;+&apos;, color = &apos;r&apos;)
  plt.xlabel(&quot;x1&quot;)
  plt.ylabel(&quot;x2&quot;)
  plt.figure(figsize=(8, 4))
  plt.show()

if __name__== &quot;__main__&quot;:
  x0_arr = []
  x1_arr = []
  x2_arr = []
  y_arr = []
  make_samples(x0_arr,x1_arr,x2_arr,y_arr)
  xita = [1,20,-1]
  xita_bak = [10000,10000]
  count = 0
  while 1:
    xita[0] = xita[0] - l(x0_arr, x1_arr, x2_arr, y_arr, xita, 1,x0_arr)/derl(x0_arr)
    xita[1] = xita[1] - l(x0_arr, x1_arr, x2_arr, y_arr, xita, 1, x1_arr) / derl(x1_arr)
    if abs(l(x0_arr, x1_arr, x2_arr, y_arr, xita, 1,x0_arr)) &lt;= 0.001 :
      print(&quot;xita = &quot;,xita)
      break;
    xita_bak[1]=xita[1]
    xita_bak[0]=xita[0]
    count = count + 1
    print(&quot;count=&quot;, count, &quot; xita = &quot;, xita)

  showPic(xita[1],xita[0],x1_arr,x2_arr)
  print(xita)
</code></pre><p>得到\({\theta _0}\)和\({\theta _1}\)分别为<br>2.7762287194293407, 4.835356147838368。得到的曲线如下：</p>
<p><img src="/images/机器学习/监督学习-分类问题计算结果3.png" width="50%" height="50%" text-align="center/"></p>
<h2 id="6-参考文献"><a href="#6-参考文献" class="headerlink" title="6 参考文献"></a>6 参考文献</h2><ul>
<li>1 &lt;&lt; cs229-notes1 &gt;&gt;</li>
<li>2 &lt;&lt; 计算机科学计算 &gt;&gt; 施吉林,张宏伟，金光日编</li>
</ul>
<blockquote>
<p>未来代码将会维护在<a href="https://github.com/zhengchenyu/mlearning" target="_blank" rel="external">https://github.com/zhengchenyu/mlearning</a></p>
</blockquote>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-机器学习-2.1-监督学习之线性回归" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2017/06/10/机器学习-2.1-监督学习之线性回归/" class="article-date">
      <time datetime="2017-06-10T13:52:17.000Z" itemprop="datePublished">2017-06-10</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/06/10/机器学习-2.1-监督学习之线性回归/">机器学习 2.1 监督学习之线性回归</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <script type="text/javascript" src="/Users/zcy/Desktop/study/git/MathJax-master/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h1 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h1><p><img src="http://ord3iii9m.bkt.clouddn.com/image/github/mlearning%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E8%AF%B4%E6%98%8E.png" alt="监督学习说明"> </p>
<p>如上图所示，监督学习实质就是: 在给定训练集合，使用某种学习算法得到学习函数h，能够较为准确的预测y。</p>
<h2 id="1-线性回归"><a href="#1-线性回归" class="headerlink" title="1 线性回归"></a>1 线性回归</h2><p>下面有一组房价关于房子面积以及卧室数目的样本数据。试着从这些样本数据中构造线性模型，以预测房价。</p>
<table>
<thead>
<tr>
<th>房间数目</th>
<th>房间大小</th>
<th>房价</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>88.0</td>
<td>1760288</td>
</tr>
<tr>
<td>2</td>
<td>88.0</td>
<td>1762136</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
<h3 id="1-1-梯度下降法"><a href="#1-1-梯度下降法" class="headerlink" title="1.1 梯度下降法"></a>1.1 梯度下降法</h3><h4 id="1-1-1-梯度下降法说明"><a href="#1-1-1-梯度下降法说明" class="headerlink" title="1.1.1 梯度下降法说明"></a>1.1.1 梯度下降法说明</h4><p>假设房价与房间数目、房屋带下呈线性关系，有如下关系:</p>

$${h_\theta }(x) = {\theta _0} + {\theta _1}{x _1} + {\theta _2}{x _2}$$

<p>假设\({x_0} = 1\),则有如下公式:</p>

$${h_\theta }(x) = {\theta _0}{x _0} + {\theta _1}{x _1} + {\theta _2}{x _2} = \sum\limits _{i = 0}^n{\theta _i}{x _i}$$

<p>假设y为样本实际的值，因此有如下损失函数可以定义：</p>

$$J(\theta ) = \frac{1}{2}\sum\limits _{i = 0}^m {{{({h _\theta }({x^i}) - {y^i})}^2}} $$

<p>我们只要保证上面的公式趋于0就是合适的，因此这个问题就转化为得到合适的使得\(J(\theta )\)最小。实际也就转化为利用最小二乘法进行回归分析。</p>
<p>做这样一个想想，对于二维的情况。\(J(\theta )\)是一个关于的二维函数的话，他一定类似于锅状的凸函数。根据梯度下降算法(参考&lt;&lt;最优化方法&gt;&gt;)，沿着某一个方向的负梯度方向就是在该方向上下降最大的方向。因此，我们可以选取各个维度的负梯度作为下降的方向，为了方便各个梯度方向选取同样的步长。因此，我们对任意维度采用如下公式进行递归(为步长)：</p>

$${\theta _j}: = {\theta _j} - \alpha \frac{{\partial J(\theta )}}{{\partial {\theta _j}}} = {\theta _j} - \alpha \sum\limits _{i = 0}^m{({h _\theta }({x^i}) - {y^i}) {x _j}}$$

<p>我们简化，我们仅仅对房间大小与房价的一维关系进行回归分析。</p>
<blockquote>
<p>注：对于每一次迭代运算都需要对所有全量信息进行计算。这样带来了大量的计算。工程上可以采用采样的方式计算新的，这里数据较少，就不用该方法了。</p>
</blockquote>
<h4 id="1-1-2-梯度下降法案例"><a href="#1-1-2-梯度下降法案例" class="headerlink" title="1.1.2 梯度下降法案例"></a>1.1.2 梯度下降法案例</h4><p>我们有一组数据，为房间大小与房价的关系。我们的目标是通过使用梯度下降算法，模拟得到满足学习函数，以预测房价。可以看如下点图:</p>
<p><img src="http://ord3iii9m.bkt.clouddn.com/image/github/mlearning%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%88%BF%E4%BB%B7%E7%82%B9%E5%9B%BE.png" alt="房价点图"></p>
<p>根据上一节的公式编写如下代码进行回归分析:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">% 以下为主算法流程</div><div class="line">% y = xita0 +xita1*x</div><div class="line">mfilename=&apos;/Users/zcy/Desktop/mlearning/prices&apos;</div><div class="line">[v,x,y]=textread(mfilename,&apos;%n%n%n&apos;);</div><div class="line"> </div><div class="line">limit = 1000;</div><div class="line">y=y./10000;</div><div class="line">% step</div><div class="line">a = 0.0000001;</div><div class="line"> </div><div class="line">xita0 = 0.01;</div><div class="line">xita1 = 1.6;</div><div class="line">err = JFun(x,y,xita0,xita1);</div><div class="line">disp([&apos;err = &apos;,num2str(err)])</div><div class="line">count = 0; </div><div class="line">while err&gt;limit </div><div class="line">    xita0 = xita0 - a*gradient(x,y,xita0,xita1,true);</div><div class="line">    xita1 = xita1 - a*gradient(x,y,xita0,xita1,false)    </div><div class="line">    err = JFun(x,y,xita0,xita1);</div><div class="line">    count = count + 1;</div><div class="line">    disp([&apos;err = &apos;,num2str(err),&apos;, count = &apos;,num2str(count)])</div><div class="line">end</div><div class="line"> </div><div class="line">scatter(x,y,&apos;k&apos;)</div><div class="line">hold on</div><div class="line">x1 = 70:0.01:100;</div><div class="line">y1 = xita0+x1.*xita1;</div><div class="line">plot(x1,y1)</div></pre></td></tr></table></figure>
<p>JFun.m文件流程:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">function [s]  = JFun(x,y,xita0,xita1)</div><div class="line">s = 0;</div><div class="line">for i = 1:size(x)</div><div class="line">   s = s + 0.5*((xita0*1+xita1*x(i)-y(i))^2);</div><div class="line">end</div></pre></td></tr></table></figure>
<p>gradient.m文件: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">function [s]  = gradient(x,y,xita0,xita1,b)</div><div class="line">s = 0;</div><div class="line">if b </div><div class="line">    for i = 1:size(x)</div><div class="line">        s = s + (xita0*1+xita1*x(i)-y(i))*1;</div><div class="line">    end</div><div class="line">else </div><div class="line">    for i = 1:size(x)</div><div class="line">        s = s + (xita0*1+xita1*x(i)-y(i))*x(i);</div><div class="line">    end</div><div class="line">end</div></pre></td></tr></table></figure>
<blockquote>
<p>这里为了示意，代码仅考虑两维，实际不建议代码这样写。另外，选取合适的limit值和步长需要经过打印调试。选取不当，容易造成函数无法收敛。</p>
</blockquote>
<p>回归得到如下曲线：</p>
<p><img src="http://ord3iii9m.bkt.clouddn.com/image/github/mlearning%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%88%BF%E4%BB%B7%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BB%93%E6%9E%9C1.png" alt="回归曲线1"></p>
<p>修改limit为500之后，回归更加准确。</p>
<p><img src="http://ord3iii9m.bkt.clouddn.com/image/github/mlearning%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%88%BF%E4%BB%B7%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BB%93%E6%9E%9C2.png" alt="回归曲线2"></p>
<h3 id="1-2-线性回归的解析解"><a href="#1-2-线性回归的解析解" class="headerlink" title="1.2 线性回归的解析解"></a>1.2 线性回归的解析解</h3><p>略，详见斯坦福大学机器学习讲义。</p>
<h3 id="1-3-线性回归的概率解释"><a href="#1-3-线性回归的概率解释" class="headerlink" title="1.3 线性回归的概率解释"></a>1.3 线性回归的概率解释</h3><p>本节从概率的角度进行线性回归分析。对我们的任意一组变量有如下公式：</p>

$${y^i} = {\theta ^T}{x^i} + {\varepsilon ^i}$$

<p>其中\(\varepsilon \)为误差，假设我们的误差服从正太分布<br>\({\varepsilon ^i} \sim {\rm N}(0,{\sigma ^2})\)。即有如下公式：</p>

$$p({\varepsilon ^i}) = \frac{1}{{\sqrt {2\pi } \sigma }}{e^{( - \frac{{{{({\varepsilon ^i})}^2}}}{{2{\sigma ^2}}})}}$$

<p>代入上面的公式有：</p>

$$p({y^i}|{x^i},\theta ) = \frac{1}{{\sqrt {2\pi } \sigma }}{e^{( - \frac{{{{({y^i} - {\theta ^T}{x^i})}^2}}}{{2{\sigma ^2}}})}}$$

<p>该公式的含义为在已知\(x ^i \)和\(\theta \)的情况下，得到准确的\(y ^i \)概率。我们可以利用最大似然估计，最大释然估计实质就是对所有采样值调整参数得到最准确y值的方法。(详细请看概率论)。有如下公式:</p>

$$L(\theta ) = \prod\limits _{i = 1}^m {p({y^i}|{x^i},\theta )}  = \prod\limits _{i = 1}^m {\frac{1}{{\sqrt {2\pi } \sigma }}{e^{( - \frac{{{{({y^i} - {\theta ^T}{x^i})}^2}}}{{2{\sigma ^2}}})}}} $$

<p>上面的公式是对各个数据采样得到准确y的概率的乘积，问题也就转化为调整使的上式最大，进一步转化为求的上式的倒数为0的情况。对上式去log，如下:</p>

$$[\log (L(\theta )) = \log (\prod\limits _{i = 1}^m {\frac{1}{{\sqrt {2\pi } \sigma }}{e^{( - \frac{{{{({y^i} - {\theta ^T}{x^i})}^2}}}{{2{\sigma ^2}}})}}} )$$


$$\log (L(\theta )) = \sum\limits _{i = 1}^m {\log \frac{1}{{\sqrt {2\pi } \sigma }}{e^{( - \frac{{{{({y^i} - {\theta ^T}{x^i})}^2}}}{{2{\sigma ^2}}})}}}  = m\log \frac{1}{{\sqrt {2\pi } \sigma }} - \frac{1}{{{\sigma ^2}}} \cdot \frac{1}{2}\sum\limits _{i = 1}^m {{{({y^i} - {\theta ^T}{x^i})}^2}} $$

<p>看上式最后一项，前面部分为常数，因此问题有转化为求最小值的问题了。</p>
<p><strong>因此从概率角度上考虑误差的情况下，我们前面基于最小二乘的算法也是正确的。</strong></p>
<h3 id="1-4-局部加权回归-Loess"><a href="#1-4-局部加权回归-Loess" class="headerlink" title="1.4 局部加权回归(Loess)"></a>1.4 局部加权回归(Loess)</h3><h4 id="1-4-1-局部加权回归说明"><a href="#1-4-1-局部加权回归说明" class="headerlink" title="1.4.1 局部加权回归说明"></a>1.4.1 局部加权回归说明</h4><p>有一组非线性去先，譬如\(y = {x^2}\)。如果使用之前的方法拟合(即假设其为y=kx+b型曲线)，必然不会得到理想的结果。对于这样的问题，我们想求出\(f(20)\),加入我们只对\(x\)在20附近的样本进行一维拟合，我们就可以得到一个精确值。因此，我们引入如下公式：</p>

$$J(\theta ) = \frac{1}{2}\sum\limits_ {i = 0}^m {{\omega ^i}{{({h_ \theta }({x^i}) - {y^i})}^2}} $$


$${\omega ^i} = {e^{ - \frac{{{{({x^i} - x)}^2}}}{{2{\tau ^2}}}}}$$

<p>其中，在原来的价值函数中引入权重\(\omega\)。\(\tau\)是波长，该值越小，越趋于向20附近的值进行回归计算。对于求\(f(20)\)的情况，在的公式中\(x\)恒为20,可以知道该值在\(x ^i\)趋近于20的时候趋近于1，趋近于\( \pm \infty \)的时候趋近于0。也就是相当于仅仅是在20附近进行拟合。由于新引入的权重函数与\(\theta\)无关。所以得到如下公式:</p>

$${\theta _j}: = {\theta _j} - \alpha \frac{{\partial J(\theta )}}{{\partial {\theta _j}}} = {\theta _j} - \alpha \sum\limits _{i = 0}^m {{\omega ^i}({h _\theta }({x^i}) - {y^i})} {x _j}$$

<p>我们这里是一个一维的问题，可以得到如下公式:</p>

$$J(\theta ) = \frac{1}{2}\sum\limits _{i = 0}^m {{\omega ^i}{{({\theta _1}{x^i} + {\theta _0} - {y^i})}^2}} $$


$${\omega ^i} = {e^{ - \frac{{{{({x^i} - 20)}^2}}}{{2{\tau ^2}}}}}$$

<h4 id="1-4-2-局部加权回归案例"><a href="#1-4-2-局部加权回归案例" class="headerlink" title="1.4.2 局部加权回归案例"></a>1.4.2 局部加权回归案例</h4><p>主流程代码:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">% y = xita0 +xita1*x</div><div class="line">x = -100:1:100;</div><div class="line">y = x.*x;</div><div class="line">% x= 20, y = 40x - 400</div><div class="line">x0 = 20;       </div><div class="line">tao = 5;</div><div class="line">a = 0.0003;</div><div class="line"> </div><div class="line">xita0 = -1;</div><div class="line">xita1 = 1;</div><div class="line">limit = 7850;</div><div class="line">err = JFun_Loess(x,y,xita0,xita1,x0,tao);</div><div class="line">count = 0; </div><div class="line">while err&gt;limit </div><div class="line">    xita0 = xita0 - a*gradient_Loess(x,y,xita0,xita1,true,x0,tao);</div><div class="line">    xita1 = xita1 - a*gradient_Loess(x,y,xita0,xita1,false,x0,tao);    </div><div class="line">    err = JFun_Loess(x,y,xita0,xita1,x0,tao);</div><div class="line">    count = count + 1;</div><div class="line">end</div><div class="line"> </div><div class="line">scatter(x,y,&apos;k&apos;)</div><div class="line">hold on</div><div class="line">x1 = -100:1:100;</div><div class="line">y1 = xita0+x1.*xita1;</div><div class="line">disp([&apos;xita0 = &apos;,num2str(xita0),&apos;, xita1 = &apos;,num2str(xita1)])</div><div class="line">plot(x1,y1)</div></pre></td></tr></table></figure>
<p>JFun_loess:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">function [s]  = JFun_Loess(x,y,xita0,xita1,x0,tao)</div><div class="line">s = 0;</div><div class="line">for i = 1:size(x&apos;)</div><div class="line">    s = s + 0.5*((xita0*1+xita1*x(i)-y(i))^2)*exp(-0.5*(x(i)-x0)*(x(i)-x0)/tao/tao);</div><div class="line">end</div></pre></td></tr></table></figure>
<p>gradient_Loess:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">function [s]  = gradient_Loess(x,y,xita0,xita1,b,x0,tao)</div><div class="line">s = 0;</div><div class="line">if b </div><div class="line">    for i = 1:size(x&apos;)</div><div class="line">        s = s + (xita0*1+xita1*x(i)-y(i))*1*exp(-0.5*(x(i)-x0)*(x(i)-x0)/tao/tao);</div><div class="line">    end</div><div class="line">else</div><div class="line">    for i = 1:size(x&apos;)</div><div class="line">        s = s + (xita0*1+xita1*x(i)-y(i))*x(i)*exp(-0.5*(x(i)-x0)*(x(i)-x0)/tao/tao);</div><div class="line">    end</div><div class="line">end</div></pre></td></tr></table></figure>
<p>在x=20处分析，理论值为y = 40x – 400。经过模拟得到的值为y=39.6825x-368.253。<br>下图为得到的结果，实际上已经非常接近这个二次曲线在20这个位置的切线了。</p>
<p><img src="http://ord3iii9m.bkt.clouddn.com/image/github/mlearning%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%B1%80%E9%83%A8%E5%8A%A0%E6%9D%83%E5%88%86%E6%9E%90%E7%BB%93%E6%9E%9C%E5%9B%BE.png" alt="局部加权分析结果图"></p>
<h2 id="2-回归分析相关概念"><a href="#2-回归分析相关概念" class="headerlink" title="2 回归分析相关概念"></a>2 回归分析相关概念</h2><p>另外引申一个概念:</p>
<p>之前我们把房价与房屋大小认为是一个一维曲线。如上图，可以看出很多点被反映到曲线中。我们可以称其为欠拟合过程。与之对应的，如果使用高阶函数进行拟合，即假如样本有5个点，我们可以通过一个4阶函数的曲线进行完整拟合，但是这样的曲线往往并不是一个良好的房价与房屋大小的反映，这被称为过拟合。(该部分内容可以想见矩阵与数值分析)</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2016-2017 zhengchenyu
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" title="本站到访数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="本页阅读量"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>
    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]]}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>